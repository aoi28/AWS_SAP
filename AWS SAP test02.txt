#format:table
#title:AWS SAP
#question_count:5
#shuffle_questions:true
#shuffle_choices:true
Test2-01. <p>As an AWS professional, you have been told to ensure that traffic to an application is evenly balanced. The application has multiple web servers that host the application. Choose an answer from the below options which will fulfill the above requirement.</p> | <p> AWSのプロフェッショナルとして、アプリケーションへのトラフィックが均等に均等になるように指示されています。アプリケーションには、アプリケーションをホストする複数のWebサーバーがあります。上記の要件を満たす回答を選択してください。</p>	sa:	Place all your web servers behind ELB. Configure a Route53 ALIAS to point to the ELB DNS name. <br>  すべてのWebサーバーをELBの背後に置きます。 ELB DNS名を指すようにRoute53 ALIASを設定します。|<p></p><p>Option A is incorrect because (a) NAT instance is ideally used to route traffic from a private subnet to the internet via a public subnet, (b) NAT instance is not managed by AWS and requires to be configured and maintained by the user; hence, adding to the overhead, and (c) if not scaled, can cause performance bottleneck. NAT Gateway is a preferred option over NAT instances.</p><p>Option B is recommending us to use AWS CloudFront and configure the distributions Origin to the web server and then use a AWS Route 53 ‘ALIAS’ for the CloudFront Distribution. Even though CloudFront is highly available and is accessible to the Internet, it would work better if the Origin for the AWS CloudFront Distribution was pointed to an AWS ELB rather than to the Web Server itself. The question does not mention the presence of an ELB.&nbsp;</p><p>Since the Origin would only be a Web Server, if this server goes offline for a period of time, the web site would become unavailable the content is not cached at the Edge location or if the TTL for the content expires.&nbsp;</p><p>So, Option B is incorrect as well.</p><p>Option C is CORRECT. Because, (a) if the web servers are behind an ELB, the load on the web servers will be uniformly distributed. Hence, if any of the web servers goes offline or becomes non-responsive, the traffic would be routed to other online web servers; making the application highly available, and (b) You can use Route53 to set the ALIAS record that points to the ELB endpoint.</p><p><img src="https://s3.amazonaws.com/awssap/2_1_1.PNG" alt="" width="522" height="534" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p></p>Option D is incorrect because AWS does not recommend to assign IP Addresses to ELB. The public IP addresses get automatically assigned to the ELB’s. You should always use the DNS name of the ELB.<p><a href="https://aws.amazon.com/elasticloadbalancing/"></a></p>	Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 ALIAS record to your CloudFront distribution. <br>  CloudFrontディストリビューションを構成し、WebサーバーのプライベートIPアドレスを指すように起点を構成します。 CloudFrontディストリビューションにRoute53 ALIASレコードを設定します。	Configure a NAT instance in your VPC Create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address. <br>  VPCでNATインスタンスを設定するNATインスタンスを介してデフォルトルートを作成し、それをすべてのサブネットに関連付けます。 NATインスタンスのパブリックIPアドレスを指すDNS Aレコードを設定します。	Configure ELB with an EIP. Place all your Web servers behind ELB Configure a Route53 A record that points to the EIP. <br> ELBをEIPで設定する。 すべてのWebサーバーをELBの背後に配置するEIPを指すRoute53 Aレコードを構成します。
Test2-02. <p>Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high-resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required that you may need to pay for a consultant.<br>How do you implement the most cost-efficient architecture without compromising high availability and quality of video delivery’?</p> | <p>あなたのウェブサイトは、オンデマンドのトレーニングビデオをあなたの労働力に提供しています。 動画は毎月高解像度のMP4形式でアップロードされます。 あなたの従業員は、ビデオを見るためにHTTPライブストリーミング（HLS）プロトコルを必要とする会社提供のタブレットを使用して、世界中で頻繁に配信されます。 あなたの会社はビデオトランスコーディングの専門知識を持っておらず、コンサルタントに支払う必要があるかもしれません。<br>ビデオ配信の高可用性と品質を犠牲にすることなく、コスト効率の高いアーキテクチャをどのように実装しますか？</p>	sa:	Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. Use S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. Use CloudFront to serve HLS transcoded videos from S3. <br>  Elastic Transcoderを使用してオリジナルの高解像度MP4ビデオをHLSにトランスコードします。 S3を使用してLifecycle Managementでビデオをホストし、数日後に元のファイルを氷河にアーカイブします。 CloudFrontを使用して、S3からHLSトランスコードされた動画を配信します。|<p><br></p><p></p><p>There are four most important design considerations here: (a)&nbsp;video transcoding expertise, (b) global distribution of the content, (c) cost-effective solution, and (d) no compromise with the high availability and quality of the video delivery.</p><p>Amazon Elastic Transcoder is a media transcoding service in the cloud. It is designed to be a highly scalable, easy to use and a cost-effective way for developers and businesses to convert (or “transcode”) media files from their source format into versions that will playback on various devices like smartphones, tablets, and PCs.</p><p><br></p><p>Option A is CORRECT because (a) it uses Amazon Elastic Transcoder that converts from MP4 to HLS, (b) S3 Object Lifecycle Management reduces the cost by archiving the files to Glacier, and (c) CloudFront - which is a highly available service - enables the global delivery of the video without compromising the video delivery speed or quality.</p><p>Option B is incorrect because (a) it necessitates the overhead of infrastructure provisioning. i.e deploying of EC2 instances, auto scaling, SQS queue / pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost efficient solution.</p><p>Option C is incorrect because the use of EBS snapshots is not a cost effective solution compared to S3 Object Lifecycle Management.</p><p>Option D is incorrect because&nbsp;(a) it necessitates the overhead of infrastructure provisioning. i.e deploying of EC2 instances, auto scaling, SQS queue / pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost efficient solution, and (d)&nbsp;the use of EBS snapshots is not a cost effective solution compared to S3 Object Lifecycle Management.</p><p>&nbsp;</p><p>For more information on Elastic Transcoder, please visit the below URL:<br></p><a href="https://aws.amazon.com/elastictranscoder/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/elastictranscoder/</a><br> <p></p><br><p><a href="https://aws.amazon.com/elastictranscoder/"></a></p>	A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number or nodes depending on the length of the queue. Use S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. Use CloudFront to serve HLS transcoding videos from Glacier <br> タスクを配布するためにSQSを使用してEC2上で実行されるビデオトランスコーディングパイプライン、およびキューの長さに応じて数またはノードを調整する自動スケーリング。 S3を使用してLifecycle Managementでビデオをホストすると、数日後にすべてのファイルを氷河にアーカイブすることができます。 CloudFrontを使用して、氷河からHLSトランスコードビデオを配信する	Elastic Transcoder to transcode original nigh-resolution MP4 videos to HLS EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. CloudFront to serve HLS transcoded videos from EC2. <br>  Elastic Transcoderを使用してオリジナルの高解像度MP4ビデオをHLS EBSボリュームにトランスコードし、ビデオおよびEBSスナップショットをホストして、数日後に元のルートを増分バックアップします。EC2からHLSトランスコードビデオを提供するCloudFront	A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days CloudFront to serve HLS transcoded videos from EC2 <br>  タスクを配布するためにSQSを使用するEC2上で実行されるビデオトランスコーディングパイプライン、および数日後に元のファイルを増分バックアップするEBSスナップショットをホストするキューEBSボリュームの長さに応じてノードの数を調整する自動スケーリング。 EC2からのHLSトランスコードビデオを提供するCloudFront
Test2-03. <p>Your company is hosting an application on the cloud. Your IT Security department has recently noticed that there seem to be some SQL Injection attacks against the application. Which of the below approach provides a cost-effective scalable mitigation to this kind of attack?</p> | <p>あなたの会社はクラウド上でアプリケーションをホストしています。あなたのITセキュリティ部門は、最近、アプリケーションに対するSQLインジェクション攻撃があるように見受けられました。以下のアプローチのどれが、この種の攻撃に対する費用効果の高いスケーラブルな軽減策ですか？</p>	sa:	Add a WAF tier by creating a new ELB and an AutoScaling group of EC2 Instances running a host-based WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would pass the traffic to the current web tier. The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group <br>  新しいELBを作成し、ホストベースのWAFを実行するEC 2インスタンスの自動スケーリンググループを作成して、WAFティアを追加します。彼らはRoute 53を新しいWAF層ELBに解決するようにリダイレクトします。WAF層は、トラフィックを現在のWeb層に渡します。Web層セキュリティグループは、WAF層セキュリティグループからのトラフィックのみを許可するように更新することができます|<p><br></p><p></p><p>In such scenarios where you are designing a solution to prevent the DDoS attack, always think of using Web Access Firewall (WAF).</p><p>AWS WAF is a web application firewall that helps protect your web&nbsp;applications from common web exploits that could affect application&nbsp;availability, compromise security, or consume excessive resources. AWS&nbsp;WAF gives you control over which traffic to allow or block to your web&nbsp;applications by defining customizable web security rules. You can use AWS&nbsp;WAF to create custom rules that block common attack patterns, such as SQL&nbsp;injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond&nbsp;quickly to changing traffic patterns.&nbsp;</p><p><br></p><p>Option A is incorrect because, although this option could work, the setup is very complex and it not a cost effective solution.</p><p>Option B is incorrect because, (a) even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and (b) it does not prevent the attack from the new source of threat.</p><p>Option C is CORRECT because (a) WAF Tiers acts as the first line of defense, it filters out the known sources of attack and blocks common attack patterns, such as SQL&nbsp;injection or cross-site scripting, (b) the ELB of the application is not exposed to the attack, and most importantly (c) this pattern - known as "WAF Sandwich" pattern - has WAF layer with EC2 instances are placed between two ELBs - one that faces the web, receives all the traffic, and sends them to WAF layer to filter out the malicious requests, and sends the filtered non-malicious requests, another ELB - which receives the non-malicious requests and send them to the EC2 instances for processing. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/2_3_1.png" alt="" width="1261" height="702" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p>Option D is incorrect because there is no such thing as&nbsp;Advanced Protocol Filtering feature for ELB.</p><p><br></p><p>For more information on WAF, please visit the below URL:</p>  <ul><li><span style="font-size: 1rem;"><a href="https://aws.amazon.com/waf/" target="_blank">https://aws.amazon.com/waf/</a></span></li></ul><p></p>	Add previously identified host file source IPs as an explicit INBOUND DENY NACL to the web tier subnet. <br>  以前に識別されたホストファイルのソースIPを明示的なINBOUND DENY NACLとしてWeb層サブネットに追加します。	Create a DirectConnect connection so that your have a dedicated connection line. <br>  専用の接続回線を持つようにDirectConnect接続を作成します。	Remove all but TLS 1 & 2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality. <br>  Web層ELBからTLS 1および2を除くすべてを削除し、Advanced Protocol Filteringを有効にします。これにより、ELB自体がWAF機能を実行できるようになります。
Test2-04. <p>Your IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your AWS resources. The solution must ensure the integrity and confidentiality of your log data. Which of these solutions would you recommend?</p> | <p> ITセキュリティコンプライアンスオフィサーは、AWSリソースに対する変更を追跡するための信頼性の高い耐久性のあるロギングソリューションを開発するように任命しました。 ソリューションでは、ログデータの整合性と機密性を確保する必要があります。 どちらのソリューションをお勧めしますか？</p>	sa:	Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option selected. Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs. <br>  1つの新しいS3バケットを使用して新しいCloudTrailトレイルを作成し、ログを保存し、グローバルサービスオプションを選択します。 ログを保存するS3バケット上で、IAMロールS3バケットポリシーとマルチファクタ認証（MFA）削除を使用します。|<p><span style="font-size: 1rem;">For the scenarios where the application is tracking (or needs to track) the changes made by any AWS service, resource, or API, always think about AWS CloudTrail service.</span></p><p>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticated AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets.&nbsp;</p><p>The most important points in this question are (a)&nbsp;S3 bucket with global services option enabled, (b) Data integrity, and (c) Confidentiality.</p><p>Option A is CORRECT because (a) it uses AWS CloudTrail with Global Option enabled, (b) a single new S3 bucket and IAM Roles so that it has the confidentiality, (c)&nbsp; MFA on Delete on S3 bucket so that it maintains the data integrity. See the AWS CloudTrail setting below which sets the Global Option.</p><p><img src="https://s3.amazonaws.com/awssap/2_4_1.png" alt="" width="808" height="184" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p>Options B is incorrect because (a) although&nbsp;it uses AWS CloudTrail,&nbsp;the Global Option is not enabled, and (b) SNS notifications can be a overhead in this situation.</p><p>Option C is incorrect because (a) as an existing S3 bucket is used, it may already be accessed to the user, hence not maintaining the confidentiality, and (b) it is not using IAM roles.</p><p>Option D is incorrect because (a)&nbsp;although&nbsp;it uses AWS CloudTrail,&nbsp;the Global Option is not enabled, and (b) three S3 buckets are not needed.</p><p><br></p><p>For more information on Cloudtrail, please visit the below URL:</p><p></p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events" target="_blank" style="font-size: 1rem;">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a><br><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a><br><p></p>  <br><p></p>	Create a new CloudTrail with one new S3 bucket to store the logs. Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs. <br>  1つの新しいS3バケットで新しいCloudTrailを作成し、ログを保存します。 ログファイル配信通知を管理システムに送信するようにSNSを設定します。 ログを格納するS3バケットにIAMロールとS3バケットポリシーを使用します。	Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the global services option selected. Use S3 ACLs and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs. <br>  既存のS3バケットを使用して新しいCloudTrailトレイルを作成して、ログを保存し、グローバルサービスオプションを選択します。 S3 ACLとMulti Factor Authentication（MFA）を使用して、ログを保存するS3バケットを削除します。	Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs. <br>  AWS管理コンソール用、AWS SDK用、コマンドラインツール用のログを格納する3つの新しいS3バケットを備えた3つの新しいCloudTrailトレイルを作成します。 ログを格納するS3バケットにIAMロールとS3バケットポリシーを使用します。
Test2-05. <p>A company has recently started using Docker cloud. This is a SaaS solution for managing Docker containers on the cloud. There is a requirement for the SaaS solution to access AWS resources. Which of the following options would meet the requirement in the most secured way assuming that the SaaS provider is also on AWS platform?</p> | <p>ある会社が最近Dockerクラウドを使用し始めました。これは、クラウド上のDockerコンテナを管理するためのSaaSソリューションです。SaaSソリューションがAWSリソースにアクセスするための要件が​​あります。SaaSプロバイダがAWSプラットフォーム上にあると仮定して、最も安全な方法で要件を満たす次のオプションはどれですか？</p>	sa:	Create an IAM role for cross-account access allows the SaaS provider’s account to assume the role and assign it a policy that allows only the actions required by the SaaS application. <br>  クロスアカウントアクセス用のIAMロールを作成すると、SaaSプロバイダのアカウントはその役割を引き受け、SaaSアプリケーションが要求するアクションのみを許可するポリシーを割り当てることができます。|<p><br></p><p></p><p>When a user, a resource, an application, or any service needs to access any AWS service or resource, always prefer creating appropriate role that has least privileged access or only required access, rather than using any other credentials such as keys.</p><p>Option A is incorrect because you should never share your access and secret keys.</p><p>Option B is incorrect because (a) when a user is created, even though it may have the appropriate policy attached to it, its security credentials are stored in the EC2 which can be compromised, and (b) creation of the appropriate role is always the better solution rather than creating a user.</p><p>Option C is CORRECT because AWS role creation allows cross-account access to the application to access the necessary resources. See the image and explanation below:</p><p>Many SaaS platforms can access AWS resources via a Cross-account access created in AWS. If you go to Roles in your identity management, you will see the ability to add a cross-account role.</p><p></p><br><p></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/2_5_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="703" height="290"></p><p><br></p><p></p><p>Option D is incorrect because the role is to be assigned to the application and it's resources, not the EC2 instances.</p><p>For more information on the cross-account role, please visit the&nbsp;below URL:</p>  <ul><li><span style="font-size: 1rem;"><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></span></li></ul><p></p><ul></ul><p></p>	Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application. Create a new access and secret key for the user and provide these credentials to the SaaS provider. <br>  エンタープライズアカウント内でIAMユーザーを作成すると、SaaSアプリケーションが要求するアクションのみを許可するユーザーポリシーをIAMユーザーに割り当てます。 ユーザの新しいアクセスと秘密鍵を作成し、これらの資格情報をSaaSプロバイダに提供します。	From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account. <br>  AWS Management Consoleから、Security Credentialsページに移動し、アカウントのアクセスと秘密鍵を取得します。。	Create an IAM role for EC2 instances, assign it a policy that allows only the actions required tor the Saas application to work, provide the role ARM to the SaaS provider to use when launching their application instances. <br>  EC2インスタンス用のIAMロールを作成し、Saasアプリケーションに必要なアクションだけを実行できるポリシーを割り当て、アプリケーションインスタンスを起動するときに使用するARMの役割をSaaSプロバイダに提供します。
Test2-06. <p>You have instances in a public subnet which downloads patches from the internet in addition to serving clients on the normal HTTP protocol. There is a requirement to ensure that just the serving protocol and the URL’s listed to get the patches are accessible from the instances. Which of the following options would you consider?</p> | <p>パブリックサブネットには、通常のHTTPプロトコルでクライアントにサービスを提供するだけでなく、インターネットからパッチをダウンロードするインスタンスがあります。パッチを取得するために表示されたサービスプロトコルとURLだけがインスタンスからアクセスできるようにする必要があります。次のうちどれを検討しますか？</p>	sa:	Configure a web proxy server in your VPC and enforce URL-based rules for outbound access. Remove default routes. <br>  VPC内にWebプロキシサーバを設定し、アウトバウンドアクセス用のURLベースのルールを適用します。既定のルートを削除します。|<p><br></p><p></p><p>There are 3 main considerations in this scenario: (a) the instances in your VPC needs internet access, (b) the access should be restricted for product updates only, and (c) all other outbound connection requests must be denied.</p><p>With such scenarios, you should not put your instances in public subnet as they would have access to internet without any restrictions. So, you should put them in a private subnet, and since there is a need of a logic for filtering the requests from client machines, configure a proxy server.</p><p><b>What is a Proxy Server?</b></p><p>Proxy server is a server that acts as a mediator between client(s) that sends requests and server that receives the requests and replies back. If any client requires any resources, it connects to the proxy server, and the proxy server evaluates the request based on its filtering rules. If the requests are valid, it connects to the server which receives the request and replies back. The proxy server also maintains cache; i.e., if any subsequent requests from same or other clients are received, it returns the result from the cache, saving the trip to and from the server. Hence, proxy servers tend to improve the performance. See the diagram below:</p><p><img src="https://s3.amazonaws.com/awssap/2_6_1.png" alt="" width="641" height="305" role="presentation" class="img-responsive atto_image_button_text-bottom"><br> <!--[if !supportLineBreakNewLine]--><br> <!--[endif]--></p><p>Option A is CORRECT because a proxy server (a) filters requests from the client, and allows only those that are related to the product updates, and (b) in this case helps filtering all other requests except the ones for the product updates.</p><p>Option B is incorrect because a security group cannot filter request based on URLs.</p><p>Option C is incorrect because even though moving the instances in a private subnet is a good idea, the routing table does not have the filtering logic, it only connects the subnets with internet gateway.</p><p>Option D is incorrect because a Network Access Control lists cannot filter request based on URLs.<br></p><p><span style="font-size: 1rem;">An example of setting up a proxy server can be found via the below URL:</span></p>  <ul><li><span style="font-size: 1rem;"><a href="https://aws.amazon.com/articles/6463473546098546" target="_blank">https://aws.amazon.com/articles/6463473546098546</a></span></li></ul><p></p>	Implement security groups and configure outbound rules to only permit traffic to the url’s. <br>  セキュリティグループを実装し、URLへのトラフィックのみを許可するように送信ルールを構成します。	Move all your instances into private VPC subnets. Remove default routes from all routing tables and add specific routes to the software depots and distributions only. <br>  すべてのインスタンスをプライベートVPCサブネットに移動します。すべてのルーティングテーブルからデフォルトルートを削除し、特定のルートをソフトウェアデポと配布にのみ追加します。	Implement network access control lists to all specific destinations, with an Implicit deny as a rule. <br>  暗黙の拒否を原則としてすべての特定の宛先にネ​​ットワークアクセス制御リストを実装します。
Test2-07. <p>Your company has recently extended its datacenter into a VPC on AWS. There is a requirement for on-premise users manages AWS resources from the AWS console. You don’t want to create IAM users for them again. Which of the below options will fit your needs for authentication?</p> | <p>あなたの会社は最近、データセンターをAWS上のVPCに拡張しました。オンプレミスユーザがAWSコンソールからAWSリソースを管理する必要があります。IAMユーザーを再度作成する必要はありません。あなたの認証ニーズに合ったオプションは次のうちどれですか？</p>	sa:	Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint. <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバーにAWSシングルサインオン（SSO）エンドポイント経由でAWS管理コンソールへのフェデレーションアクセスを許可します。|<p><br></p><p></p><p>This scenario has two requirements: (a) temporary access to AWS resources be given to certain users or application (NOC members in this case), and (b) you are not supposed to create new IAM users for the NOC members to log into AWS console.&nbsp;</p><p>This scenario is handled by a concept named "Federated Access". Read this for more information on federated access:&nbsp;</p><p></p><a href="https://aws.amazon.com/identity/federation/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/identity/federation/</a><br><p></p><p>Read this article for more information on how to establish the federated access to the AWS resources:</p><p></p><a href="https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/</a><br><p></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because OAuth 2.0 is not applicable in this scenario as we are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google etc.</span></p><p>Option B is incorrect because we are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google etc.</p><p>Option C is CORRECT because (a) it gives a federated access to the NOC members to AWS resources by using SAML 2.0 identity provider, and (b) it uses on-premise single sign on (SSO) endpoint to authenticate users and gives them access tokens prior to providing the federated access.</p><p>Option D is incorrect because, even though it uses SAML 2.0 identity provider, one of the requirements is not to let users sign in to AWS console using any security credentials.</p>  See this diagram that explains the Federated Access using SAML 2.0<br><p></p><p><img src="https://s3.amazonaws.com/awssap/2_7_1.png" alt="" width="1244" height="716" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p>	Use web Identity Federation to retrieve AWS temporary security credentials to enable your members to sign in to the AWS Management Console. <br>  Web ID Federationを使用してAWSの一時的なセキュリティ資格情報を取得し、メンバーがAWS Management Consoleにサインインできるようにします。	Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your members to sign in to the AWS Management Console. <br>  OAuth 2.0を使用して、メンバがAWS管理コンソールにサインインできるように、一時的なAWSセキュリティ資格情報を取得します。	Use your on-premises SAML 2.0-compliant identity provider (IDP) to retrieve temporary security credentials to enable members to sign in to the AWS Management Console. <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバがAWS管理コンソールにサインインできるように一時的なセキュリティ資格情報を取得します。
Test2-08. <p>What are the benefits of using an IPSec tunnel from connecting from an on-premise location to AWS?</p> <p>Choose 4 correct options from the below:<br></p> | <p>オンプレミスの場所からAWSに接続する際にIPSecトンネルを使用する利点は何ですか？</p> <p>以下の4つの正しいオプションを選択してください：<br> </p>	ma:	x:End-to-end protection of data in transit <br>  通過中のデータのエンドツーエンド保護|<p><br></p><p></p><p>IPSec is designed to provide authentication, integrity, and confidentiality of the data that is being transmitted. IPSec operates at network layer of the OSI model. Hence, it only protects the data that is in transit over the internet. For the full security of the data transmission it is very essential that both the sender and receiver need to be IPSec-aware.</p><p>See the diagram of this scenario:</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/2_8_1.png" alt="" width="642" height="481" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p><br></p><p>Option A is incorrect because (a) IPSec operates at network layer of the OSI model. Hence, it only protects the data that is in transit over the internet, and (b) both the source and the destination (client and server) may not be IPSec aware.</p><p>Option B is incorrect because the identity authentication of the origin of the data has to be done at the application layer, not the network layer.</p><p>Option C is CORRECT because the data that is transiting via the IPSec tunnel is encrypted.</p><p>Option D is CORRECT because IPSec protects the data that is in transit over the internet (fundamental responsibility of IPSec tunnel).</p><p>Option E is CORRECT because in this scenario, the IPSec tunnel is established between VPN gateway (VPG) and Customer Gateway (CGW) whose identity gets authenticated during the setup of the IPSec tunnel.</p><p>Option F is CORRECT because - as mentioned earlier - integrity of the data that is transiting via the IPSec tunnel is always preserved (fundamental responsibility of IPSec tunnel).</p><p><br></p><p>For more information on IPSec tunnel, please refer to:</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://techgenix.com/securing_data_in_transit_with_ipsec/" target="_blank">http://techgenix.com/securing_data_in_transit_with_ipsec/</a></span></li></ul> <!--[if !supportLineBreakNewLine]--><br> <!--[endif]--><p></p><p>The below link provides an article on the general working of an IPSec tunnel which outlines the advantages of an IPSec tunnel which includes:</p>  <ul><li><span style="font-size: 1rem;"><a href="http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html" target="_blank">http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html</a></span></li></ul><p></p>	x:End-to-end Identity authentication <br>  エンドツーエンドのID認証	o:Data encryption across the Internet <br>  インターネットを介したデータの暗号化	o:Protection of data in transit over the Internet <br>  インターネット上を通過するデータの保護	o:Peer identity authentication between VPN gateway and customer gateway <br>  VPNゲートウェイと顧客ゲートウェイ間のピア識別認証	o: Data integrity protection across the Internet <br>  インターネットを介したデータの完全性の保護
Test2-09. <p>What should you consider when you try to implement an IDS infrastructure on AWS?</p> <p>Choose 2 correct options from the below:</p> | <p> AWSでIDSインフラストラクチャを実装しようとする際には、どのような点を考慮する必要がありますか？</p> <p>以下の2つの正しいオプションを選択してください：</p>	ma:	o:Implement IDS/IPS agents on each Instance running In VPC <br>  VPCで動作する各インスタンスでIDS / IPSエージェントを実装する|<p><br></p><p></p><p>The main responsibility of Intrusion Detection Systems (IDS) / Intrusion Prevention Systems (IPS) is to (a) detect the vulnerabilities in your EC2 instances, (b) protect your EC2 instances from attacks, and (c) respond to intrusion or attacks against your EC2 instances.</p><p>The IDS is an appliance that is installed on the EC2 instances that continuously monitors the VPC environment to see if any malicious activity is happening and alerts the system administration if such activity is detected. IPS, on the other hand, is an appliance that is installed on the EC2 instances that monitors and analyzes the incoming and outgoing network traffic for any malicious activities and prevents the malicious requests from reaching to the instances in the VPC.</p><p>This scenario is asking you how you can setup IDS/IPS in your VPC. There are few well known ways: (a) install the IDS/IPS agents on the EC2 instances of the VPC, so that the activities of that instance can be monitored, (b) set up IDS/IPS on a proxy server/NAT through which the network traffic is flowing, or (c) setup a Security-VPC that contains EC2 instances with IDS/IPS capability and peer that VPC with your VPC and always accept the traffic from Security-VPC only.</p><p><br></p><p>Option A is CORRECT because&nbsp; it implements the IDS/IPS agents on each EC2 instances in the VPC.</p><p>Option B is incorrect because promiscuous mode is not supported by AWS.</p><p>Option C is incorrect because ELB with SSL is does not have the intrusion detection/prevention capability.</p><p>Option D is CORRECT because a reverse proxy server through which the traffic from instances inside VPC flows outside of it, has the IDS/IPS agent installed.</p><p><br></p><p>For more information on intrusion detection systems in AWS, please refer to the below link:</p>  <ul><li><span style="font-size: 1rem;"><a href="https://awsmedia.s3.amazonaws.com/SEC402.pdf" target="_blank">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a></span></li></ul><p></p><ul></ul><p></p>	x:Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic. <br>  各サブネットでインスタンスを設定して、ネットワークインターフェイスカードをプロミスキャスモードに切り替え、ネットワークトラフィックを分析します。	x:Implement Elastic Load Balancing with SSL listeners In front of the web applications <br>  SSLリスナーによるロードバランシングの実装Webアプリケーションの前で	o:Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server. <br>  Webサーバーの前にリバースプロキシレイヤを実装し、各リバースプロキシサーバーでIDS / IPSエージェントを構成します。
Test2-10. <p>An application store a set of files in a single Amazon S3 bucket. Users will upload files from their mobile device directly to Amazon S3 and will be able to view and download their uploaded files directly from Amazon S3. You want to configure security to handle potentially millions of users in the most secure manner possible. What should your server-side application do when a new user registers on the mobile application?</p> |  <p>アプリケーションは、1組のファイルを1つのAmazon S3バケットに格納します。 ユーザーはモバイルデバイスからAmazon S3に直接ファイルをアップロードし、アップロードしたファイルをAmazon S3から直接表示してダウンロードすることができます。 可能な限り最も安全な方法で、潜在的に何百万人ものユーザーを処理するようにセキュリティを構成する必要があります。 新規ユーザーがモバイルアプリケーションに登録したときに、サーバーサイドアプリケーションはどのようにすべきですか？</p>	sa:	Record the user’s Information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses their mobile app create temporary credentials using the AWS Security Token Service ‘AssumeRole’ function, store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app. <br>  ユーザーの情報をAmazon RDSに記録し、適切な権限でIAMに役割を作成します。 ユーザーがモバイルアプリケーションを使用して、AWSセキュリティトークンサービス 'AssumeRole'機能を使用して一時的な資格情報を作成するときは、これらの資格情報をモバイルアプリのメモリに保存し、それらを使用してAmazon S3にアクセスします。 次回ユーザーがモバイルアプリを実行したときに新しい認証情報を生成します。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-f6d1-a97d-ce4e-ec7294a017fd"></span></p><p dir="ltr" style="">This scenario requires the mobile application to have access to S3 bucket. There are potentially millions of users and a proper security measure should be taken. In such question, where mobile applications needs to access AWS Resources, always think about using funtions such as "AssumeRole", "AssumeRoleWithSAML", and "AssumeRoleWithWebIdentity". See the following diagram that explains the flow of actions while using "AssumeRole".</p><br><p dir="ltr" style="">You can let users sign in using a well-known third-party identity provider such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider. You can exchange the credentials from that provider for temporary permissions to use resources in your AWS account. This is known as the&nbsp;web identity federation&nbsp;approach to temporary access. When you use web identity federation for your mobile or web application, you don't need to create custom sign-in code or manage your own user identities. Using web identity federation helps you keep your AWS account secure because you don't have to distribute long-term security credentials, such as IAM user access keys, with your application.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because you should always grant the short term or temporary credentials for the mobile application. This option asks to create a long term credentials.</p><p dir="ltr" style="">Option B is CORRECT because (a) it creates an IAM Role with appropriate permissions, (b) it generates temporary security credentials using STS "AssumeRole" function, and (c) it generates new credentials when the user runs the app the next time.</p><p dir="ltr" style="">Option C is incorrect because, even though the set up is very similar to option B, it does not create IAM Role with proper permissions which is an essential step.</p><p dir="ltr" style="">Option D is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create a IAM Role so that the app can access the AWS Resource via "AssumeRole" function.</p><p dir="ltr" style="">Option E is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create a IAM Role so that the app can access the AWS Resource via "AssumeRole" function.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on AWS temporary credentials, please refer to the below link:</p><ul><li><span style="font-size: 1rem;"><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></span></li><li><a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html" target="_blank" style="font-size: 1rem;">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></li></ul><br><p></p>	Create a set of long-term credentials using AWS Security Token Service with appropriate permissions Store these credentials in the mobile app and use them to access Amazon S3. <br>  適切な権限を持つAWS Security Token Serviceを使用して長期資格情報のセットを作成するこれらの資格情報をモバイルアプリケーションに格納し、それらを使用してAmazon S3にアクセスします。	Record the user’s Information In Amazon DynamoDB. When the user uses their mobile app create  temporary credentials using AWS Security Token Service with appropriate permissions, store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app. <br>  Amazon DynamoDBでユーザーの情報を記録します。 ユーザーがモバイルアプリケーションを使用して適切な権限を持つAWS Security Token Serviceを使用して一時的な資格情報を作成する場合は、モバイルアプリのメモリにこれらの資格情報を格納し、それらを使用してAmazon S3にアクセスします。 次回ユーザーがモバイルアプリを実行したときに新しい認証情報を生成します。	Create IAM user. Assign appropriate permissions to the IAM user Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3. <br>  IAMユーザーを作成します。 IAMユーザーに適切な権限を割り当てるIAMユーザーのアクセスキーと秘密キーを生成し、モバイルアプリに格納し、これらの資格情報を使用してAmazon S3にアクセスします。	Create an IAM user. Update the bucket policy with appropriate permissions for the IAM user. Generate an access Key and secret Key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3. <br>  IAMユーザーを作成します。 バケットポリシーをIAMユーザの適切な権限で更新します。 IAMユーザーのアクセスキーと秘密鍵を生成し、モバイルアプリケーションに格納し、これらの資格情報を使用してAmazon S3にアクセスします。
Test2-11. <p>You have an application running on an EC2 Instance access an S3 bucket. How should the application use AWS credentials to access the S3 bucket securely?</p> | <p> EC2インスタンス上で実行されているアプリケーションでS3バケットにアクセスしています。アプリケーションがAWS認証を使用してS3バケットに安全にアクセスするにはどうすればよいですか？</p>	sa:	Create an IAM role for EC2 that allows list access to objects in the S3 bucket. Launch the instance with the role, and retrieve the role’s credentials from the EC2 Instance metadata <br>  ロールを使用してインスタンスを起動し、EC 2インスタンスのメタデータからロールの資格情報を取得します|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-029e-1d00-5388-4da1fa2e4262"></span></p><p dir="ltr" style="">An IAM&nbsp;role&nbsp;is similar to a user. In that, it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have any credentials (password or access keys) associated with it. Instead, if a user is assigned to a role, access keys are created dynamically and provided to the user.</p><p dir="ltr" style="">You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.</p><p dir="ltr" style="">Whenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User. The reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service. This has security concerns. Whereas, when an IAM Role is created, it has all the necessary policies attached to it. So, the use of access key and secret key is not needed. This is the preferred approach.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because you should not use the account access keys , instead you should use the IAM Role.</p><p dir="ltr" style="">Option B is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p dir="ltr" style="">Option C is CORRECT because, (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.</p><p dir="ltr" style="">Option D is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on IAM roles, please visit the below URL:</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><br><br><p></p><ul></ul><p></p>	Create an IAM user for the application with permissions that allow list access to the S3 bucket launch the instance as the IAM user and retrieve the IAM user’s credentials from the EC2 instance user data. <br>  S3バケットにアクセスしてアプリケーション用のIAMユーザーを作成し、IAMユーザーとしてインスタンスを起動し、EC 2インスタンスのユーザーデータからIAMユーザーの資格情報を取得します。	Use the AWS account access Keys. The application retrieves the credentials from the source code of the application. <br>  AWSアカウントのアクセスキーを使用します。アプリケーションは、アプリケーションのソースコードから資格情報を取得します。	Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. <br>  アプリケーションは、アプリケーションを取得し、そのアプリケーションのユーザーのみに読み取りアクセスを許可するアクセス許可を使用して、一時ディレクトリからIAMユーザーの資格情報を取得します。
Test2-12. <p>Which of the following are the recommendations from AWS when migrating a legacy application which is hosted on a virtual machine in an on-premise location?</p> <p>Choose 2 options from the below:</p> | <p>オンプレミスの場所にある仮想マシンでホストされているレガシーアプリケーションを移行する場合、AWSの推奨事項はどれですか？</p> <p>以下の2つのオプションを選択してください：</p>	ma:	x:Use a NAT instance to route traffic from the instance in the VPC. <br>  NATインスタンスを使用して、VPC内のインスタンスからトラフィックをルーティングします。|<p><br></p><p>Option A is incorrect because having NAT instance is not going to help in this scenario. NAT instance is used so that the instances in the private subnet can communicate with the internet.&nbsp;</p><p>Option B is CORRECT because using an elastic IP address you can mask the failure of an instance or the legacy app in this case by remapping the IP address to another functioning instance in a VPC subnet.&nbsp;</p><p>Option C is incorrect because Route 53 cannot resolve any dependencies on the IP addresses.&nbsp;</p><p>Option D is CORRECT because VM Import/Export enables you to easily import VM images from the on-premise location to the VPC in the form of EC2 instances, hence helping the migration of the legacy application.</p>	o:Use an Elastic IP address on the VPC instance <br>  VPCインスタンスでエラスティックIPアドレスを使用する	x:Use entries in Amazon Route 53 that allow the Instance to resolve its dependencies’ IP addresses on the on-premise location <br>  Amazon Route 53のエントリを使用して、インスタンスがオンプレミスロケーションの依存関係のIPアドレスを解決できるようにします	o:Use the VM Import facility provided by aws. <br>  awsが提供するVMインポート機能を使用します。
Test2-13. <p>You are designing an application and are considering how to mitigate distributed denial-of-service (DDoS) attacks. Which of the below are viable mitigation techniques?</p> <p>Choose 3 options from the below:</p> | <p>アプリケーションを設計しており、分散サービス拒否（DDoS）攻撃を軽減する方法を検討しています。以下のうちどれが実行可能な緩和手法ですか？</p> <p>以下の3つのオプションを選択してください：</p>	ma:	x:Add multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth. <br>  各EC 2インスタンスに複数のENI（elastic network interface）を追加してネットワーク帯域幅を拡大する。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-02a0-c855-cf85-eb8c9089c547"></span></p><p dir="ltr" style="">This question is asking you to select some of the most recommended and widely used DDoS mitigation techniques.</p><p dir="ltr" style="">What is a DDoS Attack?</p><p dir="ltr" style="">A Distributed Denial of Service (DDoS) attack is an attack orchestrated by distributed multiple sources that makes your web application unresponsive and unavailable for the end users.</p><p dir="ltr" style="">DDoS Mitigation Techniques</p><p dir="ltr" style="">Some of the recommended techniques for mitigating the DDoS attacks are&nbsp;</p><p dir="ltr" style="">(i) build the architecture using the AWS services and offerings that have the capabilities to protect the application from such attacks. e.g. CloudFront, WAF, Autoscaling, Route53, VPC etc.</p><p dir="ltr" style="">(ii) defend the infrastructure layer by over-provisioning capacity, and deploying DDoS mitigation systems.</p><p dir="ltr" style="">(iii) defend the application layer by using WAF, and operating at scale by using autoscale so that the application can withstand the attack by scaling and absorbing the traffic.</p><p dir="ltr" style="">(iv) minimizing the surface area of attack</p><p dir="ltr" style="">(v) obfuscating the AWS resources</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because ENIs do not help in increasing the network bandwidth.</p><p dir="ltr" style="">Option B is incorrect because having dedicated instances performing at maximum capacity will not help mitigating the DDoS attack. What is needed is instances behind auto-scaling so that the traffic can be absorbed while actions are being taken on the attack and the application can continue responding to the clients.</p><p dir="ltr" style="">Option C is CORRECT because (a) CloudFront is AWS managed service and it can scale automatically, (b) helps absorbing the traffic, and (c) it can help putting restriction based on geolocation. i.e. if the attack is coming from IPs from specific location, such requests can be blocked.</p><p dir="ltr" style="">Option D is CORRECT because (a) ELB helps distributing the traffic to the instances that are part of auto-scaling (helps absorbing the traffic), and (b) Amazon RDS is an Amazon managed service which can withstand the DDoS attack.</p><p dir="ltr" style="">Option E is CORRECT because CloudWatch can help monitoring the network traffic as well as CPU Utilization for suspicious activities.</p><p dir="ltr" style="">Option F is incorrect because adding and removing rules of firewall is not going to mitigate the DDoS attack.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">It is very important to read the AWS Whitepaper on Best Practices for DDoS Resiliency.</p><p dir="ltr" style=""><a href="https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><br><br><p></p><p></p>	x:Use dedicated instances to ensure that each instance has the maximum performance possible. <br>  専用インスタンスを使用して、各インスタンスが最大のパフォーマンスを発揮できるようにします。	o:Use an Amazon CloudFront distribution for both static and dynamic content. <br>  静的コンテンツと動的コンテンツの両方にAmazon CloudFrontディストリビューションを使用します。	o:Use an Elastic Load Balancer with auto scaling groups at the web, App tiers; also use Amazon Relational Database Service (RDS)? <br>  Web、アプリケーション層で自動スケーリンググループを持つElastic Load Balancerを使用します。Amazon Relational Database Service（RDS）も使用しますか？	o:Add alert Amazon CloudWatch to look for high Network in and CPU utilization. <br>  Amazon CloudWatchにアラートを追加して、ネットワークとCPU使用率の高いネットワークを探します。	x: Create processes and capabilities to quickly add and remove rules to the instance OS firewall. <br>  インスタンスOSファイアウォールにルールを迅速に追加および削除するためのプロセスと機能を作成します。
Test2-14. <p>A company has a web application hosted on AWS. The IT Security Administrator has noticed that a lot of requests are coming from a set of IPs. As an AWS professional, what can you do to ensure that this type of attack is limited?</p> | <p>ある企業にAWSでホストされているWebアプリケーションがあります。ITセキュリティ管理者は、多くの要求が一連のIPから来ていることに気付いています。この種の攻撃が制限されていることを確認するには、AWSのプロフェッショナルとして、どうすればよいですか？</p>	sa:	Create an inbound NACL (Network Access control list) associated with the web tier subnet with deny rules to block the attacking IP addresses <br>  攻撃者のIPアドレスをブロックする拒否ルールを持つWeb層サブネットに関連付けられた受信ネットワークアクセス制御リスト（NACL）を作成する|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-02a2-e434-2bc4-e2f9020c217d"></span></p><p dir="ltr" style="">In this scenario, the attack is coming from a set of certain IP addresses over specific port from a specific country. You are supposed to defend against this attack.&nbsp;</p><p dir="ltr" style="">In such questions, always think about two options: Security groups and Network Access Control List (NACL). Security Groups operate at the individual instance level, whereas NACL operates at subnet level. You should always fortify the NACL first, as it is encounter first during the communication with the instances in the VPC.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because IP addresses cannot be blocked using route table or IGW.</p><p dir="ltr" style="">Option B is incorrect because <span id="docs-internal-guid-6c5f78f5-02a4-20f1-1190-48c9e6a393b1">(a) you cannot deny port access using security groups, and (b) by default all requests are denied; you open access for particular IP address or range. You cannot deny access for particular IP addresses using security groups.</span></p><p dir="ltr" style="">Option C is incorrect because if the application servers are put in the private subnet, the application will not be accessible from the internet, especially since the option does not mention any public facing ELB or Route 53 configuration.</p><p dir="ltr" style="">Option D is CORRECT because (a) you can add deny rules in NACL and block access to certain IP addresses. See an example below:</p><img src="https://s3.amazonaws.com/awssap/2_14_1.png" alt="" width="762" height="326" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><br><p></p><ul></ul><p></p>	Create web Security Group rules to block the attacking IP addresses over port 80 <br>  ポート80を介して攻撃するIPアドレスをブロックするWebセキュリティグループルールを作成する	Put the application on the private subnet. <br>  アプリケーションをプライベートサブネットに配置します。	Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (internet Gateway) <br>  Web層に関連付けられたカスタムルートテーブルを作成し、インターネットゲートウェイ（IGW）からの攻撃IPアドレスをブロックします。
Test2-15. <p>Which of the following options will you need to consider so you can set up a solution that incorporates single sign-on from your corporate AD or LDAP directory and restricts access for each user to a designated user folder in a bucket?</p> <p>Choose 3 options from the below:</p> | <p>企業のADディレクトリまたはLDAPディレクトリからのシングルサインオンを組み込んだソリューションを設定し、各ユーザーのアクセスをバケット内の指定されたユーザーフォルダに制限するソリューションを設定できるようにするには、次のうちどれを選択する必要がありますか？ p> <p>下記の3つのオプションを選択してください：</p>	ma:	o:Setting up a federation proxy or identity provider <br>  フェデレーションプロキシまたはアイデンティティプロバイダを設定する|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-02b9-05f3-ab56-a3224c18196d"></span></p><p dir="ltr" style="">In questions like this where an application, or user needs to be given access using Single Sign On (SSO), following steps are very important:</p><p dir="ltr" style="">(i) setting up a identity provider for federated access</p><p dir="ltr" style="">(ii) authenticating users using corporate data store / active directory-user-attributes/</p><p dir="ltr" style="">(iii) getting temporary access tokens / credentials using AWS STS</p><p dir="ltr" style="">(iv) creating the IAM Role that has the access to the needed AWS Resources</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is CORRECT because as mentioned above, setting up a identity provider for federated access is needed.</p><p dir="ltr" style="">Option B is CORRECT because as mentioned above, getting temporary access tokens / credentials using AWS STS is needed.</p><p dir="ltr" style="">Option C is incorrect because tagging each folder in bucket does not help in this scenario.</p><p dir="ltr" style="">Option D is CORRECT because as mentioned above, creating the IAM Role that has the access to the needed AWS Resources is needed.</p><p dir="ltr" style="">Option E is incorrect because (a) you should be creating IAM Roles rather than IAM Users.&nbsp;</p><p dir="ltr" style="">The diagram below showcases how authentication is carried out when having an identity broker. This is an example of a SAML connection , but the same concept holds true for getting access to an AWS resource.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style=""><span style="font-size: 1rem;">&nbsp;</span><img src="https://s3.amazonaws.com/awssap/2_15_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="765" height="380" style="font-size: 1rem;"></p><p></p><p><span id="docs-internal-guid-6c5f78f5-02ba-321f-7d5a-7ce4cdc23d3a"></span></p><p dir="ltr" style="">For more information on federated access, please visit the below link:</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><br><p></p><ul style="font-size: 1rem;"></ul><p></p>	o:Using AWS Security Token Service to generate temporary tokens <br>  AWSセキュリティトークンサービスを使用して一時的なトークンを生成する	x:Tagging each folder in the bucket <br>  バケット内の各フォルダにタグを付ける	o:Configuring IAM role <br>  IAMロールの設定	x:Setting up a matching IAM user for every user in your corporate directory that needs access to a folder in the bucket <br>  社内ディレクトリ内のバケット内のフォルダにアクセスする必要があるすべてのユーザーに対応するIAMユーザーを設定する
Test2-16. <p>When one creates an encrypted EBS volume and attach it to a supported instance type, which of the following data types are encrypted?</p> <p>Choose 3 options from the below:</p> | </p> <p>暗号化されたEBSボリュームを作成し、それをサポートされているインスタンスタイプに添付すると、	ma:	o:Data at rest inside the volume <br>  ボリューム内の静止データ|<p><br></p><p>Amazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p><p><span style="font-size: 1rem;">(i) Data at rest inside the volume</span></p><p>(ii) All data moving between the volume and the instance</p><p>(iii) All snapshots created from the volume</p><p>(iv) All volumes created from those snapshots</p><p><br></p><p>Based on this, options A, B, and D are all CORRECT.</p><p>Option B is incorrect since the data that is copied to S3 is not encrypted.</p><p><br></p><p>For more information on this, please visit the link below:</p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a><br></p>	x:All data copied from the EBS volume to S3 <br>  EBSボリュームからS3にコピーされたすべてのデータ	o:All data moving between the volume and the instance <br>  ボリュームとインスタンス間を移動するすべてのデータ	o:All snapshots created from the volume <br>  ボリュームから作成されたすべてのスナップショット
Test2-17. <p>You have a periodic image analysis application that gets some files. The input stream analyzes them and for each file, it writes some data to an output stream to a number of files. The number of files in input per day is high and concentrated in a few hours of the day. Currently, you have a server on EC2 with a large EBS volume that hosts the input data and the results it takes almost 20 hours per day to complete the process</p><p><span style="font-size: 1rem;">What services could be used to reduce the elaboration time and improve the availability of the solution?</span></p> | <p>いくつかのファイルを取得する定期的な画像解析アプリケーションがあります。 入力ストリームはそれらを解析し、ファイルごとにいくつかのデータを出力ストリームにいくつかのファイルに書き込みます。 1日あたりの入力ファイル数は高く、1日の数時間で集中します。 現在、入力データをホストする大きなEBSボリュームを持つEC2上にサーバーがあり、結果を処理するには1日に約20時間かかります</p> <p> <span style = "font-size：1rem; ">どのようなサービスを使用して熟練の時間を短縮し、ソリューションの可用性を向上させることができますか？</ span> </p>	sa:	S3 to store I/O files. SQS to distribute elaboration commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue <br>  I / Oファイルを格納するためのS3。SQSを使用して、並行して動作するホストのグループに詳細なコマンドを配布します。SQSキューの長さに応じてホストのグループを動的にサイズ変更する自動スケーリング|<p></p><div></div><br><span id="docs-internal-guid-6c5f78f5-02bc-7ad1-4b11-1e6431c82268"><p dir="ltr" style="">The scenario in this question is that (a) there any EC2 instances that need to process high number of input files, (b) currently the processing takes 20 hrs a day, which needs to be reduced, (c) the availability needs to be improved.</p><p dir="ltr" style="">Looking at all the option, it appears that there are two choices to be made. (1) between S3 and EBO with PIOPS, and (2) between SQS and SNS.</p><p dir="ltr" style="">First, let's see whether we should choose S3 or EBS with PIOPS. It appears that all the options have auto-scaling in common. i.e. there will be multiple EC2 instances working in parallel on the input data.&nbsp; This should reduce the overall elaboration time, satisfying one of the requirements. Since a single EBS volume cannot be attached to multiple instances, using EBS volume seems an illogical choice. Moreover, S3 provides high availability, which satisfies the other requirement.&nbsp;</p><p dir="ltr" style="">Second, SQS is a great option to do the autonomous tasks and can queue the service requests and can be scaled to meet the high demand. SNS is a mere notification service and would not hold the tasks. Hence, SQS is certainly the correct choice.&nbsp;</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is CORRECT because, as mentioned above, it provides high availability, and can store the massive amount of data. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps distributing the commands/tasks to the group of EC2 instances.</p><p dir="ltr" style="">Option B is incorrect because, as mentioned above, neither EBS nor SNS is a valid choice in this scenario.</p><p dir="ltr" style="">Option C is incorrect because, as mentioned above, SNS is not a valid choice in this scenario.</p><p dir="ltr" style="">Option D is incorrect because, as mentioned above, EBS is not a valid choice in this scenario.</p></span><br>	EBS with Provisioned IOPS (PIOPS) to store I/O files. SNS to distribute elaboration commands to a group of hosts working in parallel Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications <br>  I / Oファイルを格納するためのプロビジョニングIOPS（PIOPS）を備えたEBS。SNSは、SNS通知の数に応じてホストのグループのサイズを動的に調整するために、パラレル自動スケーリングで動作するホストグループにエラボレーションコマンドを配信します	S3 to store I/O files, SNS to distribute evaporation commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the number of SNS notifications <br>  S3はI / Oファイルを格納し、SNSは蒸発コマンドを並行して動作するホストのグループに分配する。SNS通知の数に応じてホストのグループのサイズを動的に変更する自動スケーリング	EBS with Provisioned IOPS (PIOPS) to store I/O files SQS to distribute elaboration commands to a group of hosts working in parallel. Use Auto Scaling to dynamically size the group of hosts depending on the length of the SQS queue <br>  I / Oファイルを格納するためのプロビジョニングされたIOPS（PIOPS）を持つEBS SQSキューのサイズに応じてホストのグループにエラボレーションコマンドを配信するSQS
Test2-18. <p>A company has the requirement to analyze the clickstreams from a web application. Which of the below options will fulfill this requirement?</p> | <p>企業では、ウェブアプリケーションからクリックストリームを分析する必要があります。次のうち、この要件を満たすオプションはどれですか？</p>	sa:	Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers <br>  Amazon KinesisにセッションごとにWebクリックをプッシュし、Kinesisワーカーを使用して動作を分析する|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-02bf-9983-d010-5467e04b4d54"></span></p><p dir="ltr" style="">Whenever the question presents a scenario where the application needs to do analysis on real time data such as clickstream (i.e.massive real-time data analysis), most of the time the best option is Amazon Kinesis. It is used to collect and process large&nbsp;<a href="https://aws.amazon.com/streaming-data/" style="">streams</a>&nbsp;of data records in real time.</p><p dir="ltr" style="">You'll create data-processing applications, known as&nbsp;Amazon Kinesis Streams applications. A typical Amazon Kinesis Streams application reads data from an&nbsp;Amazon Kinesis stream&nbsp;as data records. These applications can use the Amazon Kinesis Client Library, and they can run on Amazon EC2 instances. The processed records can be sent to dashboards, used to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services</p><p dir="ltr" style="">The below diagrams from the aws documentation shows how you can create custom streams in Amazon Kinesis.</p><img src="https://s3.amazonaws.com/awssap/2_18_1.png" alt="" width="1253" height="267" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><br><img src="https://s3.amazonaws.com/awssap/2_18_2.png" alt="" width="1102" height="474" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on Kinesis, please visit the below link:</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">http://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><br><p></p><ul></ul><p></p>	Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce <br>  WeblogsのURLをURLでログに記録してAmazon S3に保存し、Elastic MapReduceで分析する	Write click events directly to Amazon Redshift and then analyze with SQL <br>  クリックイベントをAmazon Redshiftに直接書き込んでからSQLで分析する	Publish web clicks by session to an Amazon SQS queue and periodically drain these events to Amazon RDS.?Then, analyze with SQL <br>  セッションごとのWebクリックをAmazon SQSキューに公開し、これらのイベントをAmazon RDSに定期的に配信します。その後、SQLで分析します
Test2-19. <p>You currently have a placement group of instances. When you try to add new instances to the group, you receive a 'capacity error'. Which of the following actions will most likely fix this problem? Choose the correct option from the below:</p> | <p>現在インスタンスのプレースメントグループがあります。グループに新しいインスタンスを追加しようとすると、「容量エラー」が発生します。この問題を解決する可能性が最も高いのは次のうちどれですか？下記から正しいオプションを選択してください：</p>	sa:	Stop and restart the instances in the Placement Group and then try the launch again. <br>  プレースメントグループ内のインスタンスを停止してから再起動し、再度起動してください。|<p><br></p><p><span id="docs-internal-guid-86cab175-02c4-61fd-eccf-1e2f75f52fa3"></span></p><p dir="ltr" style="">Option A is incorrect because to benefit from the enhanced networking, all the instances should be in the same Placement Group. Launching the new ones in a new Placement Group will not work in this case.</p><p dir="ltr" style="">Option B is CORRECT because the most likely reason for the "Capacity Error" is that the underlying hardware may not have the capacity to launch any additional instances on it. If the instances are stopped and restarted, AWS may move the instances to a hardware that has capacity for all the requested instances.</p><p dir="ltr" style="">Option C is incorrect because there is no such limit on the number of instances in a Placement Group (however, you can not exceed your EC2 instance limit allocated to your account per region).</p><p dir="ltr" style="">Option D is incorrect because the capacity error is not related to the instance size and just ensuring that the instances are of same size will not resolve the capacity error.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">More information on Cluster Placement Group</p><p dir="ltr" style="">&nbsp;If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has the capacity for all the requested instances.</p><br><br>For more information on this, please refer to the below URL<br><br><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a><br><p></p><p></p><p></p>	Make a new Placement Group and launch the new instances in the new group. Make sure the Placement Groups are in the same subnet. <br>  新しいプレースメントグループを作成し、新しいグループに新しいインスタンスを起動します。プレースメントグループが同じサブネットにあることを確認します。	Request a capacity increase from AWS as you are initially limited to 10 instances per Placement Group. <br>  最初にプレースメントグループごとに10インスタンスに制限されているため、AWSから容量を増やすことをリクエストしてください。	Make sure all the instances are the same size and then try the launch again. <br>  すべてのインスタンスが同じサイズであることを確認してから、再度起動してください。
Test2-20. <p><span id="docs-internal-guid-846a0cce-30c1-7d43-d4a3-f4ea3b2f48a4"></span></p><p dir="ltr" style="">An International company has deployed a multi-tier web application that relies on DynamoDB in a single region. For regulatory reasons they need disaster recovery capability in a separate region with a Recovery Time Objective of 2 hours and a Recovery Point Objective of 24 hours. They should synchronize their data on a regular basis and be able to provision me web application rapidly using CloudFormation. The objective is to minimize changes to the existing web application, control the throughput of DynamoDB used for the synchronization of data and synchronize only the modified elements.</p><p dir="ltr" style="">Which design would you choose to meet these requirements?</p><br><p></p> | <p> <p id = "docs-internal-guid-846a0cce-30c1-7d43-d4a3-f4ea3b2f48a4"> </ span> <p dir = "ltr" style = "">単一領域内のDynamoDBに依存する多層Webアプリケーションです。規制上の理由から、別の地域では復旧時間目標2時間、復旧ポイント目標24時間の災害復旧機能が必要です。彼らは定期的にデータを同期し、CloudFormationを使用して私のWebアプリケーションを迅速にプロビジョニングすることができます。目的は、既存のWebアプリケーションの変更を最小限に抑え、データの同期に使用されるDynamoDBのスループットを制御し、変更された要素のみを同期させることです。</p> <p dir = "ltr" style = "">	sa:	Use AWS Data Pipeline to schedule a DynamoDB cross region copy once a day, create a “Last updated” attribute in your DynamoDB table that would represent the timestamp of the last update and use it as a filter. <br>  AWS Data Pipelineを使用してDynamoDBの領域間コピーを1日1回スケジュールし、最後の更新のタイムスタンプとなるDynamoDBテーブルに「Last updated」属性を作成し、フィルタとして使用します。|<p><br></p><p></p><span id="docs-internal-guid-846a0cce-30c1-bce9-8817-03999310bfc9"><p dir="ltr" style="">Option A is CORRECT because the DynamoDB data can be copied across region by AWS Data Pipeline.</p><p dir="ltr" style="">Option B is incorrect because (a) there is no schedule control in EMR, (b) this is a significant change to the current architecture, and (c) scan operation is expensive.</p><p dir="ltr" style="">Option C is incorrect because (a) involving S3 is unnecessary as the data can directly be replicated to table in another region via AWS Data Pipeline, and (b) this is a big change in the existing architecture which can and should be avoided.</p>Option D is incorrect because &nbsp;(a) involving SQS is unnecessary as the data can directly be replicated to table in another region via AWS Data Pipeline, and (b) this is not an automated way.<br><br></span><a href="https://aws.amazon.com/directconnect/" target="_blank" style="font-size: 1rem;"></a><br>For more information on this topic, please visit the link below:<br><a href="https://aws.amazon.com/blogs/aws/copy-dynamodb-data-between-regions-using-the-aws-data-pipeline/" target="_blank">https://aws.amazon.com/blogs/aws/copy-dynamodb-data-between-regions-using-the-aws-data-pipeline/</a><br><br><p></p>	Use EMR and write a custom script to retrieve data from DynamoDB in the current region using a SCAN operation and push it to DynamoDB in the second region. <br>  EMRを使用してカスタムスクリプトを作成し、SCAN操作を使用して現在の領域のDynamoDBからデータを取得し、2番目の領域のDynamoDBにプッシュします。	Use AWS data pipeline to schedule an export of the DynamoDB table to S3 in the current region once a day then schedule another task immediately after it that will import data from S3 to DynamoDB in the other region.? <br>  AWSデータパイプラインを使用して、現在の地域のS3へのDynamoDBテーブルのエクスポートを1日に1回スケジュールし、S3から他の地域のDynamoDBにデータをインポートする直後に別のタスクをスケジュールします。	Send each item into an SQS queue in the second region; use an auto-scaling group behind the SQS queue to replay the write in the second region. <br>  SQSキューの背後にある自動スケーリンググループを使用して、2番目の領域で書き込みを再生します。
Test2-21. <p>An auditor has been called upon to carry out an audit of the configuration of your AWS accounts. The auditor has specified that they just want to read only access to the AWS resources on all accounts. Which of the below options would help the auditor get the required access?</p> | <p> AWSアカウントの設定を監査するために審査員が呼び出されました。監査人は、すべてのアカウントのAWSリソースへのアクセスのみを読み取るように指定しました。以下のオプションのうち、監査人が必要なアクセス権を取得するのに役立つものはどれですか？</p>	sa:	Create an IAM role with read-only permissions to all AWS services in each AWS account. Create one auditor IAM account and add a permissions policy that allows the auditor to assume the ARN role for each AWS account that has an assigned role. <br>  各AWSアカウントでIAMロールを作成します。1つの監査人IAMアカウントを作成し、割り当てられた役割を持つ各AWSアカウントに対して監査人がARNロールを引き受けることを許可する許可ポリシーを追加します。|<p><br></p><p><span id="docs-internal-guid-86cab175-02c9-40cb-c2bb-9d498d9a867f"></span></p><p dir="ltr" style="">Option A is incorrect because creating an IAM User for each AWS account is an overhead and less preferred way compared to creating IAM Role.</p><p dir="ltr" style="">Option B is incorrect because the scenario says that the company does not have any on-premises identity provider.</p><p dir="ltr" style="">Option C is CORRECT because it creates an IAM Role which has all the necessary permission policies attached to it which allows the auditor to assume the appropriate role while accessing the resources.</p><p dir="ltr" style="">Option D is incorrect because using the IAM Role that has the required permissions is the preferred and more secure way of accessing the AWS resources than using the Amazon credentials. Also, this option does not use any Security Token Service that gives temporary credentials to login. Hence this is a less secure way of accessing the AWS resources.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on IAM roles please refer to the below URL</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><br><p></p><ul></ul><p></p>	Configure an on-premise AD server and enable SAML and identify federation for single sign-on to each AWS account. <br>  オンプレミスADサーバーを設定し、SAMLを有効にし、フェデレーションを各AWSアカウントへのシングルサインオンとして識別します。	Create an IAM user for each AWS account with read-only permission policies for the auditor, and disable each account when the audit is complete. <br>  監査担当者の読み取り専用権限ポリシーを使用して各AWSアカウントのIAMユーザーを作成し、監査が完了したら各アカウントを無効にします。	Create a custom identity broker application that allows the auditor to use existing Amazon credentials to log into the AWS environments. <br>  監査員が既存のAmazonの認証情報を使用してAWS環境にログインできるようにするカスタムIDブローカー・アプリケーションを作成します。
Test2-22. <p>You currently have developers who have access to your production AWS account? There is a concern raised that the developers could potentially delete the production-based EC2 resources. Which of the below options could help alleviate this concern? <br></p><p>Choose 2 options from the below:<br></p> | <p>現在、実動AWSアカウントにアクセスできる開発者がいます。 開発者が生産ベースのEC2リソースを削除する可能性があるという懸念があります。 この問題を緩和するのに役立つオプションは次のうちどれですか？<br> </p> <p>下記の2つのオプションを選択してください：<br> </p>	ma:	o:Tag the production instances with a production-identifying tag and add resource-level permissions to the developers with an explicit deny on the terminate API call to instances with the production tag. <br>  プロダクションタグを使用して本番インスタンスにタグ付けし、本番タグを持つインスタンスへのAPI呼び出しの終了を明示的に拒否して、開発者にリソースレベルの権限を追加します。|<p><br></p><p><span id="docs-internal-guid-86cab175-07d5-1ee1-bc44-52c4c645a4a7"></span></p><p dir="ltr" style="">To stop the users from manipulating any AWS resources, you can either create the applicable (allow/deny) resource level permissions and apply them to those users, or create an individual or group policy which explicitly denies the action on that resource and apply it to the individual user or the group.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">Option A is CORRECT because it (a) identifies the instances with proper tag, and (b) creates a resource level permission and explicitly denies the user the terminate option.</span></p><p dir="ltr" style="">Option B is CORRECT because it (a) identifies the instances with proper tag, and (b) creates a policy with explicit deny of terminating the instances and applies that policy to the group which contains the employees (who are not supposed to have the access to terminate the instances).</p><p dir="ltr" style="">Option C and D are incorrect because MFA is an additional layer of security given to the users for logging into AWS and accessing the resources. However, either enabling or disabling MFA cannot prevent the users from performing resource level actions.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style=""><b>More information on Tags</b></p><p dir="ltr" style="">Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type — you can quickly identify a specific resource based on the tags you've assigned to it. Each tag consists of a key and an optional value, both of which you define.</p><p dir="ltr" style="">For more information on tagging AWS resources please refer to the below URL</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p><br><br><p></p>	o:Tag the instance with a production-identifying tag and modify the employees group to allow only start, stop, and reboot API calls and not the terminate instance call. <br>  実動識別タグを使用してインスタンスにタグを付け、employeesグループを変更して、インスタンス呼び出しの終了ではなく、開始、停止、および再起動のAPI呼び出しのみを許可します。	x:Modify the IAM policy on the developers to require MFA before deleting EC2 instances and disable MFA access to the employee <br>  EC 2インスタンスを削除して従業員へのMFAアクセスを無効にする前に、MFAを要求するように開発者のIAMポリシーを変更する	x:Modify the IAM policy on the developers to require MFA before deleting EC2 instances <br>  EC 2インスタンスを削除する前にMFAを要求するように開発者のIAMポリシーを変更する
Test2-23. <p>A legacy application is being migrated to AWS. It works on the TCP protocol. There is a requirement to ensure scalability of the application and also ensure that records of the client IP using the application are recorded. <br></p><p>Which of the below-mentioned steps would you implement to fulfill the above requirement?</p> | <p>レガシーアプリケーションがAWSに移行されています。これはTCPプロトコルで動作します。アプリケーションのスケーラビリティを確保し、アプリケーションを使用するクライアントIPの記録が確実に記録されるようにする必要があります。<br> </p> <p>上記の要件を満たすために以下の手順のどれを実装しますか？</p>	sa:	Use an ELB with a TCP Listener and Proxy Protocol enabled to distribute load on two or more application servers in different AZs. <br>  さまざまなAZの2つ以上のアプリケーションサーバーに負荷を分散するには、TCPリスナーとプロキシプロトコルを有効にしたELBを使用します。|<p><br></p><p></p><span id="docs-internal-guid-6c5f78f5-070e-22ba-dca5-700e253fdf79"><p dir="ltr" style="">AWS ELB has support for Proxy Protocol. It simply depends on a humanly readable header with the client's connection information to the TCP data sent to your server.&nbsp;As per the AWS documentation, the Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections. Because load balancers intercept traffic between clients and your instances, the access logs from your instance contain the IP address of the load balancer instead of the originating client. You can parse the first line of the request to retrieve your client's IP address and the port number.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is CORRECT because it implements the proxy protocol and uses ELB with TCP listener.</p><p dir="ltr" style="">Option B is incorrect because, although implementing cross-zone load balancing provides high availability, it is not going to give the IP address of the clients.</p><p dir="ltr" style="">Option C is incorrect because Route53 latency based routing does not give the IP address of the clients.</p><p dir="ltr" style="">Option D is incorrect because Route53 Alias record does not give the IP address of the clients.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on ELB enabling support for TCP, please refer to the links given below:<br><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/" target="_blank">https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/</a></span><br><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html" target="_blank">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html</a></span></p></span><br><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/" target="_blank"></a></span></li></ul><p></p>	Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled, two application servers in different AZs. <br>  TCPリスナとクロスゾーンロードバランシングが有効になっているELBを使用して、異なるAZの2つのアプリケーションサーバを使用します。	Use Route 53 with Latency Based Routing enabled to distribute load on two or more application servers in different AZs. <br>  遅延時間ベースのルーティングを有効にしたRoute 53を使用すると、異なるAZの2つ以上のアプリケーションサーバーに負荷を分散できます。	Use Route 53 Alias Resource Record to distribute load on two application servers in different AZs. <br>  Routes 53 Alias Resource Recordを使用して、異なるAZの2つのアプリケーションサーバーに負荷を分散します。
Test2-24. <p>Which of the following are the ways to minimize the attack surface area as a DDOS minimization strategy in AWS?</p> <p>Choose 3 options from the below:</p> | <p> AWSのDDoS最小化戦略として攻撃領域を最小限に抑える方法はどれですか？</p> <p>以下の3つのオプションを選択してください：</p>	ma:	x:Configure services such as Elastic Load Balancing and Auto Scaling to automatically scale. <br>  Elastic Load BalancingやAuto Scalingなどのサービスを自動的に拡大/縮小するように設定します。|<p><br></p><p><span id="docs-internal-guid-86cab175-0733-4867-2abd-117ac80be3ff"></span></p><p dir="ltr" style="">Some important consideration when architecting on AWS is to limit the opportunities that an attacker may have to target your application. For example, if you do not expect an end user to directly interact with certain resources you will want to make sure that those resources are not accessible from the Internet. Similarly, if you do not expect end-users or external applications to communicate with your application on certain ports or protocols, you will want to make sure that traffic is not accepted. This concept is known as attack surface reduction.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because it is used for mitigating the DDoS attack where the system scales to absorb the application layer traffic in order to keep it responsive.&nbsp;</p><p dir="ltr" style="">Option B, C and D are all CORRECT as they all are used for reducing the DDoS attack surface.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">For more information on DDoS attacks in AWS, please visit the below URL</span></p><p dir="ltr" style=""><a href="https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><br><p></p><ul></ul><p></p>	o:Reduce the number of necessary Internet entry points. <br>  必要なインターネットエントリポイントの数を減らします。	o:Separate end user traffic from management traffic. <br>  エンドユーザートラフィックと管理トラフィックを区別します。	o:Eliminate non-critical Internet entry points. <br>  重要ではないインターネットエントリポイントを排除します。
Test2-25. <p>You've created a temporary application that accepts image uploads, stores them in S3, and records the information about the images in RDS. After building this architecture and accepting the images for the duration required, it's time to delete the CloudFormation template. However, your manager has informed you that, for some reason, they need to ensure that a backup is taken of the RDS when the CloudFormation template is deleted. Which of the options below will fulfill the above requirement?</p> | <p>イメージのアップロードを受け付け、S3に保存し、RDSのイメージに関する情報を記録する一時的なアプリケーションを作成しました。このアーキテクチャを構築し、必要な期間画像を受け入れたら、CloudFormationテンプレートを削除します。ただし、何らかの理由で、CloudFormationテンプレートが削除されたときにRDSのバックアップが取られるようにする必要があることを管理者から通知されています。上記の要件を満たすオプションはどれですか？</p>	sa:	Set the DeletionPolicy on the RDS resource to snapshot. <br>  RDSリソースのDeletionPolicyをスナップショットに設定します。|<p><br></p><p><span id="docs-internal-guid-86cab175-0739-5075-d5e6-34ec3ea05b0d"></span></p><p dir="ltr" style=""><span id="docs-internal-guid-86cab175-0739-5075-d5e6-34ec3ea05b0d" style="font-size: 1rem;"></span></p><p dir="ltr" style="display: inline !important;">The main point in this scenario is that even if the CloudFormation stack is deleted there should be a way to able to restore the RDS data if needed.</p><p></p><p><span><span style="font-size: 1rem;">&nbsp;</span></span></p><p dir="ltr" style="">Option A is incorrect because the DeletionPolicy of the RDS instance should be set to snapshot. If delete is used, the resource would get deleted and the dat cannot be restored in the future.</p><p dir="ltr" style="">Option B is incorrect because DeletionPolicy attribute for RDS should be&nbsp;snapshot, not&nbsp;retain&nbsp;because with&nbsp;snapshot&nbsp;option, the backup of the RDS instance would be stored in the form of snapshots (which is the requirement). With&nbsp;retain&nbsp;option, CF will keep the RDS instance alive which is unwanted. There is such no requirement on S3.</p><p dir="ltr" style="">Option C is CORRECT because it correctly sets the DeletionPolicy of the RDS to snapshot so that the data can be restored from the snapshot if needed.</p><p dir="ltr" style="">Option D is incorrect because it sets the DeletionPolicy of the RDS to retain which will keep the RDS instance alive. It just needs to take the snapshot.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">More information on DeletionPolicy on CloudFront</p><p dir="ltr" style="">DeletionPolicy options include:</p><ul style=""><li dir="ltr"><p dir="ltr">Retain:&nbsp;You retain the resource in the event of a stack deletion.</p></li><li dir="ltr"><p dir="ltr">Snapshot:&nbsp;You get a snapshot of the resource before it’s deleted. This option is available only for resources that support snapshots.</p></li><li dir="ltr"><p dir="ltr">Delete:&nbsp;You delete the resource along with the stack. This is the default outcome if you don’t set a DeletionPolicy.</p></li></ul><p dir="ltr" style="">To keep or copy resources when you delete a stack, you can specify either the Retain or Snapshot policy options.</p><p dir="ltr" style="">With the DeletionPolicy attribute, you can preserve or (in some cases) backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on Cloudformation deletion policy, please visit the below URL</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html" target="_blank">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p></p><ul></ul><p></p>	For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to Retain. <br>  CloudFormationテンプレートのRDSリソースタイプとS3リソースタイプの両方で、DeletionPolicyをRetainに設定します。	Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to delete. <br>  すべてのS3オブジェクトのコピーを維持し、削除するRDSインスタンスの削除ポリシーを設定するために、宛先バケットへのソースバケット上のS3バケットレプリケーションを有効にします。	Set the DeletionPolicy on the RDS resource to retain. <br>  保持するRDSリソースのDeletionPolicyを設定します。
Test2-26. <p>An application, basically a mobile application needs access for each user to store data in a DynamoDB table. What is the best method for granting each mobile device that ensures the application has access DynamoDB tables for storage when required? <br></p><p>Choose the correct options from the below:</p> | <p>アプリケーションでは、基本的にモバイルアプリケーションは、各ユーザーがDynamoDBテーブルにデータを格納するためにアクセスする必要があります。必要に応じて、アプリケーションがストレージ用のDynamoDBテーブルにアクセスすることを保証する各モバイルデバイスを許可するための最良の方法は何ですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	Create an IAM role with the proper permission policy to communicate with the DynamoDB table. Use web identity federation, which assumes the IAM role using AssumeRoleWithWebIdentity, when the user signs in, granting temporary security credentials using STS. <br>  DynamoDBテーブルを使用するには、適切なアクセス許可ポリシーを使用してIAMロールを作成します。ユーザーがサインインしたときにAssumeRoleWithWebIdentityを使用してIAMロールを想定し、STSを使用して一時的なセキュリティ資格情報を付与するWeb IDフェデレーションを使用します。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-0755-74a5-d7c8-8b33578abfc2"></span></p><p dir="ltr" style="">Option A is incorrect because IAM Roles are preferred over IAM Users, because IAM Users have to access the AWS resources using access and secret keys, which is a security concern.</p><p dir="ltr" style="">Option B is this is not a feasible configuration.</p><p dir="ltr" style="">Option C is CORRECT because it (a) creates an IAM Role with the needed permissions to connect to DynamoDB, (b) it authenticates the users with Web Identity Federation, and (c) the application accesses the DynamoDB with temporary credentials that are given by STS.</p><p dir="ltr" style="">Option D is incorrect because the step to create the Active Directory (AD) server and using AD for authenticating is unnecessary and costly.</p><br><p dir="ltr" style="">See the image below for more information on AssumeRoleWithWebIdentity API.</p><p dir="ltr" style=""><img src="https://s3.amazonaws.com/awssap/2_26_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><br><p dir="ltr" style="">For more information on web identity federation please refer to the below link</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><br><br><p></p><ul></ul><p></p>	Create an IAM group that only gives access to your application and to the DynamoDB tables. Then, when writing to DynamoDB, simply include the unique device ID to associate the data with that specific user. <br>  アプリケーションとDynamo DBテーブルにのみアクセスできるIAMグループを作成します。次に、Dynamo DBに書き込むときに、固有のデバイスIDを指定するだけで、その特定のユーザーとデータを関連付けることができます。	During the install and game configuration process, have each user create an IAM credential and assign the IAM user to a group with proper permissions to communicate with DynamoDB. <br>  インストールとゲームの設定プロセス中に、各ユーザーにIAM資格を作成させ、IAMユーザーをDynamo DBと通信するための適切な権限を持つグループに割り当てます。	Create an Active Directory server and an AD user for each mobile application user. When the user signs in to the AD sign-on, allow the AD server to federate using SAML 2.0 to IAM and assign a role to the AD user which is the assumed with AssumeRoleWithSAML. <br>  ユーザーがADサインオンにサインオンすると、ADサーバーはSAML 2.0を使用してIAMにフェデレートし、AssumeRoleWithSAMLで想定されるADユーザーに役割を割り当てます。
Test2-27. <p>A company has a Direct Connect established between their on-premise location and AWS. The applications hosted on the on-premise location are experiencing high latency when using S3. What could be done to ensure that the latency to S3 can be reduced?</p> | <p>企業では、社内のロケーションとAWSの間にDirect Connectが確立されています。オンプレミスロケーションでホストされているアプリケーションは、S3を使用しているときに待ち時間が長くなります。S3への待ち時間を減らすために何ができるのでしょうか？</p>	sa:	Configure a public virtual interface to connect to a public S3 endpoint resource via Direct Connect connection. <br>  直接接続接続を介してパブリックS3エンドポイントリソースに接続するようにパブリック仮想インターフェイスを構成します。|<p><br></p><p><span id="docs-internal-guid-86cab175-0760-6473-74b0-ba160ab89f16"></span></p><p dir="ltr" style="">You can create a public virtual interface to connect to public resources or a private virtual interface to connect to your VPC. You can configure multiple virtual interfaces on a single AWS Direct Connect connection, and you'll need one private virtual interface for each VPC to connect to. Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. See the image below:</p><p dir="ltr" style=""><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/2_27_1.png" alt="" width="639" height="360" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">Option A is CORRECT because, as mentioned above, it creates a public virtual interface to connect to S3 endpoint.</span></p><p dir="ltr" style="">Option B is incorrect because to connect to S3 endpoint, a public virtual interface needs to be created, not VPN.</p><p dir="ltr" style="">Option C is incorrect because to connect to S3 endpoint, a&nbsp;public&nbsp;virtual interface needs to be created,&nbsp;not&nbsp;private.</p><p dir="ltr" style="">Option D is incorrect because this setup will not help connecting to the S3 endpoint.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on virtual interfaces, please visit the below URL</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank">http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html</a></p><br><p></p><ul></ul><p></p>	Establish a VPN connection from the VPC to the public S3 endpoint. <br>  VPCからパブリックS3エンドポイントへのVPN接続を確立します。	Configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection. <br>  直接接続接続を介してパブリックS3エンドポイントに接続するようにプライベート仮想インターフェイスを設定します。	Add a BGP route as part of the on-premise router; this will route S3 related traffic to the public S3 endpoint to dedicated AWS region. <br>  BGPルートをオンプレミスルータの一部として追加します。S3関連のトラフィックをパブリックS3エンドポイントに専用のAWS領域にルーティングします。
Test2-28. <p>There is a requirement to carry out the backup of an Oracle RAC cluster which is currently hosted on the AWS public cloud. How can this be achieved?</p> | <p> AWSパブリッククラウドで現在ホストされているOracle RACクラスタのバックアップを実行する必要があります。どのようにこれを達成できますか？</p>	sa:	Create a script that runs snapshots against the EBS volumes to create backups and durability. <br>  EBSボリュームに対してスナップショットを実行するスクリプトを作成し、バックアップと耐久性を作成します。|<p><br></p><p><span id="docs-internal-guid-86cab175-0765-b836-0715-3f9219bbdee7"></span></p><p dir="ltr" style="">Currently, Oracle Real Application Cluster (RAC) is not supported as per the AWS documentation. However,&nbsp;you can deploy scalable RAC on Amazon EC2 using the recently-published&nbsp;<a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/" style="">tutorial&nbsp;</a>and Amazon Machine Images (AMI). So, in order to take the backups, you need to take the backup in the form of EBS volume snapshots of the EC2 that is deployed for RAC.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">Option A, B, and D are all incorrect because RDS does not support Oracle RAC.</p><p dir="ltr" style="">Option C is CORRECT because Oracle RAC is supported via the deployment using Amazon EC2. Hence, for the data backup, you can create a script that takes the snapshots of the EBS volumes.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on Oracle RAC on AWS, please visit the below URL:</p><p dir="ltr" style=""><a href="https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/</a></p><p dir="ltr" style=""><a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/" target="_blank">https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/</a></p><p dir="ltr" style=""><a href="https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/" target="_blank">https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/</a></p><br><p></p><ul></ul><p></p>	Enable Multi-AZ failover on the RDS RAC cluster to reduce the RPO and RTO in the event of disaster or failure. <br>  災害や障害が発生した場合にRPOとRTOを減らすために、RDSクラスタで複数AZのフェールオーバーを有効にします。	Create manual snapshots of the RDS backup and write a script that runs the manual snapshot <br>  RDSバックアップの手動スナップショットを作成し、手動スナップショットを実行するスクリプトを作成する	Enable automated backups on the RDS RAC cluster; enable auto snapshot copy to a backup region to reduce RPO and RTO. <br>  RDS RACクラスタで自動バックアップを有効にする。RPOとRTOを減らすために、バックアップ領域への自動スナップショットコピーを有効にします。
Test2-29. <p>A company is running a MySQL RDS instance inside of AWS. However, a new requirement for disaster recovery is keeping a read replica of the production RDS instance in an on-premise data center. What is the securest way of performing this replication? <br></p><p>Choose the correct option from the below:</p> | <p>会社はAWS内でMySQL RDSインスタンスを実行しています。ただし、災害復旧の新しい要件として、運用中のRDSインスタンスの読み取りレプリカを社内のデータセンターに保持することがあります。このレプリケーションを実行する安全な方法は何ですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. <br>  Virtual Private Cloudサービスを通じてOpenVPNまたはVPN / VGWのいずれかを使用してIPSec VPN接続を作成します。|<p></p><div><p>Option A is incorrect because SSL endpoint cannot be used here as it is used for securely accessing the database.</p><p>Option B is incorrect because replicating via EC2 instances is very time consuming and very expensive cost-wise.</p><p>Option C is incorrect because Data Pipeline is for batch jobs and not suitable for this scenario.</p><p>Option D is CORRECT because it is feasible to setup the secure IPSec VPN connection between the on premise server and AWS VPC using the VPN/Gateways.</p><p>See the image below:</p></div><div><br></div><div><img src="https://s3.amazonaws.com/awssap/2_29_1.png" alt="" width="673" height="375" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></div><div><br></div><div>For more information on VPN connections , please visit the below URL:<br></div><div><br></div><div><ul><li><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank" style="font-size: 1rem; background-color: rgb(255, 255, 255);"><span style="font-size: 1rem;">http://docs.aws.amazon.com/Ama</span><wbr><span style="font-size: 1rem;">zonVPC/latest/UserGuide/VPC_VP</span><wbr><span style="font-size: 1rem;">N.html</span></a></li></ul></div><br><p></p><ul></ul><p></p>	RDS cannot replicate to an on-premise database server. Instead, first configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection. <br>  RDSはオンプレミスデータベースサーバーに複製できません。代わりに、コアMySQLを使用してEC 2インスタンスにレプリケートするようにRDSインスタンスを構成し、セキュアなVPN / VPG接続を介してレプリケーションを構成します。	Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. <br>  毎晩MySQLデータをエクスポートし、HTTPSエンドポイントからS3を安全にダウンロードするデータパイプラインを作成します。	Configure the RDS instance as the master and enable replication over the open internet using a secure SSL endpoint to the on-premise server. <br>  RDSインスタンスをマスターとして構成し、セキュアSSLエンドポイントを使用してオンプレミスサーバーにインターネット経由でレプリケーションを有効にします。
Test2-30. <p>Your company’s on-premises content management system has the following architecture:</p> <p>Application Tier – Java code on a JBoss application server<br>Database Tier – Oracle database regularly backed up to Amazon Simple Storage Service (S3) using the Oracle RMAN backup utility<br>Static Content – stored on a 512GB gateway stored Storage Gateway volume attached to the application server via the iSCSI interface</p> <p>Which AWS based disaster recovery strategy will give you the best RTO?</p> | <p>社内のコンテンツ管理システムには、次のアーキテクチャがあります。<br>アプリケーション層 - JBossアプリケーションサーバー上のJavaコード<br>データベース層 - Oracleデータベースは、Oracle RMANバックアップユーティリティを使用してAmazon Simple Storage Service（S3）に定期的にバックアップされます。<br>静的コンテンツ - 512GBのゲートウェイに格納されています。ストレージ・ゲートウェイ・ボリュームは、iSCSIインタフェースを介してアプリケーション・サーバーに接続されています。<br>どのAWSベースのディザスタリカバリ戦略が最良のRTOを提供しますか？</p>	sa:	Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Generate an? EBS volume of static content from the Storage Gateway and attach it to the JBoss EC2 server. <br>  OracleデータベースとJBossアプリケーションサーバーをEC2にデプロイします。 Amazon S3からRMAN Oracleバックアップをリストアします。 Storage Gatewayから静的コンテンツのEBSボリュームを生成し、JBoss EC2サーバーに接続します。|<p><br></p><p>Option A is CORRECT because (i) it deploys the Oracle database on EC2 instance by restoring the backups from S3 which is quick, and (ii) it generates the EBS volume of static content from Storage Gateway. Due to these points, option A meet the best RTO compared to all the remaining options.&nbsp;</p><p>Option B is incorrect because restoring the backups from the Amazon Glacier will be slow and will not meet the RTO.</p><p>Option C is incorrect because there is no need to attach the Storage Gateway as an iSCSI volume; you can just easily and quickly create an EBS volume from the Storage Gateway. Then you can generate snapshots from the EBS volumes for better recovery time.</p><p>Option D is incorrect as restoring the content from Virtual Tape Library will not fit into the RTO.</p><p><br></p>	Deploy the Oracle database on RDS. Deploy the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon Glacier. Generate an EBS volume of static content from the Storage Gateway and attach it to the JBoss EC2 server. <br>  OracleデータベースをRDSにデプロイします。 JBossアプリケーションサーバーをEC2にデプロイします。 Amazon GlacierからRMAN Oracleバックアップを復元します。 Storage Gatewayから静的コンテンツのEBSボリュームを生成し、JBoss EC2サーバーに接続します。	Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Restore the static content by attaching an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the JBoss EC2 server. <br>  OracleデータベースとJBossアプリケーションサーバーをEC2にデプロイします。 Amazon S3からRMAN Oracleバックアップをリストアします。 Amazon EC2上で動作するAWS Storage GatewayをiSCSIボリュームとしてJBoss EC2サーバーに接続することにより、静的コンテンツを復元します。	Deploy the Oracle database and the JBoss app server on EC2. Restore the RMAN Oracle backups from Amazon S3. Restore the static content from an AWS Storage Gateway-VTL running on Amazon EC2 <br>  OracleデータベースとJBossアプリケーションサーバーをEC2にデプロイします。 Amazon S3からRMAN Oracleバックアップをリストアします。 Amazon EC2上で動作するAWS Storage Gateway-VTLから静的コンテンツを復元する
Test2-31. <p>An ERP application is deployed in multiple Availability Zones in a single region. In the event of failure, the RTO must be less than 3 hours and the RPO is 15 minutes. The customer realizes that data corruption occurred roughly 1.5 hours ago. Which DR strategy can be used to achieve this RTO and RPO in the event of this kind of failure?</p> | <p> ERPアプリケーションは、単一の地域にある複数の可用性ゾーンに配備されています。障害が発生した場合、RTOは3時間未満、RPOは15分間でなければなりません。顧客は、およそ1.5時間前にデータ破損が発生したことを認識します。このような障害が発生した場合に、このRTOとRPOを達成するために使用できるDR戦略は何ですか？</p>	sa:	Take hourly DB backups to Amazon S3, with transaction logs stored in S3 every 5 minutes. <br>  トランザクションログをS3に5分ごとに格納して、時間軸のDBバックアップをAmazon S3に転送します。|<p>&nbsp;</p><p>Option A is incorrect because restoring the backups from Amazon Glacier would be slow and will definitely not meet the RTO and RPO.&nbsp;</p><p>Option B is incorrect because with the synchronous replication you cannot go back to point in time recovery. You will always have the latest data.&nbsp;</p><p>Option C is CORRECT because it takes hourly backups to Amazon S3 - which makes restoring the backups quick, and since the transaction logs are stored in S3 every 5 minutes, it will help to restore the application to a state that is within the RPO of 15 minutes.&nbsp;</p><p>Option D is incorrect because instant store volume is ephemeral. i.e. the data can get lost when the instance is terminated.</p><p><br></p><p>NOTE:</p><p></p><p>Although Glacier supports expedited retrieval (On-Demand and Provisioned), it is an expensive option and is recommended only for occasional urgent request for a small number of archives. Having said this (and even if we go with glacier as solution), the option also mentions taking database snapshots every 15 minutes. Now if you keep taking backups every 15 mins, the database users are going to face lot of outages during the backup (due to I/O suspension especially in non-AZ deployment). Also, within 15 minutes the backup process may not even finish!</p><p><br></p><p>As an architect you need to use the database change (transaction) logs along with the backups to restore your database to a point in time. Since option (c) stores the transaction details up to last 5 minutes, you can easily restore your database and meet the RPO of 15 minutes. Hence, C is the best choice.</p><br><p></p>	Use synchronous database master-slave replication between two Availability Zones. <br>  2つのアベイラビリティゾーン間の同期データベースマスタースレーブレプリケーションを使用します。	Take 15-minute DB backups stored in Amazon Glacier, with transaction logs stored in Amazon S3 every 5 minutes. <br>  5分ごとにAmazon S3にトランザクションログを保存し、Amazon Glacierに格納された15分間のDBバックアップを取得します。	Take hourly DB backups to an Amazon EC2 instance store volume, with transaction logs stored in Amazon S3 every 5 minutes. <br>  アマゾンS3にトランザクションログを5分ごとに格納して、時間別DBバックアップをAmazon EC 2インスタンスストアボリュームに適用します。
Test2-32. <p>The Marketing Director in your company asked you to create a mobile app that lets users post sightings of good deeds known as random acts of kindness in 80-character summaries. You decided to write the application in JavaScript so that it would run on the broadest range of phones, browsers, and tablets. Your application should provide access to Amazon DynamoDB to store the good deed summaries. Initial testing of a prototype shows that there aren’t large spikes in usage. Which option provides the most cost-effective and scalable architecture for this application?</p> | <p>あなたの会社のマーケティングディレクターは、80文字の要約でランダムな行為を知らせるモバイルアプリを作成するようユーザーに要求しました。JavaScriptを使ってアプリケーションを作成し、最も幅広い携帯電話、ブラウザ、タブレット上で動作させることにしました。あなたのアプリケーションは、Amazon DynamoDBにアクセスして、適切な要約を保存する必要があります。プロトタイプの最初のテストでは、使用量に大きなスパイクがないことが示されています。このアプリケーションで最も費用対効果の高いスケーラブルなアーキテクチャを提供するオプションはどれですか？</p>	sa:	Register the application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow S3 gets and DynamoDB puts. You serve your mobile application out of an S3 bucket enabled as a web site. Your client updates DynamoDB. <br>  Amazon、Google、またはFacebookにアプリケーションを登録し、そのプロバイダのIAMロールを作成し、IAMロールのアクセス許可を設定してS3取得とDynamoDBプットを許可します。S3バケットからモバイルアプリケーションを提供します。ウェブサイトとして有効にします。クライアントがDynamoDBを更新します。|<p><br></p><p>This scenario asks to design a cost-effective and scalable solution where a multi-platform application needs to communicate with DynamoDB. For such scenarios, federated access to the application is the most likely solution.</p><p><br></p><p>Option A is incorrect because the Token Vending Machine (STS Service) is implemented on a single EC2 instance which is a single point of failure. This is not a scalable solution either as the instance can become the performance bottleneck.</p><p>Option B is CORRECT because, (i) it authenticates the application via federated identity provider such as Amazon, Google, Facebook etc, (ii) it sets up the proper permisssion for DynamoDB access, and (iii) S3 website which supports Javascript - is a highly scalable and cost effective solution.</p><p>Option C is incorrect because deploying EC2 instances in auto-scaled environment is not as cost-effective solution as the S3 website, even though it is scalable.</p><p>Option D is incorrect because (i) it does not mention any security token service that generates temporary credentials, and (ii) deploying EC2 instances in auto-scaled environment is not as cost-effective solution as the S3 website, even though it is scalable.</p><p><br></p><p><br></p><ul></ul><p></p>	Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance to provide signed credentials mapped to an Amazon Identity and Access Management (IAM) user allowing DynamoDB puts and S3 gets. You serve your mobile application out of an S3 bucket enabled as a web site. Your client updates DynamoDB. <br>  EC 2インスタンス上のトークン自動販売機（TVM）を使用して、セキュリティトークンサービスからの一時的な資格情報をJavaScriptクライアントに提供し、AmazonのIDとアクセス管理（IAM）ユーザーにマッピングされた署名入りの資格情報をDynamoDBのputとS3取得に提供します。Webサイトとして有効になっているS3バケットからモバイルアプリケーションを提供します。クライアントがDynamoDBを更新します。	Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials mapped to an IAM user allowing DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoDB. <br>  JavaScriptクライアントにToken Vending Machine（TVM）からの一時的な資格情報を提供して、DynamoDBが許可するIAMユーザーにマップされた署名済みの資格情報を提供します。負荷分散および自動拡張されたApache EC2インスタンスからモバイルアプリケーションを提供します。EC2インスタンスは、DynamoDBの配置を可能にするIAMロールで構成されます。サーバーがDynamoDBを更新します。	Register the JavaScript application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoDB. <br>  Amazon、Google、FacebookなどのWebアイデンティティプロバイダにJavaScriptアプリケーションを登録し、そのプロバイダのIAMロールを作成し、DynamoDBの配置を許可するIAMロールのアクセス許可を設定します。ロードバランスされ、自動拡張されたApache EC 2インスタンスからモバイルアプリケーションを提供します。EC2インスタンスは、DynamoDBのputを許可するIAMロールで構成されています。サーバーがDynamoDBを更新します。
Test2-33. <p>You are building a website that will retrieve and display highly sensitive information to users. The amount of traffic the site will receive is known and not expected to fluctuate. The site will leverage SSL to protect the communication between the clients and the web servers. Due to the nature of the site you are very concerned about the security of your SSL private key and want to ensure that the key cannot be accidentally or intentionally moved outside your environment. Additionally, while the data the site will display is stored on an encrypted EBS volume, you are also concerned that the web servers’ logs might contain some sensitive information; therefore, the logs must be stored so that they can only be decrypted by employees of your company. Which of these architectures meets all of the requirements?</p> | <p>高度に機密性の高い情報を取得してユーザーに表示するWebサイトを構築しています。サイトが受け取るトラフィック量は既知であり、変動するとは予想されません。このサイトは、SSLを利用してクライアントとWebサーバー間の通信を保護します。サイトの性質上、SSL秘密鍵のセキュリティは非常に懸念されており、誤ってか意図せずに自分の環境の外に鍵を移動できないようにする必要があります。さらに、サイトに表示されるデータは暗号化されたEBSボリュームに保存されますが、Webサーバーのログには機密情報が含まれている可能性もあります。したがって、ログは会社の社員のみが解読できるように保存する必要があります。これらのアーキテクチャのどれがすべての要件を満たしていますか？</p>	sa:	Use Elastic Load Balancing to distribute traffic to a set of web servers, configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption. <br>  Elastic Load Balancingを使用してWebサーバーにTCPロードバランシングし、AWS CloudHSMを使用してSSLトランザクションを実行する。Amazon S3サーバサイド暗号化を使用したプライベートAmazon S3バケットにWebサーバーログを書き込む。 |<p><br></p><p>Option A and D both are incorrect because the logs - which contain the sensitive information - are written to ephemeral volume. So there are chances that the data can get lost upon termination of the EC2 instance.</p><p>Option B is incorrect because it does not use a secure way of managing the SSL private key for SSL transaction.</p><p>Option C is CORRECT because it uses CloudHSM for performing the SSL transaction without requiring any additional way of storing or managing the SSL private key. This is the most secure way of ensuring that the key will not be moved outside of the AWS environment. Also, it uses the highly available and durable S3 service for storing the logs.</p><p><br></p><p><b>More information on AWS CloudHSM:</b></p><p>The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) appliances within the AWS cloud. With CloudHSM, you control the encryption keys and cryptographic operations performed by the HSM.</p><p><br></p><p>For more information on AWS CloudHSM, please refer to the link:</p><p></p><a href="https://aws.amazon.com/cloudhsm/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/cloudhsm/</a><br><br><p></p>	Use Elastic Load Balancing to distribute traffic to a set of web servers. Use TCP load balancing on the load balancer and configure your web servers to retrieve the private key from a private Amazon S3 bucket on boot. Write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。 ロードバランサでTCPロードバランシングを使用し、起動時に非公開のAmazon S3バケットから秘密鍵を取得するようにWebサーバーを構成します。 Amazon S3サーバー側の暗号化を使用して、Webサーバーログを非公開のAmazon S3バケットに書き込みます。	Use Elastic Load Balancing to distribute traffic to a set of web servers. To protect the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。 SSL秘密鍵を保護するには、鍵をロードバランサにアップロードし、ロードバランサを設定してSSLトラフィックをオフロードします。 ランダムに生成されたAESキーを使用して暗号化されたエフェメラルボリュームにWebサーバーログを書き込みます。	Use Elastic Load Balancing to distribute traffic to a set of web servers. Configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。 TCPロードバランシングを実行するようにロードバランサを設定し、AWS CloudHSMを使用してSSLトランザクションを実行し、ランダムに生成されたAESキーを使用して暗号化されたエフェメラルボリュームにWebサーバーログを書き込みます。
Test2-34. <p>An organization has the requirement to store 10TB worth of scanned files. There is a requirement to have a search application in place which can be used to search through the scanned files. <br></p><p>Which of the below is the best option for implementing the search facility?</p> | <p>組織は10TB相当のスキャン済みファイルを保存する必要があります。スキャンしたファイルを検索するための検索アプリケーションを用意する必要があります。<br> </p> <p>検索機能を実装するための最良の選択肢はどれですか？</p>	sa:	Use S3 with standard redundancy to store and serve the scanned files. Use CloudSearch for query  processing, and use Elastic Beanstalk to host the website across multiple availability zones. <br>  標準の冗長性を備えたS3を使用して、スキャンしたファイルを保存して提供します。CloudSearchを使用してクエリ処理を行い、Elastic Beanstalkを使用して複数の可用性ゾーンにわたってWebサイトをホストします。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-07a9-0fa9-0971-c96ff3709e9b"></span></p><p dir="ltr" style="">This question presents following scenarios: (1) type of storage that can store large amount of data (10TB), (2) the commercial search product is at its end of life, (3) the architecture should be cost effective, highly available, and durable.</p><p dir="ltr" style="">Tip: Whenever a storage service that can store large amount of data with low cost, high availability, and high durability, always think about using S3.&nbsp;</p><br><p dir="ltr" style="">Option A is incorrect because even though it uses S3, it uses the commercial search software which is at its end of life.</p><p dir="ltr" style="">Option B is incorrect because striped EBS is not as durable solution as S3 and certainly not as cost effective as S3. Also, it has maintenance overhead.</p><p dir="ltr" style="">Option C is CORRECT because (a) it uses S3 to store the images, (b) instead of the commercial product that is at its end of life, it uses CloudSearch for query processing, and (c) with multi AZ implementation, it achieves high availability.</p><p dir="ltr" style="">Option D is incorrect because with single AZ RDS instance, it does not have high availability.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;"><b>Amazon CloudSearch</b></span></p><p dir="ltr" style="">With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance. With a few clicks in the&nbsp;<a href="https://aws.amazon.com/console/">AWS Management Console</a>, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.</p><p dir="ltr" style="">You can easily change your search parameters, fine tune search relevance, and apply new settings at any time. As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on AWS CloudSearch, please visit the below link</p><p dir="ltr" style=""><a href="https://aws.amazon.com/cloudsearch/" target="_blank">https://aws.amazon.com/cloudsearch/</a></p><br><br><p></p><ul></ul><p></p>	Model the environment using CloudFormation. Use an EC2 instance running Apache webserver and an open source search application, stripe multiple standard EBS volumes together to store the scanned files with a search index. <br>  CloudFormationを使用して環境をモデル化する。Apache Webサーバーとオープンソース検索アプリケーションを使用して複数の標準EBSボリュームをストライプ化し、スキャンしたファイルを検索インデックスとともに格納します。	Use S3 with reduced redundancy lo store and serve the scanned files. Install a commercial search  application on EC2 Instances and configure with auto-scaling and an Elastic Load Balancer. <br>  S3を使用して冗長性を減らし、スキャンしたファイルを提供してください。EC 2自動スケーリングとElastic Load Balancerを使用してインスタンス化し、構成します。	Use a single-AZ RDS MySQL instance to store the search index for the scanned files and use an EC2 instance with a custom application to search based on the index. <br>  1つのAZ RDS MySQLインスタンスを使用してスキャンしたファイルの検索インデックスを格納し、カスタムアプリケーションとともにEC2インスタンスを使用してインデックスに基づいて検索します。
Test2-35. <p>You are designing network connectivity for your fat client application. The application is designed for business travelers who must be able to connect to it from their hotel rooms, cafes, public Wi-Fi hotspots, and elsewhere on the Internet. While you do not want to publish the application on the Internet.</p> <p>Which network design meets the above requirements while minimizing deployment and operational costs? Choose the correct answer from the options below</p> | <p>ファットクライアントアプリケーションのネットワーク接続を設計しています。このアプリケーションは、ホテルの客室、カフェ、公共Wi-Fiホットスポット、およびインターネットの他の場所から接続できる必要があるビジネストラベラー向けに設計されています。あなたはインターネット上でアプリケーションを公開したくはありません。</p> <p>展開と運用コストを最小限に抑えながら、どのネットワーク設計が上記の要件を満たしていますか？下記のオプションから正解を選択してください</p>	sa:	Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it. <br>  VPCのパブリックサブネットにSSL VPNソリューションを設定し、すべてのユーザコンピュータにSSL VPNクライアントソフトウェアをインストールして設定します。 VPCにプライベートサブネットを作成し、アプリケーションサーバーをそのサブネットに配置します。 |<p><br></p><p>Option A is incorrect because AWS Direct Connect is not a cost effective solution compared to using VPN solution.</p><p>Option B is incorrect because it does not mention how the application would be accessible only to the business travelers and not to the public.</p><p>Option C is incorrect because if the application servers are put in the public subnet, they would be publicly accessible via the internet.</p><p>Option D is CORRECT because configuring the SSL VPN solution is cost-effective and allows access only to the business travelers and since the application servers are in private subnet, the application is not accessible via the internet.</p><p><br></p>	Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application. <br>  アプリケーションへのバックエンド接続を終了するSSLリスナーを使用してElastic Load Balancingを実装します。	Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it. <br>  IPsec VPN接続を構成し、ユーザーに構成の詳細を提供します。 VPCにパブリックサブネットを作成し、アプリケーションサーバーを配置します。	Implement AWS Direct Connect, and create a private interface to your VPC. Create a public subnet and place your application servers in it. <br>  AWS Direct Connectを実装し、VPCへのプライベートインターフェイスを作成します。 パブリックサブネットを作成し、アプリケーションサーバーを配置します。
Test2-36. <p>Which of the below components is used by AWS Data Pipeline to poll for tasks and then performs those tasks?</p> | <p> AWS Data Pipelineは、タスクをポーリングしてそれらのタスクを実行するために以下のコンポーネントのどれを使用しますか？</p>	sa:	Task Runner <br>  タスクランナー|<p>Task Runner is a task agent application that polls AWS Data Pipeline for scheduled tasks and executes them on Amazon EC2 instances, Amazon EMR clusters, or other computational resources, reporting status as it does so.</p><p>&nbsp;</p><p>For more information on the Taskrunner in AWS pipeline, please refer to the below link</p><p></p><span style="font-size: 1rem;"><a href="http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html" target="_blank">http://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-using-task-runner.html</a></span><br><p></p>	S3 <br>  S3	Definition Syntax File <br>  定義構文ファイル	AWS OpsWork <br>  AWS OpsWork
Test2-37. <p>An organization has created multiple components of a single application. Currently, all the components are hosted on a single EC2 instance. Due to security reasons, the organization wants to implement 2 separate SSL certificates for the separate modules. <br></p><p>How can the organization achieve this with a single instance? <br></p> | <p>組織が単一のアプリケーションの複数のコンポーネントを作成しました。現在、すべてのコンポーネントは1つのEC2インスタンスでホストされています。セキュリティ上の理由から、組織は別々のモジュール用に2つの個別のSSL証明書を実装する必要があります。<br> </p> <p>組織はどのようにして単一インスタンスでこれを達成できますか？<br> </p>	sa:	Create an EC2 instance which has multiple network interfaces with multiple elastic IP addresses. <br>  複数の弾性IPアドレスを持つ複数のネットワークインターフェイスを持つEC 2インスタンスを作成します。|<p><br></p><p><span id="docs-internal-guid-86cab175-07bf-9078-7283-0772ad150351"></span></p><p dir="ltr" style="">It can be useful to assign multiple IP addresses to an instance in your VPC to do the following:</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">(1) Host multiple websites on a single server by using multiple SSL certificates on a single server and associating each certificate with a specific IP address.</p><p dir="ltr" style="">(2) Operate network appliances, such as firewalls or load balancers, that have multiple IP addresses for each network interface.</p><p dir="ltr" style="">(3) Redirect internal traffic to a standby instance in case your instance fails, by reassigning the secondary IP address to the standby instance.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">Option A is CORRECT because, as mentioned above, if you have multiple elastic network interfaces (ENIs) attached to the EC2 instance, each network IP can have a component running with a separate SSL certificate.</p><p dir="ltr" style="">Option B is incorrect because having separate rules in security group as well as NACL does not mean that the instance supports multiple SSLs.</p><p dir="ltr" style="">Option C is incorrect because an EC2 instance cannot have multiple subnets.</p><p dir="ltr" style="">Option D is incorrect because NAT address is not related to supporting multiple SSLs.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on Multiple IP Addresses, please refer to the link below:</p><p dir="ltr" style=""><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html</a></p><p></p><ul></ul><p></p>	Create an EC2 instance which has both an ACL and the security group attached to it and have separate rules for each IP address. <br>  ACLとそれに接続されたセキュリティグループの両方を持つEC 2インスタンスを作成し、各IPアドレスに対して個別のルールを設定します。	Create an EC2 instance which has multiple subnets attached to it and each will have a separate IP address. <br>  複数のサブネットが接続されたEC 2インスタンスを作成し、それぞれに別々のIPアドレスを割り当てます。	Create an EC2 instance with a NAT address. <br>  NATアドレスを持つEC 2インスタンスを作成します。
Test2-38. <p><span id="docs-internal-guid-846a0cce-30c6-f398-54a8-3c3082d9eb14"><span id="docs-internal-guid-846a0cce-30c7-6227-5c3b-20ad5b60d707"><span id="docs-internal-guid-846a0cce-30ca-2933-e024-3d94026bd35e">A company has configured and peered two VPCs: VPC-1 and VPC-2. The VPC-1 contains only private subnets, and VPC-2 contains only public subnets. The company uses a single AWS Direct Connect connection and private virtual interface to connect their on-premises network with VPC-1. Which two methods increase the fault tolerance of the connection to VPC-1? Choose 2 answers:</span></span></span></p> | <span id = "docs-internal-guid-846a0cce-30c6-f398-54a8-3c3082d9eb14"> <span id = "docs-internal-guid-846a0cce-30c7-6227-5c3b-20ad5b60d707"> <span id = "docs-internal-guid-846a0cce-30ca-2933-e024-3d94026bd35e">企業では、VPC-1とVPC-2という2つのVPCが設定され、ピアリングされています。VPC-1にはプライベートサブネットのみが含まれ、VPC-2にはパブリックサブネットのみが含まれます。同社は、単一のAWS Direct Connect接続とプライベート仮想インターフェイスを使用して、自社のネットワークをVPC-1に接続します。どの2つの方法でVPC-1への接続のフォールトトレランスが向上しますか？2つの回答を選択してください：</ span> </ span> </ span> </p>	ma:	x:Establish a hardware VPN over the internet between VPC-2 and the on-premises network. <br>  VPC-2とオンプレミスネットワーク間でハードウェアVPNを確立します。|<p><br></p><p><span id="docs-internal-guid-846a0cce-30cc-1e6f-03d7-3ca3f66c39c2"></span></p><p dir="ltr" style="">Option A &amp; C are incorrect because peered VPC does not support Edge-to-Edge Routing, so connecting to VPC2 will not work.</p><p dir="ltr" style="">Option B is CORRECT because hardware VPN can be created to connect the VPC-1 with the on-premises network.</p><p dir="ltr" style="">Option D is incorrect because AWS Direct Connect is a regional service and you cannot reach VPC1 if the direct connect is in a different region.</p><p dir="ltr" style="">Option E is CORRECT because AWS Direct Connect is a regional service and will work if it is in the same region as that of the VPC-1.</p><br><br><p dir="ltr" style="">For more information on VPC peering, please see the links below:</p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/invalid-peering-configurations.html" target="_blank">https://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/invalid-peering-configurations.html</a><br><p></p><br><p></p>	o:Establish a hardware VPN over the internet between VPC-1 and the on-premises network. <br>  VPC-1とオンプレミスネットワーク間でハードウェアVPNを確立します。	x:Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2. <br>  VPC-2と同じ地域に新しいAWS Direct Connect接続とプライベート仮想インターフェイスを確立します。	x:Establish a new AWS Direct Connect connection and private virtual interface in a different AWS region than VPC-1. <br>  新しいAWS Direct Connect接続とプライベート仮想インターフェイスをVPC-1とは異なるAWS領域に確立する。	o:Establish a new AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1. <br>  VPC-1と同じAWS地域に新しいAWS Direct Connect接続とプライベート仮想インターフェイスを確立します。
Test2-39. <p><span id="docs-internal-guid-846a0cce-312a-7074-2e43-ae8ba95e6458">Your company produces customer commissioned one-of-a-kind skiing helmets combining nigh fashion with custom technical enhancements Customers can show off their individuality on the ski slopes and have access to head-up-displays. GPS rear-view cams and any other technical innovation they wish to embed in the helmet. The current manufacturing process is data rich and complex including assessments to ensure that the custom electronics and materials used to assemble the helmets are to the highest standards. Assessments are a mixture of human and the automated assessments.You need to add a new set of assessment to model the failure modes of the electronics using GPUs with CUDA across a cluster of servers with low latency networking. What architecture would allow you to automate the existing process using a hybrid approach and ensure that the architecture can support the evolution of processes over time? </span></p> | <p> <span id = "docs-internal-guid-846a0cce-312a-7074-2e43-ae8ba95e6458">あなたの会社は、ファッション性を兼ね備えた特製のスキー用ヘルメットと、彼らの個性はスキー場にあり、ヘッドアップディスプレイにアクセスできます。GPSのリアビューカメラ、ヘルメットに埋め込むことを望む技術革新 現在の製造プロセスは、ヘルメットを組み立てるために使用されるカスタム電子機器および材料が最高基準になるようにする評価を含む、データが豊富で複雑です。評価は、人間と自動評価の両方を組み合わせたものです。待ち時間の短いネットワーキングを持つサーバーのクラスタ全体で、CUDAを使用してGPUを使用してエレクトロニクスの障害モードをモデル化するための新しい評価セットを追加する必要があります。ハイブリッドアプローチを使用して既存のプロセスを自動化し、アーキテクチャが時間の経過とともにプロセスの進化をサポートできるようにするには、どのようなアーキテクチャが必要ですか？</ span> </p>	sa:	Use Amazon Simple Workflow (SWF) to manage assessments, movement of data & meta-data. Use an auto-scaling group of G2 instances in a placement group. <br>  Amazon Simple Workflow（SWF）を使用して、アセスメント、データおよびメタデータの移動を管理します。プレースメントグループ内のG2インスタンスの自動スケーリンググループを使用します。|<br><p dir="ltr" style="">The main point to consider in this question is that the assessments include human interaction as well. In most of such cases always look for SWF in the options.</p><br><p dir="ltr" style="">Option A is incorrect because this will be useful during the batch jobs which deal with the automated assessments. For the human assessment, this will not be a useful option.</p><p dir="ltr" style="">Option B is CORRECT because (a) &nbsp;it enables assessment via human interaction, (b) uses autoscaled G2 instances that are efficient in automated assessments due to their GPU and low latency networking.</p><p dir="ltr" style="">Option C is incorrect because, C3 instances and SR-IOV will not provide required GPU.</p><p dir="ltr" style="">Option D is incorrect because, (a) this will be useful during the batch jobs which deal with the automated assessments. For the human assessment, this will not be a useful option, and (b) C3 instances and SR-IOV will not provide required GPU.</p></span><br><p></p>	Use AWS Data Pipeline to manage movement of data & meta-data and assessments. Use an autoscaling group of G2 instances in a placement group. <br>  aWSデータパイプラインを使用して、データとメタデータとアセスメントの移動を管理します。プレースメントグループ内のG2インスタンスの自動拡張グループを使用します。	Use Amazon Simple Workflow (SWF) lo manages assessments movement of data & meta-data. Use an auto-scaling group of C3 instances with SR-IOV (Single Root I/O Virtualization). <br>  Amazon Simple Workflow（SWF）を使用すると、データとメタデータの評価移動を管理します。SR-IOV（シングルルートI / O仮想化）を備えたC3インスタンスの自動スケーリンググループを使用します。	Use AWS data Pipeline to manage movement of data & meta-data and assessments. Use autoscaling group of C3 with SR-IOV (Single Root I/O virtualization). <br>  AWSデータパイプラインを使用して、データとメタデータと評価の移動を管理します。SR-IOV（シングルルートI / O仮想化）でC3の自動拡張グループを使用します。
Test2-40. <p><span id="docs-internal-guid-846a0cce-30d0-9ded-4266-6b2c442be0b0">A web company is looking to implement an external payment service into their highly available application deployed in a VPC. Their application EC2 instances are behind a public facing ELB. Auto scaling is used to add additional instances as traffic increases under normal load. The application runs 2 instances in the Auto Scaling group but at peak it can scale 3x in size. The application instances need to communicate with the payment service over the Internet which requires whitelisting of all public IP addresses used to communicate with it. A maximum of 4 whitelisting IP addresses are allowed at a time and can be added through an API. How should they architect their solution?</span></p> | <p> <span id = "docs-internal-guid-846a0cce-30d0-9ded-4266-6b2c442be0b0"> Web会社は、VPCに配備された高可用性アプリケーションに外部支払いサービスを実装しようとしています。彼らのアプリケーションEC2のインスタンスは、一般向けのELBの背後にあります。自動スケーリングは、通常の負荷でトラフィックが増加するにつれてインスタンスを追加するために使用されます。アプリケーションはAuto Scalingグループで2つのインスタンスを実行しますが、ピーク時には3倍に拡大できます。アプリケーションインスタンスは、インターネット上の支払いサービスと通信する必要があり、通信に使用されるすべてのパブリックIPアドレスをホワイトリストに登録する必要があります。一度に許可されるホワイトリストIPアドレスは最大4つあり、API経由で追加できます。彼らはどのようにソリューションを設計するべきですか？</ span> </p>	sa:	Route payment requests through two NAT instances setup for High Availability and whitelist the Elastic IP addresses attached to the NAT instances. <br>  高可用性用に設定された2つのNATインスタンスを介して支払い要求をルーティングし、NATインスタンスに接続された弾性IPアドレスをホワイトリストに登録します。|<p><br></p><p></p><span id="docs-internal-guid-846a0cce-30e3-522c-529d-2503bdadbcbb"><p dir="ltr" style="">Option A is CORRECT because (a) the requests originated from the instances in the subnet would be routed through the NAT, so the requests would have the NAT’s IP address (which is whitelisted), and (b) two NAT instances would provide high availability. </p><p dir="ltr" style="">Option B is incorrect because (a) Internet Gateway (IGW) can only route the traffic, it cannot whitelist any particular IP and payment requests, and (b) EC2 instances with public IP addresses in a public subnet are routed through the gateway, but will keep their own IP address, so they will not get whitelisted.</p><p dir="ltr" style="">Option C is incorrect because the outbound traffic cannot be routed through an ELB.</p><p dir="ltr" style="">Option D is incorrect because, the ASG will have 6 servers during the peak load, and the payment service only allows 4 to be whitelisted; so, it will exceed the allowed 4 IP addresses.</p></span><br><a href="https://aws.amazon.com/elasticache/" target="_blank" style="font-size: 1rem;"></a><br><p></p>	Whitelist the VPC Internet Gateway Public IP and route payment requests through the Internet Gateway. <br>  VPCインターネットゲートウェイパブリックIPをホワイトリストに登録し、インターネットゲートウェイ経由で支払い要求をルーティングします。	Whitelist the ELB IP addresses and route payment requests from the Application servers through the ELB. <br>  ELBのIPアドレスをホワイトリストに登録し、ELBを介してアプリケーションサーバーから支払い要求をルーティングします。	Automatically assign public IP addresses to the application instances in the Auto Scaling group and run a script on boot that adds each instances public IP address to the payment validation whitelist API. <br>  Auto ScalingグループのアプリケーションインスタンスにIPアドレスを自動的に割り当て、起動時に各インスタンスのパブリックIPアドレスを支払い検証ホワイトリストAPIに追加するスクリプトを実行します。
Test2-41. <p>Which of the following AWS services can be used to define alarms to trigger on a certain activity in the AWS Data pipeline?</p> | <p> AWSデータパイプラインの特定のアクティビティでトリガするアラームを定義するために使用できる、以下のAWSサービスのどれですか？</p>	sa:	SNS <br>  SNS|<p><br></p><p>Amazon Simple Notification Service (Amazon SNS) is a fast, flexible, fully managed push notification service that lets you send individual messages or to fan-out messages to large numbers of recipients. Amazon SNS makes it simple and cost effective to send push notifications to mobile device users, email recipients or even send messages to other distributed services.</p><p>&nbsp;</p><p><span style="font-size: 1rem;">For more information on SNS, please refer to the below link</span></p><p></p><a href="https://aws.amazon.com/sns/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/sns/</a><br><br><p></p>	SQS <br>  SQS	SES <br>  ITS	CodeDeploy <br>  CodeDeploy
Test2-42. <p><span id="docs-internal-guid-846a0cce-30e8-dd5b-92a9-1359853e8f7e"></span></p><p dir="ltr" style="">Your company currently has a 2-tier web application running in an on-premises data center. You have experienced several infrastructure failures in the past two months resulting in significant financial losses. Your CIO is strongly agreeing to move the application to AWS. While working on achieving buy-in from the other company executives, he asks you to develop a disaster recovery plan to help improve Business continuity in the short term. He specifies a target Recovery Time Objective (RTO) of 4 hours and a Recovery Point Objective (RPO) of 1 hour or less. He also asks you to implement the solution within 2 weeks. Your database is 200GB in size and you have a 20Mbps Internet connection. How would you do this while minimizing costs?</p><br><p></p> | <p> <span id = "docs-internal-guid-846a0cce-30e8-dd5b-92a9-1359853e8f7e"> </ span> </p> <p dir = "ltr" style = "">オンプレミスデータセンターで実行されている2層Webアプリケーション 過去2ヶ月間にいくつかのインフラストラクチャの障害が発生したため、大きな財務的損失が発生しました。CIOは、アプリケーションをAWSに移行することに強く同意しています。他の会社の役員からバイインを購入する際には、短期間でビジネス継続性を向上させるための災害復旧計画を作成するように求められます。目標復旧時間目標（RTO）を4時間、復旧ポイント目標（RPO）を1時間以下と指定しています。また、2週間以内にソリューションの実装を依頼します。あなたのデータベースのサイズは200GBで、20Mbpsのインターネット接続があります。費用を最小限に抑えながらこれをどうやって行うのですか？</p> <br> <p> </p>	sa:	Create an EBS backed private AMI which includes a fresh install of your application. Develop a CloudFormation template which includes your AMI and the required EC2, AutoScaling, and ELB resources to support deploying the application across Multiple- Availability-Zones. Asynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection. <br>  AMIであるCloudFormationテンプレートと、複数の可用性ゾーンにわたるアプリケーションのデプロイメントをサポートするために必要なEC2、AutoScaling、およびELBリソースを作成します。オンプレミスデータベースからセキュリティ保護されたVPN接続を介してAWSのデータベースインスタンスにトランザクションを非同期でレプリケートします。|<br><p dir="ltr" style="">Option A is CORRECT because (a) with AMIs, the newly created EC2 instances will be ready with the pre-installed application; thus, reducing the RTO, (b) with CloudFormation, the entire stack can be automatically provisioned, and (c) since there are no additional services used, the cost will stay low.</p><br><p dir="ltr" style="">Option B is incorrect because although this could work, (a) deploying EC2 instances for this scenario will be expensive, and (b) in case of disaster, the recovery will potentially be slower, since the new EC2 need to be manually updated with the application software and patches, especially since it does not use the AMIs.</p><br><p dir="ltr" style="">Option C is incorrect because it has a low performance issue. (a) Backing up local DB of 200GB on a 20Mbps connection every hour will be very slow, and (b) even with the incremental backup, recovering from the incremental backup take times and might not satisfy the given RTO.</p><br><p dir="ltr" style="">Option D is incorrect because (a) the EC2 instance is a single point of failure, which needs to be made highly available via an autoscaling, and (b) it can only handle the average load of the application; so, in case of peak load, it may fail, and (c) AWS Direct Connection will be an expensive solution compared to the setup of option A.</p></span><br><p></p>	Deploy your application on EC2 instances within an Auto Scaling group across multiple availability zones. Asynchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure VPN connection. <br>  オンプレミスデータベースからセキュリティ保護されたVPN接続を介してAWSのデータベースインスタンスにトランザクションを非同期でレプリケートします。	Create an EBS backed private AMI which includes a fresh install of your application. Setup a script in your data center to backup the local database every 1 hour and to encrypt and copy the resulting file to an S3 bucket using multi-part upload. <br>  マルチパートアップロードを使用してEBSバックアップのプライベートAMIを作成します。	Install your application on a compute-optimized EC2 instance capable of supporting the application’s average load. Synchronously replicate the transactions from your on-premises database to a database instance in AWS across a secure Direct Connect connection. <br>  セキュアなダイレクトコネクト接続を介して、オンプレミスデータベースからAWSのデータベースインスタンスにトランザクションを同期レプリケートします。
Test2-43. <p>An organization is planning to setup a management network on the AWS VPC. The organization is trying to secure the web server on a single VPC instance such that it allows the internet traffic as well as the back-end management traffic. The organization wants to make sure that the back end management network interface can receive the SSH traffic only from a selected IP range, while the internet facing web server will have an IP address which can receive traffic from all the internet IPs.</p><p>How can the organization achieve this by running the web server on a single instance?</p> | <p>組織は、AWS VPCで管理ネットワークを設定する予定です。組織は、インターネットトラフィックとバックエンド管理トラフィックを許可するように、単一のVPCインスタンスでWebサーバーを保護しようとしています。組織はバックエンド管理ネットワークインターフェイスが選択されたIP範囲からのみSSHトラフィックを受信できるようにしたいと考えていますが、インターネットに直面しているWebサーバーはすべてのインターネットIPからのトラフィックを受信できるIPアドレスを持っています。 <p>組織はどのようにして単一のインスタンスでWebサーバーを実行することでこれを達成できますか？</p>	sa:	The organization should create 2 network interfaces, one for the internet traffic and the other for the backend traffic <br>  組織は、インターネットトラフィック用とバックエンドトラフィック用の2つのネットワークインターフェイスを作成する必要があります|<p><br></p><p>An Elastic Network Interface (ENI) is a virtual network interface that you can attach to an instance in a VPC. Network interfaces are available only for instances running in a VPC.</p><p>A network interface can include the following attributes:</p> <ul> <li>A primary private IPv4 address</li> <li>One or more secondary private IPv4 addresses</li> <li>One Elastic IP address (IPv4) per private IPv4 address</li> <li>One public IPv4 address</li> <li>One or more IPv6 addresses</li> <li>One or more security groups</li> <li>A MAC address</li> <li>A source/destination check flag</li> <li>A description</li> </ul> <p><br></p><p>See an example below how the route table can be configured to allow the IP based access via multiple ENIs.</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/2_43_1.png" alt="" width="769" height="642" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>For more information on ENI , please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a><br><p></p>	It is not possible to have 2 IP addresses for a single instance <br>  1つのインスタンスに2つのIPアドレスを割り当てることはできません	The organization should create 2 EC2 instances as this is not possible with one EC2 instance <br>  2つのEC 2インスタンスを作成する必要があります。これは、EC 2インスタンスではないためです	This is not possible <br>  これは不可能です
Test2-44. <p><span id="docs-internal-guid-846a0cce-30f1-5d45-9847-ae9b6f3eae77"></span></p><p dir="ltr" style="">A web design company currently runs several FTP servers that their 250 customers use to upload and download large graphic files They wish to move this system to AWS to make it more scalable, but they wish to maintain customer privacy and keep the costs to a minimum.</p><p dir="ltr" style="">What AWS architecture would you recommend?</p><br><p></p> | </ span> <p dir = "ltr" style = "">現在ウェブデザイン会社250人のお客様が大規模なグラフィックファイルのアップロードとダウンロードに使用する複数のFTPサーバーを実行しています。このシステムをAWSに移行して、スケーラビリティを向上させたいと考えていますが、顧客のプライバシーを維持し、どのようなAWSアーキテクチャをお勧めしますか？</p> <br> <p> </p>	sa:	Ask their customers to use an S3 client instead of an FTP client. Create a single S3 bucket. Create an IAM user for each customer. Put the IAM Users in a Group that has an IAM policy that permits access to subdirectories within the bucket via use of the ‘username’ policy variable. <br>  顧客にFTPクライアントの代わりにS3クライアントを使用するように依頼します。 単一のS3バケットを作成します。 顧客ごとにIAMユーザーを作成します。 IAMユーザーを、「username」ポリシー変数を使用してバケット内のサブディレクトリへのアクセスを許可するIAMポリシーを持つグループに入れます。|<p dir="ltr" style=""><br></p><p dir="ltr" style="">The main considerations in this scenario are: (1) the architecture should be scalable, (2) customer privacy should be maintained, and (3) the solution should be cost-effective.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is CORRECT because (a) it creates permissions via IAM policy where each user will have access to only those subdirectory named with their username, and (b) S3 is a cost-effective and highly scalable solution. </p><p dir="ltr" style="">Note: Even though creating one IAM User per user/customer is not the best way forwards, but given the other choices, this is the best option.</p><p dir="ltr" style="">Option B is incorrect because even though it uses RRS which is a less expensive solution than S3, creating one bucket per user is not a scalable architecture. Currently, the number of customers is 250, but in future the number can grow and if it does, it will put limits on the number of buckets.</p><p dir="ltr" style="">Option C is incorrect because creating auto-scaling group of FTP servers is a costly solution compared to creating buckets on S3 and appropriate IAM policies.</p>Option D is incorrect because (a) creating one bucket per user is not a scalable architecture. Currently, the number of customers is 250, but in future the number can grow and if it does, it will put limits on the number of buckets, and (b) you configure buckets to be Requester Pays when you want to share the data but not incur charges associated with others accessing the data. This will keep the cost down for the company, but will increase the cost for the customer who will access the buckets.<br></span><br><p></p>	Create a single S3 bucket with Reduced Redundancy Storage turned on and ask their customers to use an S3 client instead of an FTP client Create a bucket for each customer with a Bucket Policy that permits access only to that one customer. <br>  Redundancy Storageがオンになっているバケットを作成し、FTPクライアントの代わりにS3クライアントを使用するよう顧客に依頼します。その顧客にのみアクセスを許可するバケットポリシーを使用して、各顧客のバケットを作成します。	Create an auto-scaling group of FTP servers with a scaling policy to automatically scale-in when minimum network traffic on the auto-scaling group is below a given threshold. Load a central list of ftp users from S3 as part of the user data startup script on each Instance. <br>  スケーリングポリシーを使用して自動スケーリンググループを作成し、自動スケーリンググループの最小ネットワークトラフィックが所定のしきい値を下回ったときに自動的にスケールインします。 各インスタンスのユーザーデータスタートアップスクリプトの一部として、S3からftpユーザーの中央リストをロードします。	Create a single S3 bucket with Requester Pays turned on and ask their customers to use an S3 client instead of an FTP client. Create a bucket for each customer with a bucket policy that permits access only to that one customer. <br>  リクエスタペイをオンにして単一のS3バケットを作成し、顧客にFTPクライアントの代わりにS3クライアントを使用するように依頼します。 その顧客にのみアクセスを許可するバケットポリシーを使用して、各顧客のバケットを作成します。
Test2-45. <p>There is a requirement for an application hosted on a VPC to access the On-premise LDAP server. The VPC and the on-premise location are connected via an IPSec VPN. Which of the below are the right options for the application to authenticate each user?</p><p>Choose 2 options from the below:</p> | <p> VPCにホストされているアプリケーションがオンプレミスLDAPサーバーにアクセスするための要件が​​あります。VPCと構内ロケーションは、IPSec VPNを介して接続されています。</p> <p>下記の2つのオプションを選択してください：</p>	ma:	x:Develop an identity broker that authenticates against IAM security Token service to assume a IAM role in order to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials. <br>  一時的なAWSセキュリティ資格を取得するために、IAMセキュリティトークンサービスに対して認証するIDブローカーを開発してIAMロールを引き継ぐ。 アプリケーションはアイデンティティブローカーを呼び出して、AWSの一時的なセキュリティ資格情報を取得します。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-07c2-6b10-99ea-85fd28b61ca5"></span></p><p dir="ltr" style="">There are two architectural considerations here: (1) The users must be authenticate via the on-premise LDAP server, and (2) each user should have access to S3 only.</p><p dir="ltr" style="">With this information, it is important to first authenticate the users using LDAP, get the IAM Role name, then get the temporary credentials from STS, and finally access the S3 bucket using those credentials. And second, create an IAM Role that provides access to S3.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">Option A is incorrect because the users need to be authenticated using LDAP first, not STS. Also, the temporary credentials to log into AWS are provided by STS, not identity broker.</span></p><p dir="ltr" style="">Option B is CORRECT because it follows the correct sequence. It authenticates users using LDAP, gets the security token from STS, and then accesses the S3 bucket using the temporary credentials.</p><p dir="ltr" style="">Option C is CORRECT because it follows the correct sequence. It develops an identity broker that authenticates users against LDAP, gets the security token from STS, and then accesses the S3 bucket using the IAM federated user credentials.</p><p dir="ltr" style="">Option D is incorrect because you cannot use the LDAP credentials to log into IAM.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">An example diagram of how this works from the AWS documentation is given below.</span></p><p dir="ltr" style=""><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/2_45_1.png" alt="" width="641" height="615" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">For more information on federated access, please visit the below link:</span></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a><br><p></p><ul></ul><p></p>	o:The application authenticates against LDAP and retrieves the name of an IAM role associated with the user. The application then calls the IAM Security Token Service to assume that IAM role. The application can use the temporary credentials to access any AWS resources. <br>  アプリケーションはLDAPに対して認証を行い、そのユーザーに関連付けられたIAMロールの名前を取得します。 次に、アプリケーションはIAMセキュリティトークンサービスを呼び出して、そのIAMロールを想定します。 アプリケーションは、一時的な資格情報を使用してAWSリソースにアクセスできます。。	o:Develop an identity broker that authenticates against LDAP and then calls IAM Security Token Service to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate AWS service. <br>  LDAPに対して認証を行い、IAMセキュリティトークンサービスを呼び出してIAMフェデレーションされたユーザー資格情報を取得するIDブローカーを開発します。 アプリケーションはアイデンティティブローカーを呼び出して、適切なAWSサービスにアクセスできるIAMフェデレーションされたユーザーの資格情報を取得します。	x:The application authenticates against LDAP the application then calls the AWS identity and Access Management (IAM) Security service to log in to IAM using the LDAP credentials the application can use the IAM temporary credentials to access the appropriate AWS service. <br>  アプリケーションはLDAPに対して認証され、アプリケーションはAWS IDとアクセス管理（IAM）セキュリティサービスを呼び出してLDAP資格情報を使用してIAMにログインし、アプリケーションはIAM一時資格情報を使用して適切なAWSサービスにアクセスできます。
Test2-46. <p>In Amazon Cognito, your mobile app authenticates with the Identity Provider (IdP) using the provider’s SDK. Once the end user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito. Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?</p> | <p> Amazon Cognitoでは、モバイルアプリはプロバイダのSDKを使用してアイデンティティプロバイダ（IdP）で認証されます。エンドユーザがIdPで認証されると、IdPから返されたOAuthまたはOpenID ConnectトークンがアプリによってAmazon Cognitoに渡されます。ユーザが一連の一時的な限定特権のAWS認証情報を提供するために返されるのは次のうちどれですか？</p>	sa:	Cognito Identity ID <br>  Cognito ID|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">If you're allowing unauthenticated users, you can retrieve a unique Amazon Cognito identifier (identity ID) for your end user immediately. If you're authenticating users, you can retrieve the identity ID after you've set the login tokens in the credentials provider</span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/2_46_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on Cognito ID, please refer to the below link:</p><p></p><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html</a><br><p></p>	Cognito Key pair <br>  Cognito Keyのペア	Cognito SDK <br>  SDKの意識	Cognito API <br>  APIの意識
Test2-47. <p><span id="docs-internal-guid-08d78d15-32c6-a2c9-5219-3a1c5fb3ad9c">You have been asked to design network connectivity between your existing data centers and AWS. Your application's EC2 instances must be able to connect to existing backend resources located in your data center. Network traffic between AWS and your data centers will start small, but ramp up to 10s of GB per second over the course of several months. The success of your application is dependent upon getting to market quickly. Which of the following design options will allow you to meet your objectives?</span></p> | <p> <span id = "docs-internal-guid-08d78d15-32c6-a2c9-5219-3a1c5fb3ad9c">既存のデータセンターとAWS間のネットワーク接続を設計するように求められました。アプリケーションのEC2インスタンスは、データセンターにある既存のバックエンドリソースに接続できる必要があります。AWSとお客様のデータセンター間のネットワークトラフィックは小さくなりますが、数カ月の間に1秒あたり最大10GBのGBまで増加します。アプリケーションの成功は、市場投入の迅速化に依存します。あなたの目標を達成するための設計オプションはどれですか？</ span> </p>	sa:	Provision a VPN connection between a VPC and existing on-premises equipment, submit a Direct Connect partner request to provision cross connects between your data center and the Direct Connect location, then cut over from the VPN connection to one or more Direct Connect connections as needed. <br>  VPCと既存のオンプレミス間のVPN接続を提供し、データセンターとダイレクトコネクトの間のクロスコネクトをプロビジョニングするためのダイレクトコネクトパートナー要求を送信し、必要に応じてVPN接続から1つ以上のダイレクトコネクトへ切り替える。|<br><p dir="ltr" style="">The most important considerations in this scenario are: (1) the network traffic would be initially small, and will increase increase in future, and (2) the application should be up quickly, so time is critical. One thing should be noted that it takes time initially to set up the AWS Direct Connect (See the link below for latest information).</p><p dir="ltr" style=""><a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html" target="_blank">https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html</a></p><br><p dir="ltr" style="">Option A is incorrect because setting up of Direct Connect will take time; so, the backend servers will not be connected in quick time.</p><p dir="ltr" style="">Option B is incorrect because provisioning VPN only is not a long term solution since the traffic would increase to over 10Gbps.</p><p dir="ltr" style="">Option C is CORRECT because (a) it provides quick connection between the on-premise data center and AWS via VPN, and (b) it also initiates the provision of a Direct Connect solution to tackle the requirement of higher bandwidth (for 10Gbps network) for later.</p><p dir="ltr" style="">Option D is incorrect because setting up of Direct Connect will take time and the application will not be up within time as it is time critical.</p><br><p dir="ltr" style="">For more information on VPN and Direct Connect, please visit the link below:</p><p dir="ltr" style=""><a href="https://datapath.io/resources/blog/aws-direct-connect-vs-vpn-vs-direct-connect-gateway/" target="_blank">https://datapath.io/resources/blog/aws-direct-connect-vs-vpn-vs-direct-connect-gateway/</a><br></p></span><br><br><p></p>	Allocate EIPs and an Internet Gateway for your VPC instances to use for quick, temporary access to your backend applications, then provision a VPN connection between a VPC and existing on -premises equipment. <br>  VPCインスタンスがバックエンドアプリケーションにすばやく一時的にアクセスするために使用するEIPとインターネットゲートウェイを割り当て、次にVPCと既存のオンプレミスの装置間にVPN接続をプロビジョニングします。	Quickly create an internal ELB for your backend applications, submit a Direct Connect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed. <br>  バックエンドアプリケーション用に内部ELBをすばやく作成し、データセンターとVPC間の1 GbpsクロスコネクトをプロビジョニングするためのDirect Connectリクエストを送信し、必要に応じてダイレクトコネクト接続の数またはサイズを増やします。	Quickly submit a Direct Connect request to provision a 1 Gbps cross connect between your data center and VPC, then increase the number or size of your Direct Connect connections as needed. <br>  データセンターとVPC間の1 Gbpsクロスコネクトをプロビジョニングするためのダイレクトコネクト要求をすばやく送信し、必要に応じてダイレクトコネクト接続の数またはサイズを増やします。
Test2-48. <p>Which of the following features ensures even distribution of traffic to Amazon EC2 instances in multiple Availability Zones registered with a load balancer?</p> | <p>ロードバランサに登録された複数のアベイラビリティゾーン内のAmazon EC2インスタンスへのトラフィックの均等な配信を保証する次の機能はどれですか？</p>	sa:	Elastic Load Balancing cross-zone load balancing <br>  Elastic Load Balancingクロスゾーンロードバランシング|<p><br></p><p>Option A is incorrect because there is no request routing option available on ELB.</p><p>Option B is incorrect because Route 53 Weighted Routing will help resolving the DNS requests to different end points. Even though it is a DNS level load balancing, it will not help balancing the load on instances across multiple availability zones while being able to register/un-register instances based on the health check. That functionality is carried out by ELB.</p><p>Option C is CORRECT because you can enable the "Cross Zone Load Balancing" on ELB to even distribution of the traffic across instances in multiple AZs. See the image below:</p><p><br></p><p><span style="font-size: 1rem;">&nbsp;</span><span style="font-size: 1rem;">&nbsp;<img src="https://s3.amazonaws.com/awssap/2_48_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="571" height="553"></span></p><p>Option D is incorrect because Route 53 Latency Based Routing resolves the DNS queries with the resources that provide the best latency. It will not help in this scenario.</p><p><br></p><p>To get more information on ELB cross load balancing, please refer to the link:</p><p></p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html</a><br><p></p>	An Amazon Route 53 weighted routing policy <br>  Amazon Route 53加重ルーティングポリシー	Elastic Load Balancing request routing <br>  エラスティックロードバランシング要求ルーティング	An Amazon Route 53 latency routing policy <br>  Amazon Route 53レイテンシルーティングポリシー
Test2-49. <p>Which of the following items are required to allow an application deployed on an EC2 instance to write data to a DynamoDB table? Assume that no security keys are allowed to be stored on the EC2 instance.</p><p>Choose 3 options from the below:</p> | <p> EC2インスタンスにデプロイされたアプリケーションがDynamoDBテーブルにデータを書き込めるようにするには、次の項目のどれが必要ですか？</p> <p>以下の3つのオプションを選択してください：</p>	ma:	o:Create an IAM Role that allows write access to the DynamoDB table <br>  DynamoDBテーブルへの書き込みアクセスを許可するIAMロールを作成する|<p><br></p><p>To enable an AWS service to access another one, the most important requirement is to create an appropriate IAM Role and attaching that role to the service that needs the access.</p><p><br></p><p>Option A is CORRECT because it create the appropriate IAM Role for accessing the DynamoDB table.</p><p>Option B is CORRECT because you can attach the role to a running EC2 instance that needs the access.</p><p>Option C and D are incorrect because IAM Role is preferred and more secured way than IAM User.</p><p>Option E is CORRECT because it launches the EC2 instance after attaching the required role.</p><p><br></p><p>See the steps below:</p><p><b>1. Create the IAM Role with appropriate permissions</b></p><p><img src="https://s3.amazonaws.com/awssap/2_49_1.png" alt="" width="1027" height="557" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/2_49_2.png" alt="" width="1030" height="557" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/2_49_3.png" alt="" width="1003" height="557" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><b>2. Launch an EC2 instance with this role</b></p><p><img src="https://s3.amazonaws.com/awssap/2_49_4.png" alt="" width="1016" height="510" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><b>3. Attach the role to a running EC2</b></p><p><img src="https://s3.amazonaws.com/awssap/2_49_5.png" alt="" width="716" height="430" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/2_49_6.png" alt="" width="914" height="284" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Reference Link:&nbsp;</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a><br><a href="https://aws.amazon.com/about-aws/whats-new/2017/02/new-attach-an-iam-role-to-your-existing-amazon-ec2-instance/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2017/02/new-attach-an-iam-role-to-your-existing-amazon-ec2-instance/</a><br><p></p>	o:Add an IAM Role to a running EC2 instance <br>  実行中のEC 2インスタンスにIAMロールを追加する	x:Create an IAM User that allows write access to the DynamoDB table <br>  DynamoDBテーブルへの書き込みアクセスを許可するIAMユーザーを作成する	x:Add an IAM User to a running EC2 instance <br>  実行中のEC 2インスタンスにIAMユーザーを追加する	o:Launch an EC2 Instance with the IAM Role included in the launch configuration <br>  起動構成にIAMロールを含むEC 2インスタンスを起動する
Test2-50. <p><span id="docs-internal-guid-846a0cce-30f5-422c-3382-16fd07993eb5"></span></p><p dir="ltr" style="">Your customer wishes to deploy an enterprise application on AWS which will consist of several web servers, several application servers and a small (50GB) Oracle database. The information is stored, both in the database and the file systems of the various servers. The backup system must support database recovery, whole server and whole disk restores, and individual file restores with a recovery time of no more than two hours. They have chosen to use RDS Oracle as the database.</p><p dir="ltr" style="">Which backup architecture will meet these requirements?</p><br><p></p> | <p> <p id = "docs-internal-guid-846a0cce-30f5-422c-3382-16fd07993eb5"> </ span> <p dir = "ltr" style = "">お客様が展開したい複数のWebサーバー、複数のアプリケーションサーバー、および小規模な（50GB）Oracleデータベースで構成されるAWS上のエンタープライズアプリケーションです。情報は、さまざまなサーバーのデータベースとファイルシステムの両方に格納されます。バックアップ・システムは、データベース・リカバリ、サーバー全体およびディスク全体のリストア、および個別のファイル・リストアをサポートしていなければなりません。リカバリ時間は2時間以内です。彼らはデータベースとしてRDS Oracleを使用することを選択しました。</p> <p dir = "ltr" style = "">これらの要件を満たすバックアップアーキテクチャはどれですか？</p> <br> <p>	sa:	Backup RDS using automated daily DB backups. Backup the EC2 instances using AMIs, and supplement with file-level backup to S3 using traditional enterprise backup software to provide file level restore. <br>  自動DBバックアップを使用してRDSをバックアップします。AMIを使用してEC 2インスタンスをバックアップし、従来のエンタープライズバックアップソフトウェアを使用してファイルレベルのリストアを提供するS3へのファイルレベルのバックアップを補完します。|<p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><span id="docs-internal-guid-846a0cce-30f7-73f6-4714-c43ecbc6f7bf"><p dir="ltr" style="">Option A is CORRECT because (a) it uses automated daily backups, from which the recovery can be made quickly, (b) the file-level backup to S3 will ensure that the recovery can be done at the individual file level - which satisfies the requirements</p><p dir="ltr" style="">Option B is incorrect because Multi-AZ deployment is for Disaster Recovery, not for data backup.</p><p dir="ltr" style="">Option C is incorrect because Glacier is an archival solution and most certainly will not meet the criteria of RTO of 2 hours.</p><p dir="ltr" style="">Option D is incorrect because Amazon RDS does not use RMAN for backups. See the link given in the “More information” section.</p></span><br>For more information on this topic, please visit the links below:<br><a href="http://www.boyter.org/wp-content/uploads/2014/12/Backup-And-Recovery-ApproachesUsing-Aws.pdf" target="_blank">http://www.boyter.org/wp-content/uploads/2014/12/Backup-And-Recovery-ApproachesUsing-Aws.pdf</a><br><a href="https://blogs.oracle.com/pshuff/amazon-rds" target="_blank">https://blogs.oracle.com/pshuff/amazon-rds</a><br></span><p><br></p>	Backup RDS using a Multi-AZ Deployment. Backup the EC2 instances using AMIs, and supplement by copying file system data to S3 to provide file level restore. <br>  Multi-AZ展開を使用してRDSをバックアップします。AMIを使用してEC 2インスタンスをバックアップし、ファイルシステムのデータをS3にコピーしてファイルレベルのリストアを提供します。	Backup RDS using automated daily DB backups. Backup the EC2 instances using EBS snapshots and supplement with file-level backups to Amazon Glacier using traditional enterprise backup software to provide file level restore. <br>  自動DBバックアップを使用してRDSをバックアップします。従来のエンタープライズバックアップソフトウェアを使用してEBSスナップショットを使用してEC 2インスタンスをバックアップし、ファイルレベルのバックアップをAmazon Glacierに追加し、ファイルレベルのリストアを提供します。	Backup RDS database to S3 using Oracle RMAN. Backup the EC2 instances using AMIs, and supplement with EBS snapshots for individual volume restore. <br>  Oracle RMANを使用してRDSデータベースをS3にバックアップします。AMIを使用してEC 2インスタンスをバックアップし、個別のボリューム復元用のEBSスナップショットを補完します。
Test2-51. <p>How can you secure data at rest on an EBS volume?</p> | <p> EBSボリュームでデータを安全に保護する方法は？</p>	sa:	Use an encrypted file system on top of the EBS volume. <br>  EBSボリュームの上に暗号化されたファイルシステムを使用します。|<p>In order to secure data at rest on an EBS volume, you either have to encrypt the volume when it is being created or encrypt the data after the volume is created. Hence, option E is CORRECT.</p><p><br></p><p>For more information on EBS encryption, please refer to the link</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a><br><p></p>	Write the data randomly instead of sequentially. <br>  逐次ではなくランダムにデータを書き込む。	Encrypt the volume using the S3 server-side encryption service. <br>  S3サーバー側の暗号化サービスを使用してボリュームを暗号化します。	Create an IAM policy that restricts read and write access to the volume. <br>  ボリュームへの読み取りおよび書き込みアクセスを制限するIAMポリシーを作成します。	Attach the volume to an instance using EC2’s SSL interface. <br>  EC 2のSSLインタフェースを使用して、ボリュームをインスタンスに接続します。
Test2-52. <p>A company needs to monitor the read and write IOPs metrics for their AWS MySQL RDS instance and send real-time alerts to their operations team. Which AWS services can accomplish this?</p><p>Choose 2 options from the below:</p> | <p>企業は、AWS MySQL RDSインスタンスのIOPメトリックの読み取りと書き込みを監視し、運用チームにリアルタイムアラートを送信する必要があります。</p> <p>下記の2つのオプションを選択してください：</p>	ma:	x:Amazon Simple Email Service <br>  Amazon Simple Emailサービス|<p><br></p><p>Option A is incorrect as SNS would be a better choice for sending real time notifications compared to SES.</p><p>Option B is CORRECT because CloudWatch is used for monitoring the metrics pertaining to the AWS resources.</p><p>Option C is incorrect because SQS can neither monitor any metrics, nor send out any real time notifications.</p><p>Option D is incorrect because Route 53 cannot monitor any metrics.</p><p>Option E is CORRECT because SNS is used for sending the real time notifications based on the thresholds set in CloudWatch.</p><p><br></p><p>For more information on cloudwatch metrics, please refer to the link:</p><p></p><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html</a><br><p></p>	o:Amazon CloudWatch <br>  Amazon CloudWatch	x:Amazon Simple Queue Service <br>  Amazon Simple Queueサービス	x:Amazon Route 53 <br>  アマゾンルート53	o:Amazon Simple Notification Service <br>  Amazon Simple Notification Service
Test2-53. <p>A custom script needs to be passed to a new Amazon Linux instances created in your Auto Scaling group. Which feature allows you to accomplish this?</p> | <p>カスタムスクリプトは、自動スケーリンググループで作成された新しいAmazon Linuxインスタンスに渡す必要があります。どの機能でこれを達成できますか？</p>	sa:	User data <br>  ユーザーデータ|<p>When you configure an instance during creation, you can add custom scripts to the User data section.</p><p>So in Step 3 of creating an instance, in the Advanced Details section, we can enter custom scripts in the User Data section. The below script installs Perl during the instance creation of the EC2 instance.</p><p>&nbsp;<span style="font-size: 1rem;">&nbsp;<img src="https://s3.amazonaws.com/awssap/2_53_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="874" height="479"></span></p><p>For more information on user data please refer to the URL:</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a><br><p></p>	EC2Config service <br>  EC2Configサービス	IAM roles <br>  今の役割	AWS Config <br>  AWS Config
Test2-54. <p>You have multiple Amazon EC2 instances running in a cluster across multiple Availability Zones within the same region. What combination of the following should be used to ensure the highest network performance (packets per second), lowest latency, and lowest jitter? <br></p><p>Choose 3 options from the below:<br></p> | <p>同じリージョン内の複数の可用性ゾーンにまたがるクラスタ内で複数のAmazon EC2インスタンスを実行しています。最高のネットワークパフォーマンス（1秒あたりのパケット数）、最低の待ち時間、および最低ジッタを保証するために、次のどのような組み合わせを使用する必要がありますか？<br> </p> <p>下記の3つのオプションを選択してください：<br> </p>	ma:	x:Cluster placement group <br>  クラスタ配置グループ|<p><br></p><p><span style="font-size: 1rem;"></span></p><div>Option A is Incorrect.&nbsp;A cluster placement group is a logical grouping of instances within a single Availability Zone and it cannot span multipleAZ's.<br></div><div>Option B is CORRECT because Enhanced networking uses single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities on supported instance types.</div><div>Option C is incorrect because it is recommended to use HVM AMIs for better performance compared to PV AMIs.</div><div>option D is CORRECT because&nbsp;HVM AMIs take advantage of Enhanced Networking; whereas PV AMIs do not.</div><div>Option E is incorrect because using Amazon Linux does not necessarily improve any performance.</div><div>Option F is CORRECT because VPC endpoints&nbsp;allow communication between instances in the VPC and AWS services without imposing availability risks or bandwidth constraints on the network traffic.</div><span style="font-size: 1rem;"><br>For more information on Enhanced Networking, please visit the URL&nbsp;<br><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a><br></span><p></p><p>Linux Amazon Machine Images use one of two types of virtualization: paravirtual (PV) or hardware virtual machine (HVM). The main difference between PV and HVM AMIs is the way in which they boot and whether they can take advantage of special hardware extensions (CPU, network, and storage) for better performance. For the best performance, we recommend that you use current generation instance types and HVM AMIs when you launch your instances.&nbsp;</p><p><span style="font-size: 1rem;">For more information on Enhanced Networking, please visit the URL</span></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html</a><br><p></p>	o:Enhanced networking <br>  ネットワークの強化	x:Amazon PV AMI <br>  Amazon PV AMI	o:Amazon HVM AMI <br>  Amazon HVM AMI	x:Amazon Linux <br>  Amazon Linux	o: Amazon VPC Endpoints <br>  Amazon VPCエンドポイント
Test2-55. <p>You have a video transcoding application running on Amazon EC2. Each instance polls a queue to find out which video should be transcoded and then runs a transcoding process. If this process is interrupted, the videos will be transcoded by another instance based on the queuing system. You have a large backlog of videos which need to be transcoded and would like to reduce this backlog by adding more instances. You will need these instances only until the backlog is reduced. Which type of Amazon EC2 instances should you use to reduce the backlog in the most cost-efficient way?</p> | <p> Amazon EC2上で動作するビデオトランスコーディングアプリケーションがあります。各インスタンスはキューをポーリングして、トランスコードされるビデオを見つけ出し、トランスコード処理を実行します。このプロセスが中断された場合、ビデオはキューイングシステムに基づいて別のインスタンスによってトランスコードされます。トランスコードする必要のある大きなバックログがあり、インスタンスをさらに追加することでこのバックログを減らすことができます。バックログが減少するまでこれらのインスタンスが必要になります。最も費用対効果の高い方法でバックログを削減するために使用するAmazon EC2インスタンスのタイプはどれですか？</p>	sa:	Spot instances <br>  スポットインスタンス|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Since this is like a batch processing job, the best type of instance to use is a Spot instance. Since these jobs don’t last for the entire duration of the year, they can bid upon and allocated and deallocated as requested.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A and C are incorrect because the application need the instances only until the backlog is reduced. With reserved/dedicated instances, there is a possibility that the instances might get idle after the backlog reduction. So, this is a costly solution.</span></p><p><span style="font-size: 1rem;">Option B is CORRECT because (i) they are less expensive than reserved instances, (ii) interruption in the transcoding process is affordable since the videos will be transcoded by another instance based on the queuing system.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because (i) on-demand instances are most expensive, (ii) you can afford interruption in the transcoding process, and (iii) on demand instances would have been suited if there was no alternate way of transcoding the videos and interruption was not affordable.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">For more information on Spot Instances, please visit the URL –</span></p><a href="https://aws.amazon.com/ec2/spot/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/ec2/spot/</a><br> <p></p>	Reserved instances <br>  予約済みのインスタンス	Dedicated instances <br>  専用インスタンス	On-demand instances <br>  オンデマンドインスタンス
Test2-56. <p>A company has a requirement to host an application behind an AWS ELB. The application will be supporting multiple device platforms. Each device platform will need separate SSL certificates assigned to it. <br></p><p>Which of the below options is the best setup in AWS to fulfill the above requirement?</p> | <p>企業は、AWS ELBの後ろにアプリケーションをホストする必要があります。アプリケーションは複数のデバイスプラットフォームをサポートします。各デバイスプラットフォームには、別々のSSL証明書が割り当てられている必要があります。<br> </p> <p>上記の要件を満たすためのAWSの最適な設定はどれですか？</p>	sa:	Set up an Application Load Balancer with Server Name Indicator support, for handling separate SSL certificate for each device platform. <br>  各デバイスプラットフォームごとに個別のSSL証明書を処理するために、サーバー名インジケータのサポートを備えたアプリケーションロードバランサを設定します。|<p></p>Originally, Application Load Balancers used to support only one certificate for a standard HTTPS listener (port 443) and you had to use Wildcard or Multi-Domain (SAN) certificates to host multiple secure applications behind the same load balancer. The potential security risks with Wildcard certificates and the operational overhead of managing Multi-Domain certificates presented challenges.<b>&nbsp;With SNI support you can associate multiple certificates with a listener and each secure application behind a load balancer can use its own certificate.&nbsp;</b>&nbsp;You can use host conditions to define rules that forward requests to different target groups based on the host name in the host header (also known as&nbsp;<i>host-based routing</i>). This enables you to support multiple domains using a single load balancer.<p dir="ltr" style=""></p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;">Option A is incorrect because it is not cost effective to handle such hybrid architecture.</span></p><p dir="ltr" style="">Option C is incorrect because even though ELB supports multiple SSL certificates, distributing the load based on the platform type will not be feasible. You will still require multiple ELBs.</p><p dir="ltr" style="">Option D is incorrect as it is not required since there is support for multiple TLS/SSL certificates on Application Load Balancers.</p><p dir="ltr" style="">For more information on ELB, please visit the below URL</p><p dir="ltr" style=""><a href="https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/" target="_blank">https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/</a></p><br><p></p><ul></ul><p></p>	Setup a hybrid architecture to handle multiple SSL certificates by using separate EC2 Instance groups running web applications for different platform types running in a VPC. <br>  VPCで実行されているさまざまなプラットフォームタイプのWebアプリケーションを実行するEC2インスタンスグループを使用して、複数のSSL証明書を処理するハイブリッドアーキテクチャをセットアップします。	You just need to set single ELB. Since it supports multiple SSL certificates, it should be sufficient enough for the different device platforms <br>  あなたは単ELBを設定するだけです。複数のSSL証明書をサポートしているため、異なるデバイスプラットフォームで十分なはずです	Create multiple ELB’s for each type of certificate for each device platform. <br>  各デバイスプラットフォームの証明書の種類ごとに複数のELBを作成します。
Test2-57. <p><span id="docs-internal-guid-846a0cce-3194-8e27-9353-b1cdc24fb5d2">You deployed your company website using Elastic Beanstalk and you enabled log file rotation to S3. An Elastic MapReduce job is periodically analyzing the logs on S3 to build a usage dashboard that you share with your CIO. You recently improved overall performance of the website using CloudFront for dynamic content delivery and your website as the origin. After this architectural change, the usage dashboard shows that the traffic on your website dropped by an order of magnitude. How do you fix your usage dashboard?</span></p> | <p> <span id = "docs-internal-guid-846a0cce-3194-8e27-9353-b1cdc24fb5d2"> Elastic Beanstalkを使用して会社のWebサイトを展開し、ログファイルのローテーションをS3に設定しました。Elastic MapReduceジョブは、定期的にS3のログを分析して、CIOと共有する使用ダッシュボードを構築します。最近、CloudFrontを使用して動的コンテンツ配信やWebサイトを起点としてWebサイト全体のパフォーマンスを向上させました。このアーキテクチャの変更後、使用ダッシュボードでは、Webサイトのトラフィックが一桁減少したことが示されます。使用ダッシュボードをどのように修正しますか？</ span> </p>	sa:	Enable CloudFront to deliver access logs to S3 and use them as input of the Elastic MapReduce job. <br>  CloudFrontがS3にアクセスログを配信できるようにし、それらをElastic MapReduceジョブの入力として使用します。|<br><p dir="ltr" style="">Option A is CORRECT because the website is now only accessible via CloudFront. So, for the dashboard to have the up-to-date information via EMR, the logs from the CloudFront must be stored on S3 (to be analyzed by the EMR). Once these logs are delivered to S3, the dashboard should show the correct traffic information.</p><p dir="ltr" style="">Option B is incorrect because CloudTrail log will not show the required information, it will only show the insights of the AWS services and APIs accessed by the application.</p><p dir="ltr" style="">Option C is incorrect because the dashboard must be showing the information about the traffic pertaining to the website. CloudWatch will show the information based on the metrics related to AWS resources (not the website).</p><p dir="ltr" style="">Option D is incorrect because configuration of the Elastic Beanstalk environment is independent of the CloudFormation setting. In order to have the information related to the dynamic content, the logs created by the CloudFormation must be delivered to S3. “Rebuild Environment” of Elastic Beanstalk will not be of any use.</p><p dir="ltr" style="">Option E is incorrect because “Restart App Server(s)” causes the environment to restart the application container server running on each Amazon EC2 instance. It is totally unrelated to the information that is shown by the dashboard.</p></span><br><p></p>	Turn on CloudTrail and use trail log tiles on S3 as input of the Elastic MapReduce job. <br>  CloudTrailを有効にして、Elastic MapReduceジョブの入力としてS3のトレイルログタイルを使用します。	Change your log collection process to use CloudWatch ELB metrics as input of the Elastic MapReduce job. <br>  CloudWatch ELBメトリックをElastic MapReduceジョブの入力として使用するように、ログ収集プロセスを変更します。	Use Elastic Beanstalk "Rebuild Environment" option to update log delivery to the Elastic MapReduce job. <br>  Elastic Beanstalkを使用して、環境再構築オプションを使用して、Elastic MapReduceジョブへのログ配信を更新します。	Use Elastic Beanstalk 'Restart App Server(s)" option to update log delivery to the Elastic MapReduce job. <br>  Elastic Beanstalkの「アプリケーションサーバーの再起動」オプションを使用して、Elastic MapReduceジョブへのログ配信を更新します。
Test2-58. <p>You decide to configure a bucket for static website hosting. As per the AWS documentation, you create a bucket named 'mybucket.com' and then you enable website hosting with an index document of 'index.html' and you leave the error document as blank. You then upload a file named 'index.html' to the bucket. After clicking on the endpoint of mybucket.com.s3-website-us-east-1.amazonaws.com you receive 403 Forbidden error. You then change the CORS configuration on the bucket so that everyone has access, however, you still receive the 403 Forbidden error. What additional step do you need to do so that the endpoint is accessible to everyone? <br></p><p>Choose the correct option from the below:</p> | <p>静的Webサイトホスティング用のバケットを設定することにしました。AWSのドキュメントに従って、 'mybucket.com'という名前のバケットを作成してから、index.htmlというインデックスドキュメントを使用してウェブサイトホスティングを有効にし、エラードキュメントを空白のままにします。次に、「index.html」という名前のファイルをバケットにアップロードします。mybucket.com.s3-website-us-east-1.amazonaws.comのエンドポイントをクリックすると、403 Forbiddenエラーが表示されます。バケット上のCORS設定を変更して、誰もがアクセスできるようにします。ただし、403 Forbiddenエラーが表示されます。誰にでもエンドポイントにアクセスできるようにするためには、追加のステップは何ですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	Change the permissions on the index.html file also, so that everyone has access <br>  誰もがアクセスできるように、index.htmlファイルのアクセス権を変更する|<p>You are receiving the 403 Forbidden Error because you do not have the permissions to view the index.html file.</p><p>Option A is incorrect because this is an S3 hosted website, Route 53 does not come into picture.</p><p>Option B is incorrect because it is a static website hosted on S3. This issue is not related to DNS resolution.</p><p>Option C is incorrect because even if you add the error document, you will get the error, because you need to set the proper permissions.</p><p>Option D is CORRECT because it sets the appropriate permissions so that the user has access to the index.html.</p><p><span style="font-size: 1rem;">For more information on web site hosting in S3, please visit the below link:</span></p><p></p><ul><li><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html" target="_blank">http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></span></li></ul><p></p>	Wait for the DNS change to propagate <br>  DNSの変更が反映されるのを待ちます。	You need to add a name for the error document, because it is a required field <br>  必須フィールドであるため、エラー文書の名前を追加する必要があります	Register mybucket.com on Route53 <br>  ルート53にmybucket.comを登録
Test2-59. <p>Server-side encryption is about data encryption at rest. That is, Amazon S3 encrypts your data at the object level as it writes it to disk in its data centers and decrypts it for you when you go to access it. There are a few different options depending on how you choose to manage the encryption keys. One of the options is called 'Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)'. Which of the following best describes how this encryption method works? <br></p><p>Choose the correct option from the below:</p> | <p>サーバー側の暗号化は、安心してデータを暗号化することです。つまり、Amazon S3は、オブジェクトレベルでデータを暗号化し、データセンターのディスクに書き込んだり、アクセスしたときに復号化したりします。暗号化キーの管理方法に応じていくつかのオプションがあります。オプションの1つは、「Amazon S3管理キーを使用したサーバーサイド暗号化（SSE-S3）」と呼ばれています。次の中で、この暗号化方式の仕組みを説明しているのはどれですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	Each object is encrypted with a unique key employing strong encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. <br>  各オブジェクトは、強力な暗号化を使用する一意のキーで暗号化されます。 追加の保護手段として、それは定期的に回転するマスターキーでキー自体を暗号化します。|<p><br></p><p>Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p><br></p><p>Option A is incorrect because there are no separate permissions to the key that protects the data key.</p><p>Option B is CORRECT because as mentioned above, each object is encrypted with a strong unique key and that key itself is encypted by a master key.</p><p>Option C is incorrect because the keys are managed by the AWS.</p><p>Option D is incorrect because there is no randomly generated key and client does not do the encryption.</p><p><br></p><p>For more information on S3 encryption, please visit the link</p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a><br></p><p></p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a><br><p></p>	There are separate permissions for the use of an envelope key (that is, a key that protects your data's encryption key) that provides added protection against unauthorized access of your objects in S3 and also provides you with an audit trail of when your key was used and by whom. <br>  エンベロープキー（データの暗号化キーを保護するキー）を使用するための別個のアクセス許可があります。これにより、S3でのオブジェクトへの不正アクセスからの保護が強化され、誰によってキーが使用されたかの監査証跡が提供されます。	You manage the encryption keys and Amazon S3 manages the encryption, as it writes to disk, and decryption, when you access your objects. <br>  暗号化キーを管理すると、オブジェクトにアクセスすると、Amazon S3によって暗号化がディスクに書き込まれ、暗号化が管理され、復号化されます。	A randomly generated data encryption key is returned from Amazon S3, which is used by the client to encrypt the object data. <br>  ランダムに生成されたデータ暗号化キーがAmazon S3から返されます。Amazon S3は、クライアントがオブジェクトデータを暗号化するために使用します。
Test2-60. <p>Explain what the following resource in a CloudFormation template does.</p><p> Choose the best possible answer.</p><p>&nbsp;</p><p>"SNSTopic" : {</p><p>&nbsp;&nbsp;&nbsp; "Type" : "AWS::SNS::Topic",</p><p>&nbsp;&nbsp;&nbsp; "Properties" : {</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Subscription" : [{</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Protocol" : "sqs",</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Endpoint" : { "Fn::GetAtt" : [ "SQSQueue", "Arn" ] }</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }]</p><p>}</p> | </p> <p>＆nbsp; <p> "SNSTopic"：</p> <p> CloudFormationテンプレート内の次のリソースが何をしているかを説明します。 p> <p>＆nbsp;＆nbsp;＆nbsp; "タイプ"： "AWS :: SNS ::トピック"、</p> <p>＆nbsp;＆nbsp;＆nbsp; "プロパティ"：{</p> <p>＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp; "サブスクリプション"：[{</p>]＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp; "Protocol"： "sqs"、<p>＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp; nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp; "Endpoint"：{"Fn :: GetAtt"：["SQSQueue"、 "Arn"]} </p> <p>＆nbsp;＆nbsp;＆nbsp;＆nbsp;＆nbsp; }] </p> <p>} </p>	sa:	Creates an SNS topic and adds a subscription ARN endpoint for the SQS resource created under the logical name SQSQueue <br>  SNSトピックを作成し、論理名SQSQueueで作成されたSQSリソースのサブスクリプションARNエンドポイントを追加します。|<p><br></p><p>Option A is incorrect because it is not adding any parameter in the template.</p><p>Option B is incorrect because it is not adding a subscription endpoint for the SQS resource named Arn. It is actually creating an SNS topic and adding a subscription ARN endpoint for the SQS resource name SQSQueue.</p><p>Option C is incorrect because it does not create any SQS queue.</p><p>Option D is CORRECT because it creates an SNS topic and adds a subscription ARN endpoint for the SQS resource.</p><p><br></p><p>For more information on Fn:: GetAtt function please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a><br><p></p>	Creates an SNS topic and adds a subscription ARN endpoint for the SQS resource named Arn <br>  SNSトピックを作成し、Arnという名前のSQSリソース用のサブスクリプションARNエンドポイントを追加します。	Creates an SNS topic and then invokes the call to create an SQS queue with a logical resource name of SQSQueue <br>  SNSトピックを作成し、呼び出しを呼び出して、論理リソース名がSQSQueueのSQSキューを作成します。	Creates an SNS topic which allows SQS subscription endpoints to be added as a parameter on the template <br>  SQSサブスクリプションエンドポイントをテンプレートのパラメータとして追加できるようにするSNSトピックを作成します。
Test2-61. <p></p><span id="docs-internal-guid-846a0cce-3126-893a-96c7-0fda286834cd">A customer implemented AWS Storage Gateway with a gateway-cached volume at their main office. An event takes the link between the main and branch office offline. Which methods will enable the branch office to access their data? Choose 3 answers:</span><p></p> | <p> </p> <span id = "docs-internal-guid-846a0cce-3126-893a-96c7-0fda286834cd">メインオフィスにゲートウェイキャッシュボリュームを持つAWS Storage Gatewayを実装しました。イベントは、メインオフィスとブランチオフィス間のリンクをオフラインにします。ブランチオフィスがデータにアクセスするための方法はどれですか？3つの回答を選択してください：</ span> <p> </p>	ma:	x:Use a HTTPS GET to the Amazon S3 bucket where the files are located. <br>  ファイルが配置されているAmazon S3バケットにHTTPS GETを使用します。|<br><p dir="ltr" style="">Option A is incorrect because, all gateway-cached volume data and snapshot data is stored in Amazon S3 encrypted at rest using server-side encryption (SSE) and it cannot be visible or accessed with S3 API or any other tools. (Ref: https://forums.aws.amazon.com/thread.jspa?threadID=109748)</p><p dir="ltr" style="">Option B is incorrect you cannot apply Lifecycle Policies as the AWS Storage Gateway does not give you that option.</p><p dir="ltr" style="">Option C is incorrect because the cached volumes are never stored to Glacier.</p><p dir="ltr" style="">Option D is CORRECT because, you can take point-in-time snapshots of gateway volumes that are made available in the form of Amazon EBS snapshots. You can launch an EC2 instance from that.</p><p dir="ltr" style="">Option E is CORRECT because, you can take point-in-time snapshots of gateway volumes that are made available in the form of Amazon EBS snapshots. A new EBS volume can be created from the snapshot which can be mounted to an existing EC2 instance.</p><p dir="ltr" style="">Option F is CORRECT because, you can take point-in-time snapshots of gateway volumes that are made available in the form of Amazon EBS snapshots. A Volume Gateway allows you to mount iSCSI devices that you can mount to on-premise machines. You can then restore the data from the point-in-time snapshot.</p><br><p dir="ltr" style="">For more information on this topic, please refer to the AWS FAQs:</p></span><a href="https://aws.amazon.com/storagegateway/faqs/" target="_blank">https://aws.amazon.com/storagegateway/faqs/</a><br><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html" target="_blank" style="font-size: 1rem;"></a><br><p></p>	x:Restore by implementing a lifecycle policy on the Amazon S3 bucket. <br>  Amazon S3バケットにライフサイクルポリシーを実装して復元します。	x:Make an Amazon Glacier Restore API call to load the files into another Amazon S3 bucket within four to six hours. <br>  Amazon Glacier Restore APIを呼び出して、ファイルを別のAmazon S3バケットに4〜6時間でロードします。	o:Launch a new AWS Storage Gateway instance AMI in Amazon EC2, and restore from a gateway snapshot. <br>  Amazon EC 2で新しいAWS Storage GatewayインスタンスAMIを起動し、ゲートウェイスナップショットから復元します。	o:Create an Amazon EBS volume from a gateway snapshot, and mount it to an Amazon EC2 instance. <br>  ゲートウェイスナップショットからAmazon EBSボリュームを作成し、Amazon EC 2インスタンスにマウントします。	o: Launch an AWS Storage Gateway virtual iSCSI device at the branch office, and restore from a gateway snapshot. <br>  ブランチオフィスでAWS Storage Gateway仮想iSCSIデバイスを起動し、ゲートウェイスナップショットから復元します。
Test2-62. <p>You created three S3 buckets – “mydomain.com”, “downloads.mydomain.com”, and “www.mydomain.com”. You uploaded your files, enabled static website hosting, specified both of the default documents under the “enable static website hosting” header, and set the “Make Public” permission for the objects in each of the three buckets. All that’s left for you to do is to create the Route 53 Aliases for the three buckets. You are going to have your end users test your websites by browsing to http://mydomain.com/error.html, http://downloads.mydomain.com/index.html, and http://www.mydomain.com. What problems will your testers encounter? <br></p><p>Choose an option from the below:</p> | <p> 3つのS3バケット（ "mydomain.com"、 "downloads.mydomain.com"、 "www.mydomain.com"）を作成しました。ファイルをアップロードし、静的なWebサイトのホスティングを有効にし、両方の既定のドキュメントを「静的なWebサイトのホスティングを有効にする」ヘッダーの下に指定し、3つのバケットのそれぞれのオブジェクトの「公開」アクセス許可を設定しました。3つのバケットのルート53エイリアスを作成するだけです。http://mydomain.com/error.html、http://downloads.mydomain.com/index.html、およびhttp://www.mydomain.comにアクセスして、エンドユーザーがウェブサイトをテストするようにします。 。あなたのテスターはどんな問題に遭遇しますか？<br> </p> <p>下記からオプションを選択してください：</p>	sa:	There will be no problems, all three sites should work <br>  問題はなく、3つのサイトはすべて動作するはずです|<p></p><div><br></div><div>Previously only allowed domain prefix when we are creating AWS Route53 aliases for AWS S3 static websites was the “www”.</div><div>However, this is no longer the case. You can now use other sub-domains.</div><p></p><p><br></p><p>For more information on S3 web site hosting please visit the below link:</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a><br><p></p>	http://www.mydomain.com will not work because the URL does not include a file name at the end of it <br>  URLの末尾にファイル名が含まれていないため、http://www.mydomain.comは機能しません	http://mydomain.com/error.html will not work because you did not set a value for the error.html file <br>  error.htmlファイルの値を設定していないため、http://mydomain.com/error.htmlは機能しません	http://downloads.mydomain.com/index.html will not work because the “downloads” prefix is not a supported prefix for S3 websites using Route 53 aliases <br>  "downloads"プレフィックスはサポートされていないため、http://downloads.mydomain.com/index.htmlは機能しません。ルート53エイリアスを使用するS3ウェブサイトのプレフィックスのサポート
Test2-63. <p>Your supervisor is upset about the fact that SNS topics that he subscribed to are now cluttering up his email inbox. How can he stop receiving the email from SNS without disrupting other users’ ability to receive the email from SNS?</p><p>Choose 2 options from the below:</p> | <p>スーパーバイザは、自分が購読しているSNSのトピックが自分のメール受信ボックスを乱雑にしているという事実に悩まされています。</p> <p> SNSからの電子メールの受信を中断することなく、他のユーザーのSNSからのメール受信を中断することはできませんか？</p> <p>	ma:	o:You can delete the subscription from the SNS topic responsible for the emails <br>  電子メールを担当するSNSのトピックからサブスクリプションを削除できます|<p><br></p><p><span style="font-size: 1rem;">Every request has a unsubscribe URL which can be used. Also from the aws console , one can just delete the subscription</span></p><p><span style="font-size: 1rem;">Option A is CORRECT because deleting the subscription for the user from the SNS topic will ensure that he will not receive any notifications (basically just unsubscribe him).</span></p><p><span style="font-size: 1rem;">Option B is incorrect because you cannot delete the endpoint from the SNS subscription.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because if you delete the topic then none of the subscribers will get any notifications.&nbsp;</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because the notifications has an option to unsubscribe which the user can avail to stop receiving the notifications.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on SNS subscription please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/sns/latest/api/API_Subscribe.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/sns/latest/api/API_Subscribe.html</a><br><p></p>	x:You can delete the endpoint from the SNS subscription responsible for the emails <br>  電子メールを担当するSNSサブスクリプションからエンドポイントを削除することができます	x:You can delete the SNS topic responsible for the emails <br>  電子メールを担当するSNSトピックを削除することができます	o:He can use the unsubscribe information provided in the emails <br>  彼は電子メールで提供された購読解除情報を使用することができます
Test2-64. <p>You have created an Elastic Load Balancer with Duration-Based sticky sessions enabled in front of your six EC2 web application instances in US-West-2. For High Availability, there are three web application instances in Availability Zone 1 and three web application instances in Availability Zone 2. To load test, you set up a software-based load tester in Availability Zone 2 to send traffic to the Elastic Load Balancer, as well as letting several hundred users browse to the ELB’s hostname.</p><p>After a while, you notice that the users’ sessions are spread evenly across the EC2 instances in both AZ’s, but the software-based load tester’s traffic is hitting only the instances in Availability Zone 2. What steps can you take to resolve this problem?</p><p>Choose 2 correct options from the below:</p> | <p> US-West-2の6つのEC2 Webアプリケーションインスタンスの前に、期間ベースのスティッキセッションを有効にしたElastic Load Balancerを作成しました。 ハイアベイラビリティの場合、可用性ゾーン1に3つのWebアプリケーションインスタンスがあり、可用性ゾーン2に3つのWebアプリケーションインスタンスがあります。テストをロードするには、可用性ゾーン2にソフトウェアベースのロードテスターを設定して、Elastic Load Balancer、 </ p> <p>しばらくすると、両方のAZのEC2インスタンスにユーザーのセッションが均等に分散されていることがわかりますが、ソフトウェアベースの負荷テスト担当者のトラフィック 可用性ゾーン2のインスタンスだけがヒットしています。この問題を解決するにはどのような手順が必要ですか？</p> <p>下記の2つの正しいオプションを選択してください：</p> <p>この問題を解決するには、	ma:	x:Create a software-based load tester in US-East-1 and test from there. <br>  US-East-1にソフトウェアベースの負荷テスターを作成し、そこからテストします。|<p><br></p><p>When you create an elastic load balancer, a default level of capacity is allocated and configured. As Elastic Load Balancing sees changes in the traffic profile, it will scale up or down. The time required for Elastic Load Balancing to scale can range from 1 to 7 minutes, depending on the changes in the traffic profile. When Elastic Load Balancing scales, it updates the DNS record with the new list of IP addresses. To ensure that clients are taking advantage of the increased capacity, Elastic Load Balancing uses a TTL setting on the DNS record of 60 seconds. It is critical that you factor this changing DNS record into your tests. If you do not ensure that DNS is re-resolved or use multiple test clients to simulate increased load, the test may continue to hit a single IP address when Elastic Load Balancing has actually allocated many more IP addresses. Because your end users will not all be resolving to that single IP address, your test will not be a realistic sampling of real-world behavior.<br></p><p><br></p><p>Option A is incorrect because creating load tester in US-East-1 will face the same problem of traffic hitting only the instances in that AZ.</p><p>Option B is CORRECT because if you do not ensure that DNS is re-resolved the test may continue to hit the single IP address.</p><p>Option C is CORRECT because if the requests come from globally distributed users, the DNS will not be resolved to a single IP address and the traffic would be distributed evenly across multiple instances.</p><p>Option D is incorrect because the traffic will be routed to the same back-end instances as the users continue to access your application. The load will not be evenly distributed across the AZs.</p><p><br></p><p>Please refer to the below article for more information:</p><p></p><span style="font-size: 1rem;"><a href="http://aws.amazon.com/articles/1636185810492479" target="_blank">http://aws.amazon.com/articles/1636185810492479</a><br></span><br><p></p>	o:Force the software-based load tester to re-resolve DNS before every request. <br>  ソフトウェアベースの負荷テスト担当者がすべての要求の前にDNSを再解決するように強制します。	o:Use a third party load-testing service to send requests from globally distributed clients. <br>  サードパーティの負荷テストサービスを使用して、グローバルに分散しているクライアントから要求を送信します。	x:Switch to application-controlled sticky sessions. <br>  アプリケーション制御のスティッキセッションに切り替えます。
Test2-65. <p>You run an ad-supported photo sharing website using S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business. What is an effective method to mitigate this? <br></p><p>Choose the correct answer from the below options:<br></p> | <p> S3を使用して広告付きの写真共有サイトを実行して、サイトの訪問者に写真を配信します。ある時点で、他のサイトがあなたのサイトの写真にリンクしていることが分かり、あなたのビジネスに迷惑をかけることになります。これを軽減する効果的な方法は何ですか？<br> </p> <p>以下のオプションから正解を選択します：<br> </p>	sa:	Remove public read access and use signed URLs with expiry dates. <br>  公開読取りアクセスを削除し、有効期限付きの署名入りURLを使用します。|<p><br></p><p>You can distribute private content using a signed URL that is valid for only a short time—possibly for as little as a few minutes. Signed URLs that are valid for such a short period are good for distributing content on-the-fly to a user for a limited purpose, such as distributing movie rentals or music downloads to customers on demand.&nbsp;</p><p><br></p><p>Option A is incorrect because using CloudFront is an expensive option compared to using signed URLs.</p><p>Option B is incorrect because the website is hosted on S3.</p><p>Option C is CORRECT because, as mentioned above, it will ensure that only the trusted/authenticated users get access to the content.</p><p>Option D is incorrect because the website is hosted on S3 which does not have any security group setting.</p><p><br></p><p>For more information on Signed URL’s please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a><br><p></p>	Store photos on an EBS volume of the web server. <br>  WebサーバーのEBSボリュームに写真を保存します。	Use CloudFront distributions for static content. <br>  静的コンテンツにはCloudFrontディストリビューションを使用します。	Block the IPs of the offending websites in Security Groups. <br>  セキュリティグループ内の違反しているWebサイトのIPをブロックする。
Test2-66. <p>You are using DynamoDB to store data in your application. One of the tables named "Users", you have defined "UserID" as it primary key. However, you envision that, in some cases, you might need to query the table by "UserName" which cannot be set as primary key. What changes would you do to this table to be able to query using UserName?<br></p><p>Choose correct option from the below:</p> | <p> DynamoDBを使用してアプリケーションにデータを格納しています。"Users"という名前のテーブルの1つでは、 "UserID"をプライマリキーとして定義しています。ただし、場合によっては、主キーとして設定できない "UserName"で表を照会する必要がある場合もあります。UserNameを使用してクエリを実行できるようにするには、この表をどのように変更しますか？</p> <p>以下から正しいオプションを選択してください：</p>	sa:	Create a secondary index. <br>  セカンダリインデックスを作成します。|<p><br></p><p>Amazon DynamoDB provides fast access to items in a table by specifying primary key values. However, many applications might benefit from having one or more secondary (or alternate) keys available, to allow efficient access to data with attributes other than the primary key. To address this, you can create one or more secondary indexes on a table, and issue Query or Scan requests against these indexes.</p><p><br></p><p>Option A is incorrect because creating another table is costly and unnecessary.</p><p>Option B is incorrect because UserName cannot be primary key.</p><p>Option C is CORRECT because, as mentioned above, creating a secondary index on UserName would allow the user to efficiently access the table via querying on this attribute rather than UserID which is the primary key.</p><p>Option D is incorrect because DynamoDB tables are partitioned based on the primary key and you cannot make UserName as the primary key.</p><p><br></p><p></p><a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html</a><br><p></p>	Create a hash and range primary key. <br>  ハッシュと範囲主キーを作成します。	Create a second table that contains all the information, but make UserName the primary key. <br>  すべての情報を含む2番目のテーブルを作成しますが、UserNameは主キーにします。	Partition the table using UserName rather than UserID. <br>  UserIDではなくUserNameを使用してテーブルを分割します。
Test2-67. <p>A web application is currently hosted on an on-premise location. There is ad-campaign underway and there is a probability that the influx of traffic on the website is going to increase. The company does not have the time to migrate this application to AWS.</p><p>Which scenario below will provide full site functionality, while helping to improve the ability of your application to take the influx of traffic in the short timeframe required?</p> | <p>現在、Webアプリケーションはオンプレミスの場所でホストされています。広告キャンペーンが進行中で、ウェブサイトへのトラフィックの流入が増加する可能性があります。このアプリケーションをAWSに移行する時間はありません。</p> <p>次のシナリオでは、サイト全体の機能を提供しながら、必要な短時間でトラフィックの流入を改善する機能を提供します？</p>	sa:	Offload traffic from on-premises environment by setting up a CloudFront distribution and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behaviour and select a TTL that objects should exist in cache. <br>  CloudFrontディストリビューションを設定して、オンプレミス環境からトラフィックをオフロードし、カスタム起点からオブジェクトをキャッシュするようにCloudFrontを設定します。オブジェクトキャッシュの動作をカスタマイズし、オブジェクトがキャッシュに存在するはずのTTLを選択することを選択します。|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">In this scenario, the major points of consideration are: (1) your application may get unpredictable bursts of traffic, (b) you need to improve the current infrastructure in shortest period possible, and (3) your web servers are on premise.</span></p><p><span id="docs-internal-guid-6c5f78f5-07cd-f132-141e-02df41f26c20"></span></p><p dir="ltr" style="">Since the time period in hand is short, instead of migrating the app to AWS, you need to consider different ways where the performance would improve without doing much modification to the existing infrastructure.</p><br><p dir="ltr" style="">Option A is CORRECT because (a) CloudFront is AWS’s highly scalable, highly available content delivery service, where it can perform excellently even in case of sudden unpredictable burst of traffic, (b) the only change you need to make is make the on-premises load balancer as the custom origin of the CloudFront distribution.&nbsp;</p><p dir="ltr" style="">Option B is incorrect because you are supposed to improve the current situation in shortest time possible. Migrating to AWS would be more time consuming than simply setting up the CloudFront distribution.</p><p dir="ltr" style="">Option C is incorrect because you cannot host dynamic web sites on S3 bucket. Also, this option provides insufficient infrastructure set up options.</p><p dir="ltr" style="">Option D is incorrect because ELB cannot do balancing between AWS EC2 instances and on-premise instances.</p><p dir="ltr" style=""><span style="font-size: 1rem;"><br></span></p><p dir="ltr" style=""><span style="font-size: 1rem;"><b>More information on CloudFront:</b></span></p><p dir="ltr" style="">You can have CloudFront sit in front of your on-premise web environment, via a custom origin. This would protect against unexpected bursts in traffic by letting CloudFront handle the traffic from the cache, thus removing some of the load from the on-premise web servers.<br><br></p><p dir="ltr" style="">Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Like other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long-term. commitments or minimum fees. With CloudFront, your files are delivered to end-users using a global network of edge locations.<br>If you have dynamic content, then it is best to have the TTL set to 0.</p><p dir="ltr" style=""><br></p>For more information on CloudFront, please visit the below URL:<br><a href="https://aws.amazon.com/cloudfront/" style="">https://aws.amazon.com/cloudfront/</a><br><p></p><ul></ul><p></p>	Migrate to AWS because this is the only option. Use VM import ‘Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group which uses the imported AMI to scale the web tier based on incoming traffic. <br>  VMインポート 'エクスポートを使用すると、オンプレミスWebサーバーをAMIにすばやく変換し、インポートされたAMIを使用して着信トラフィックに基づいてWeb層を拡張する自動スケーリンググループを作成します。	Create an S3 bucket and configure it tor website hosting. Migrate your DNS to Route53 using zone? import and leverage Route53 DNS failover to failover to the S3 hosted website. <br>  S3バケットを作成し、ウェブサイトホスティングを設定します。ゾーンを使用してDNSをRoute 53に移行しますか？ルート53のDNSフェールオーバーをインポートして活用し、S3ホストされたWebサイトへのフェールオーバーを実現します。	Create an AMI which can be used of launch web servers in EC2. Create an Auto Scaling group which uses the AMI’s to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS. <br>  Elastic Load Balancingを活用して、オンプレミスWebサーバーとAWSでホストされているWebサーバーとの間のトラフィックを分散します。
Test2-68. <p>Which section in your CloudFormation template would you modify to fire up different instance sizes based off of environment type (Dev/Staging/Production)?</p><p> Choose the correct answer from below options:<br></p> | <p>環境タイプ（開発/ステージング/プロダクション）に基づいて異なるインスタンスサイズを起動するCloudFormationテンプレートのセクションはどれですか？</p> <p>以下のオプションから正しい答えを選択してください：<br> < / p>	sa:	Conditions <br>  条件|<p><br></p><p>The optional&nbsp;Conditions&nbsp;section includes statements that define when a resource is created or when a property is defined. For example, you can compare whether a value is equal to another value. Based on the result of that condition, you can conditionally create resources. If you have multiple conditions, separate them with commas.</p><p><br></p><p>For more information on Cloudformation conditions please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html</a><br><p></p>	Resources <br>  リソース	Mappings <br>  マッピング	Outputs <br>  出力
Test2-69. <p>There are currently multiple applications hosted in a VPC. During monitoring, it has been noticed that multiple port scans are coming in from a specific IP Address block. The internal security team has requested that all offending IP Addresses be denied for the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP Addresses?</p> | <p>現在、VPCでホストされている複数のアプリケーションがあります。監視中に、特定のIPアドレスブロックから複数のポートスキャンが到着していることに気付きました。社内のセキュリティチームは、問題となっているすべてのIPアドレスが今後24時間は拒否されるように要求しています。指定したIPアドレスからのアクセスを迅速かつ一時的に拒否する最も良い方法はどれですか？</p>	sa:	Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block. <br>  VPC内のすべてのパブリックサブネットに関連付けられたネットワークACLを変更して、IPアドレスブロックからのアクセスを拒否します。|<p><br></p><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p><p><br></p><p>Option A and D are incorrect because (a)it will only work for windows-based instances, and (b)better approach is to block the traffic at the subnet layer via NACL rather than instance layer (windows firewall).</p><p>Option B is CORRECT because the best way to allow or deny IP address-based access to the resources in the VPC is to configure rules in the Network access control list (NACL) which are applied at the subnet level.</p><p>Option C is incorrect because (a)you cannot explicitly deny access to particular IP addresses via security group, and (b)better approach is to block the traffic at the subnet layer via NACL rather than instance layer (security group).</p><p><br></p><p>For more information on network ACL’s please refer to the below link:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a><br><br><p></p>	Create an AD policy to modify the Windows Firewall settings on all hosts in the VPC to deny access from the IP Address block. <br>  ADポリシーを作成して、VPC内のすべてのホストのWindowsファイアウォール設定を変更し、IPアドレスブロックからのアクセスを拒否します。	Add a rule to all of the VPC Security Groups to deny access from the IP Address block. <br>  すべてのVPCセキュリティグループにルールを追加して、IPアドレスブロックからのアクセスを拒否します。	Modify the Windows Firewall settings on all AMI's that your organization uses in that VPC to deny access from the IP address block. <br>  組織がそのVPC内でIPアドレスブロックからのアクセスを拒否するために使用するすべてのAMIのWindowsファイアウォール設定を変更します。
Test2-70. <p>You have an Auto Scaling group associated with an Elastic Load Balancer (ELB). You have noticed that instances launched via the Auto Scaling group are being marked unhealthy due to an ELB health check but these unhealthy instances are not being terminated. What do you need to do to ensure that the instances marked unhealthy by the ELB will be terminated and replaced?</p> | <p> Elastic Load Balancer（ELB）に関連付けられたAuto Scalingグループがあります。Auto Scalingグループを介して起動されたインスタンスは、ELBヘルスチェックのために不健康にマークされていますが、これらの不健全なインスタンスは終了していません。ELBによって不健康とマークされたインスタンスが終了され、置き換えられることを確実にするためには、何をする必要がありますか？</p>	sa:	Add an Elastic Load Balancing health check to your Auto Scaling group <br>  Auto ScalingグループにElastic Load Balancingヘルスチェックを追加する|<p><br></p><p>To discover the availability of your EC2 instances, an&nbsp;ELB periodically sends pings, attempts connections, or sends requests to test the EC2 instances. These tests are called&nbsp;health checks. The status of the instances that are healthy at the time of the health check is&nbsp;InService. The status of any instances that are unhealthy at the time of the health check is&nbsp;OutOfService.</p><p>When you allow the Auto Scaling group (ASG) to receive the traffic from the ELB, it gets notified when the instance becomes unhealthy and then it terminates it. See the images in the "More information..." section for more details.</p><p><br></p><p>Option A is incorrect because changing the threshold will not enable ASG to know about the unhealthy instances.</p><p>Option B is CORRECT because when you associate the ELB with ASG, you allow the ASG to receive the traffic from that ELB. As a result, the&nbsp;ASG will get aware about the unhealthy instances and it terminates them.</p><p>Option C is incorrect because increasing the interval will still not communicate the information about the unhealthy instances to the ASG.</p><p>Option D is incorrect because this setting will not communicate the information about the unhealthy instances to the ASG either.</p><p><br></p><p>More information on ELB with Auto Scaling Group:</p><p><img src="https://s3.amazonaws.com/awssap/2_70_1.png" alt="" width="612" height="482" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/2_70_2.png" alt="" width="1097" height="792" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">For more information on ELB, please visit the below URL:</span></p><p><span style="font-size: 1rem;"><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html" target="_blank">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a><br></span></p><br><p></p>	Change the thresholds set on the Auto Scaling group health check <br>  自動スケーリンググループヘルスチェックで設定されたしきい値を変更する	Increase the value for the Health check interval set on the Elastic Load Balancer <br>  Elastic Load Balancerに設定されたヘルスチェック間隔の値を増やす	Change the health check set on the Elastic Load Balancer to use TCP rather than HTTP checks <br>  HTTP検査ではなくTCPを使用するようにElastic Load Balancerで設定されたヘルスチェックを変更する
Test2-71. <p>You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets. One instance is running a database and the other instance an application that will interface with the database.</p><p>You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC?</p><p>Choose 2 correct options from the below:<br></p> | <p>同一の可用性ゾーン（AZ）内の異なるサブネットにある仮想プライベートクラウド（VPC）内に2つのEC2インスタンスがあります。1つのインスタンスはデータベースを実行し、もう1つのインスタンスはデータベースとインタフェースするアプリケーションを実行します。</p> <p>アプリケーションが正常に動作するために、互いに話すことができるかどうかを確認します。これらのEC2インスタンスがVPC内で通信できるように、VPC設定で確認する必要があるのはどちらですか？</p> <p>以下の2つの正しいオプションを選択してください：<br> </p>	ma:	o:Security groups are set to allow the application host to talk to the database on the right port/protocol. <br>  セキュリティグループは、アプリケーションホストが正しいポート/プロトコルでデータベースと通信できるように設定されています。|<p><br></p><p>In order to have the instances communicate with each other, you need to properly configure both Security Group and Network access control lists (NACLs). For the exam, remember that Security Group operates at the instance level; where as, the NACL operates at subnet level.</p><p><span style="font-size: 1rem;">Option A is CORRECT because the security groups must be defined in order to allow web server to communicate with the database server. An example image from the AWS documentation is given below:</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/2_71_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="462" height="471"></p><p><br></p><p>Option B is incorrect because it is not necessary to have the two instances of the same type or be using same key-pair.</p><p>Option C is incorrect because configuring NAT instance or NAT gateway will not enable the two servers to communicate with each other. NAT instance/NAT gateway are used to enable the communication between instances in the private subnets and internet.</p><p>Option D is CORRECT because the two servers are in two separate subnets. In order for them to communicate with each other, you need to h<span style="font-size: 1rem;">ave the NACL’s configured as shown below:</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/2_71_2.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="487" height="545"></p><p><br></p><p>For more information on VPC and Subnets, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><br><p></p>	x:Both instances are the same instance class and using the same key-pair. <br>  両方のインスタンスが同じインスタンスクラスであり、同じキーペアを使用しています。	x:That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. <br>  既定のルートがNATインスタンスまたはインターネットゲートウェイ（IGW）に設定されて通信すること。	o:A network ACL that allows communication between the two subnets. <br>  2つのサブネット間の通信を可能にするネットワークACL。
Test2-72. <p>You are managing a legacy application inside VPC with hard-coded IP addresses in its configuration. Which mechanisms will allow the application to failover to new instances without the need for reconfiguration?</p><p>Choose 2 options from the below:<br></p> | <p>ハードコードされたIPアドレスを持つVPC内のレガシーアプリケーションを構成で管理しています。再構成を必要とせずにアプリケーションが新しいインスタンスにフェイルオーバーできるようにするメカニズムはどれですか？</p> <p>以下の2つのオプションを選択してください：<br> </p>	ma:	x:Create an ELB to reroute traffic to the failover instance <br>  フェールオーバーインスタンスへのトラフィックを再ルーティングするELBを作成する|<p><br></p><p>Option A is incorrect because rerouting to a failover instance in case of hardcoded IP address is not possible via ELB.</p><p>Option B is CORRECT because the attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.</p><p>Option C is incorrect because Route 53 cannot reroute the traffic between the to failover instance with the same IP address.</p><p>Option D is CORRECT because you can have a secondary IP address that can be configured on the primary ENI of the failover instance.</p><p><br></p><p><b>Best Practices for Configuring Network Interfaces</b></p><p></p><div><ul type="disc"><li><p>You can attach a network interface to an instance when it's running (hot attach), when it's stopped (warm attach), or when the instance is being launched (cold attach).</p></li><li><p>You can detach secondary (eth<em>N</em>) network interfaces when the instance is running or stopped. However, you can't detach the primary (eth0) interface.</p></li><li><p>You can attach a network interface in one subnet to an instance in another subnet in the same VPC; however, both the network interface and the instance must reside in the same Availability Zone.</p></li><li><p>When launching an instance from the CLI or API, you can specify the network interfaces to attach to the instance for both the primary (eth0) and additional network interfaces.</p></li><li><p>Launching an Amazon Linux or Windows Server instance with multiple network interfaces automatically configures interfaces, private IPv4 addresses, and route tables on the operating system of the instance.</p></li><li><p>A warm or hot attach of an additional network interface may require you to manually bring up the second interface, configure the private IPv4 address, and modify the route table accordingly. Instances running Amazon Linux or Windows Server automatically recognize the warm or hot attach and configure themselves.</p></li><li><p>Attaching another network interface to an instance (for example, a NIC teaming configuration) cannot be used as a method to increase or double the network bandwidth to or from the dual-homed instance.</p></li><li><p>If you attach two or more network interfaces from the same subnet to an instance, you may encounter networking issues such as asymmetric routing. If possible, use a secondary private IPv4 address on the primary network interface instead. For more information, see&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html#ManageMultipleIP">Assigning a Secondary Private IPv4 Address</a>.</p></li></ul></div><br><p></p><p>For more information on Network Interfaces, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a><br><p></p>	o:Create a secondary ENI that can be moved to the failover instance <br>  フェールオーバーインスタンスに移動できるセカンダリENIを作成する	x:Use Route53 health checks to reroute the traffic to the failover instance <br>  Route 53ヘルスチェックを使用して、トラフィックをフェールオーバーインスタンスに再ルーティングする	o:Assign a secondary private IP address to the primary ENI of the failover instance <br>  セカンダリプライベートIPアドレスをフェイルオーバーインスタンスのプライマリENIに割り当てます。
Test2-73. <p>A media company produces new video files on-premises every day with a total size of around 100GB after compression. All files have a size of 1 -2 GB and need to be uploaded to Amazon S3 every night in a fixed time window between 3 AM and 5 AM. Current upload takes almost 3 hours, although less than half of the available bandwidth is used. What step(s) would ensure that the file uploads are able to complete in the allotted time window?</p> | <p>メディア会社は、圧縮後に毎日約100GBの容量で新しいビデオファイルを毎日作成します。すべてのファイルは1〜2GBのサイズで、毎晩3時から5時の間の固定時間帯にAmazon S3にアップロードする必要があります。現在のアップロードには、使用可能な帯域幅の半分以下が使用されますが、ほぼ3時間かかります。ファイルのアップロードが割り当てられた時間枠内で完了できるようにするにはどのような手順が必要ですか？</p>	sa:	Upload the files in parallel to S3 <br>  S3に並行してファイルをアップロードする|<p><br></p><p>When uploading large videos it’s always better to make use of AWS multipart file upload, especially when the bandwidth is not fully utilized.</p><p>Option A is incorrect because existing bandwidth itself is not fully utilized. Increasing the bandwidth is not going to help; in fact, it will add to the cost.</p><p>Option B is CORRECT because parallel upload of the files via AWS multipart upload will fully utilize the available bandwidth and increase the throughput. It also has additional benefits as mentioned below in the "More Information" section.</p><p>Option C is incorrect because there is a restriction on the size of upload in a single PUT operation. You cannot upload a file of size more than 5GB in a single upload. So this option is not going to help at all. You need to use multipart upload.</p><p>Option D is incorrect because this option requires you to put all the files daily on a storage drive and send it to AWS. Since the data has to be uploaded in a certain time frame and there is sufficient bandwidth already available, multipart upload is the best option compared to AWS Import/Export.</p><p><br></p><p><b>More information on and benefits of Multipart upload on S3</b></p><p><span style="font-size: 1rem;">&nbsp;Below is the advantage of multipart upload:</span></p> <ul> <li>Improved throughput—you can upload parts in parallel to improve throughput.</li> <li>Quick recovery from any network issues—smaller part size minimizes the impact of restarting a failed upload due to a network error.</li> <li>Pause and resume object uploads—you can upload object parts over time. Once you initiate a multipart upload there is no expiry; you must explicitly complete or abort the multipart upload.</li> <li>Begin a upload before you know the final object size—you can upload an object as you are creating it.</li></ul><span style="font-size: 1rem;"><br>For more information on Multi-part file upload for S3, please visit the URL - <br></span><br><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html</a><br>	Increase your network bandwidth to provide faster throughput to S3 <br>  ネットワーク帯域幅を拡大してS3へのスループットを向上	Pack all files into a single archive, upload it to S3, and then extract the files in AWS <br>  すべてのファイルを1つのアーカイブにまとめ、S3にアップロードしてから、AWSでファイルを展開します	Use AWS Import/Export to transfer the video files <br>  AWS Import / Exportを使用してビデオファイルを転送する
Test2-74. <p>Your team is excited about the use of AWS because now they have access to "programmable Infrastructure”. You have been asked to manage your AWS infrastructure in a manner similar to the way you might manage application code. You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development, test, QA , and production). Which approach addresses this requirement?</p> | <p>あなたのチームは、「プログラマブルインフラストラクチャ」にアクセスできるようになり、AWSの使用に興奮しています。アプリケーションコードを管理する方法と同様の方法でAWSインフラストラクチャを管理するように求められました。さまざまなバージョンのインフラストラクチャの正確なコピーを展開し、さまざまな環境に変更を加え、以前のバージョンに戻し、特定の時点（実行、テスト、QA、および運用）で実行されているバージョンを特定します。 </p>	sa:	Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. <br>  AWS CloudFormationとGITなどのバージョン管理システムを使用して、インフラストラクチャを導入および管理します。|<p><br></p><p>You can use AWS Cloud Formation’s sample templates or create your own templates to describe the AWS resources, and any associated dependencies or runtime parameters, required to run your application. You don’t need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work. CloudFormation takes care of this for you. After the AWS resources are deployed, you can modify and update them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software. You can also visualize your templates as diagrams and edit them using a drag-and-drop interface with the AWS CloudFormation Designer.</p><p><br></p><p><span style="font-size: 1rem;">Option A is incorrect because Cost Allocation Reports is not helpful for the purpose of the question.</span></p><p>Option B is incorrect because CloudWatch is used for monitoring the metrics pertaining to different AWS resources.</p><p>Option C is incorrect because it does not have the concept of programmable Infrastructure.</p><p>Option D is CORRECT because&nbsp;AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.</p><p><br></p><p>For more information on CloudFormation, please visit the link:</p><p></p><a href="https://aws.amazon.com/cloudformation/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/cloudformation/</a><br><p></p>	Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. <br>  AWS CloudWatchのメトリックとアラートをリソースタギングとともに使用して、インフラストラクチャを導入および管理します。	Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. <br>  AWS BeanstalkとGITのようなバージョン管理システムを使用して、インフラストラクチャを展開および管理します。	Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. <br>  コスト割り当てレポートとAWS Opsworkを使用して、インフラストラクチャを展開および管理します。
Test2-75. <p>What would happen to an RDS (Relational Database Service) multi-Availability Zone deployment if the primary DB instance fails?</p> | <p>プライマリDBインスタンスに障害が発生した場合、RDS（Relational Database Service）マルチアベイラビリティゾーンの展開はどうなりますか？</p>	sa:	The canonical name record (CNAME) is changed from primary to standby. <br>  標準名レコード（CNAME）は、プライマリからスタンバイに変更されます。|<p><br></p><p>Option A is incorrect because IP address of the primary and standby instances remain same and are not changed.</p><p>Option B is incorrect because the CNAME record of the primary DB instance changes to the standby instance.</p><p>Option C is incorrect because there is no new instance created in the standby AZ.</p><p>Option D is CORRECT because the CNAME of the primary DB instance changes to the standby instance so that there is no impact of on the application setting or any reference to the primary instance.</p><p><br></p><p><b>More information on Amazon RDS Multi-AZ deployment:</b></p><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure (for example, instance hardware failure, storage failure, or network disruption), Amazon RDS performs an automatic failover to the standby, so that you can resume database operations as soon as the failover is complete.&nbsp;</p><p>And as per the AWS documentation, the CNAME is changed to the standby DB when the primary one fails.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/2_75_1.png" alt="" width="1087" height="300" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p><img src="https://s3.amazonaws.com/awssap/2_75_2.png" alt="" width="793" height="433" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>For more information on Multi-AZ RDS, please visit the link:</p><p></p><a href="https://aws.amazon.com/rds/details/multi-az/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/rds/details/multi-az/</a><br><p></p>	The primary RDS (Relational Database Service) DB instance reboots and remains as primary. <br>  プライマリRDS（リレーショナルデータベースサービス）DBインスタンスが再起動され、プライマリとして残ります。	A new DB instance is created in the standby availability zone. <br>  スタンバイ可用性ゾーンに新しいDBインスタンスが作成されます。	The IP address of the primary DB instance is switched to the standby DB instance. <br>  プライマリDBインスタンスのIPアドレスは、スタンバイDBインスタンスに切り替えられます。
Test2-76. <p>A user is trying to save some cost on the AWS services. Which of the below-mentioned options will not help him to save cost?</p> | <p>ユーザーがAWSサービスの費用を節約しようとしています。以下のオプションのどれが費用を節約するのに役立たないでしょうか？</p>	sa:	Delete the AutoScaling launch configuration after the instances are terminated. <br>  インスタンスの終了後にAutoScaling起動設定を削除します。|<p><br>Option A is incorrect because EBS volumes do have a costing aspect and hence deleting the unutilized volumes will save some cost.</p><p>Option B is CORRECT because an unused AutoScaling launch configuration will not cost anything.</p><p>Option C is incorrect because non-associated Elastic IP will cost you if not released.</p><p>Option D is incorrect because an ELB without any instances behind it incurs costs.</p><p><br></p><p><span style="font-size: 1rem;">For more information on AWS Pricing, please visit the link:</span></p><p><a href="https://aws.amazon.com/pricing/services/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/pricing/services/</a></p><p></p>	Delete the unutilized EBS volumes once the instance is terminated. <br>  インスタンスが終了したら、未使用のEBSボリュームを削除します。	Release the elastic IP if not required once the instance is terminated. <br>  インスタンスが終了した後は必須ではない場合、エラスティックIPを解放します。	Delete the AWS ELB after all the instances behind it are terminated. <br>  AWS ELBの背後にあるすべてのインスタンスが終了したら、AWS ELBを削除します。
Test2-77. <p>An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the requirement for making an orderly deployment of the software?</p> | <p>組織は、プロダクションロールアウトにAWSを使用する予定です。組織は、自動的にLAMPスタックを作成し、最新のPHPインストール可能ファイルをS3からダウンロードし、ELBをセットアップするように、導入のための自動化を実装したいと考えています。以下のAWSサービスのうち、ソフトウェアの整然とした展開を行うための要件を満たすものはどれですか？</p>	sa:	AWS Elastic Beanstalk <br>  AWS Elastic Beanstalk|<p></p><div>The Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services.</div><div><br></div><div>We can simply upload code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. Meanwhile we can retain full control over the AWS resources used in the application and can access the underlying resources at any time.</div><div><br></div><div>Hence, A is the CORRECT answer.</div><div><br></div><div>For more information on launching a LAMP stack with Elastic Beanstalk:</div><div><span style="font-size: 1rem;"><a href="https://aws.amazon.com/getting-started/projects/launch-lamp-web-app/" target="_blank">https://aws.amazon.com/getting-started/projects/launch-lamp-web-app/</a></span></div><div><div><br></div></div><br><p></p>	AWS Cloudfront <br>  AWS Cloudfront	AWS Cloudformation <br>  AWSクラウドフォーメーション	AWS DevOps <br>  AWS DevOps
Test2-78. <p>Your company has a lot of GPU intensive workloads. Also, these workloads are part of a process in which some steps need manual intervention. Which of the below options works out for the above-mentioned requirement?</p> | <p>貴社にはGPUの負荷が大きい作業がたくさんあります。また、これらのワークロードは、一部の手順で手動による介入が必要なプロセスの一部です。下のオプションのうち、上記の要件に適合するものはどれですか？</p>	sa:	Use Amazon Simple Workflow (SWF) to manage the workflow. Use an autoscaling group of G2 instances in a placement group. <br>  Amazon Simple Workflow（SWF）を使用してワークフローを管理します。プレースメントグループ内のG2インスタンスの自動拡張グループを使用します。|<p><br></p><p><span id="docs-internal-guid-6c5f78f5-07d1-17c5-619f-922f676fec70"></span></p><p dir="ltr" style="">Tip: Whenever the scenario in the question mentions about high graphical processing servers with low latency networking, always think about using G2 instances. And, when there are tasks involving human intervention, always think about using SWF.</p><br><p dir="ltr" style="">Option A is incorrect because AWS Data Pipeline cannot work in hybrid approach where some of the tasks involve human actions.</p><p dir="ltr" style="">Option B is CORRECT because (a) it uses G2 instances which are specialized for high graphical processing of data with low latency networking, and (b) SWF supports workflows involving human interactions along with AWS services.</p><p dir="ltr" style="">Option C is incorrect because it uses C3 instances which are used for situations where compute optimization is required. In this scenario, you should be using G2 instances.</p><p dir="ltr" style="">Option D is incorrect because (a) AWS Data Pipeline cannot work in hybrid approach where some of the tasks involve human actions, and (b) it uses C3 instances which are used for situations where compute optimization is required. In this scenario, you should be using G2 instances.</p><br><p dir="ltr" style="">More information on G2 instances:</p><p dir="ltr" style="">Using G2 instances is preferred. Hence option C and D are wrong.</p><p></p><p><img src="https://s3.amazonaws.com/awssap/2_78_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="964" height="446"></p><p><br></p><p><span id="docs-internal-guid-6c5f78f5-07d2-7083-22c6-a3cda64731cd"></span></p><p dir="ltr" style="">For more information on Instances types, please visit the below URL:</p><p dir="ltr" style=""><a href="https://aws.amazon.com/ec2/instance-types/" target="_blank">https://aws.amazon.com/ec2/instance-types/</a></p><p dir="ltr" style="">Since there is an element of human intervention, SWF can be used for this purpose.</p><p dir="ltr" style="">For more information on SWF, please visit the below URL:</p><p dir="ltr" style=""><a href="https://aws.amazon.com/swf/" target="_blank">https://aws.amazon.com/swf/</a></p><br><p></p><ul></ul><p></p>	Use AWS Data Pipeline to manage the workflow. Use an auto-scaling group of G2 instances in a placement group. <br>  AWSデータパイプラインを使用してワークフローを管理します。プレースメントグループ内のG2インスタンスの自動スケーリンググループを使用します。	Use Amazon Simple Workflow (SWF) to manage the workflow. Use an autoscaling group of C3 instances with SR-IOV (Single Root I/O Virtualization). <br>  Amazon Simple Workflow（SWF）を使用してワークフローを管理します。SR-IOV（シングルルートI / O仮想化）を備えたC3インスタンスのオートスケーリンググループを使用します。	Use AWS data Pipeline to manage the workflow. Use auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization). <br>  AWSデータパイプラインを使用してワークフローを管理します。SR-IOV（シングルルートI / O仮想化）でC3の自動スケーリンググループを使用します。
Test2-79. <p>An organization is generating digital policy files which are required by the admins for verification. Once the files are verified they may not be required in the future unless there is some compliance issue. Which is the best possible solution if the organization wants to save them in a cost-effective way?</p> | <p>組織は、管理者が検証に必要とするデジタルポリシーファイルを生成しています。ファイルが検証されたら、将来的にはコンプライアンスの問題がなければ必要ないかもしれません。組織が費用対効果の高い方法でそれらを保存したい場合は、どの方法が最善の解決策ですか？</p>	sa:	AWS Glacier <br>  AWS氷河|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">This question is basically asking you to choose a cost-effective archival solution. Amazon Glacier is most suited for such scenarios.</span></p><p><span style="font-size: 1rem;">Amazon Glacier is an extremely low-cost storage service that provides secure, durable, and flexible storage for data backup and archival. With Amazon Glacier, customers can reliably store their data for as little as $0.004 per gigabyte per month. Amazon Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS, so that they don’t have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection and repair, or time-consuming hardware migrations.</span></p><p>Option A and B are incorrect because they are used for real time storage.</p><p>Option C is incorrect because this is a database service not an archival one.</p><p>Option D is , as mentioned above, CORRECT.</p><p><br></p><p>For more information on Glacier please visit the link –</p><p></p><a href="https://aws.amazon.com/glacier/details/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/glacier/details/</a><br><p></p>	AWS S3 <br>  AWS S3	AWS RDS <br>  AWS RDS	AWS RRS <br>  AWS RRS
Test2-80. <p>You have large EC2 instances in your AWS infrastructure which you have recently setup. These instances carry out the task of creating JPEG files and store them on a S3 bucket and occasionally need to perform high computational tasks. After close monitoring you see that the CPUs of these instances remain idle most of the time.&nbsp;<br></p><p>Which of the below solutions will ensure better utilization of resources?</p> | <p>最近設定したAWSインフラストラクチャに大きなEC2インスタンスがあります。これらのインスタンスは、JPEGファイルを作成してS3バケットに格納するタスクを実行し、時には高度な計算タスクを実行する必要があります。詳細な監視の後、これらのインスタンスのCPUはほとんどの場合アイドル状態のままです。＆nbsp; <br> </p> <p>リソースをより効率的に活用するためのソリューションはどれですか？</p>	sa:	Use T2 instances if possible. <br>  可能であれば、T2インスタンスを使用してください。|<p><br></p><p>In this scenario the problem is that the large EC2 instances are mostly remaining unused. Hence, the solution should be to use instances that can cost less but still be able to carry out occasional high computational tasks.&nbsp;</p><p><span style="font-size: 1rem;">T2 instances are Burstable Performance Instances that provide a baseline level of CPU performance with the ability to burst above the baseline. The baseline performance and ability to burst are governed by CPU Credits. T2 instances accumulate CPU Credits when they are idle, and consume CPU Credits when they are active. T2 instances are the lowest-cost Amazon EC2 instance option designed to dramatically reduce costs for applications that benefit from the ability to burst to full core performance whenever required.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because there is no issue with the current use of S3.</span></p><p>Option B is incorrect because adding another large instance is, on the contrary, an expensive solution and would add to the existing cost.</p><p>Option C is CORRECT because T2 instances are cost-effective and also provide a baseline level of CPU performance with the ability to burst above the baseline whenever required.</p><p>Option D is incorrect because this option is not going to make efficient use of the current instances. It will not lower the cost of the architecture.</p><p><br></p><p>For more information on Instances types, please visit the below URL:</p><p></p><a href="https://aws.amazon.com/ec2/instance-types/t2/" target="_blank">https://aws.amazon.com/ec2/instance-types/t2/</a><br><p></p>	Add additional large instances by introducing a task group. <br>  タスクグループを導入して、大きなインスタンスを追加します。	Use Amazon glacier instead of S3. <br>  S3の代わりにAmazonの氷河を使用してください。	Ensure the application hosted on the EC2 instances uses larger files on S3 to handle more load. <br>  より多くの負荷を処理するために、S3上の大きなファイルを使用してアプリケーションがEC 2インスタンスでホストされていることを確認してください。
