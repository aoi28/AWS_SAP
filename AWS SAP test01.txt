#format:table
#title:AWS SAP
#question_count:5
#shuffle_questions:false
#shuffle_choices:true
Test1-01. <p>Your company is hosting a web application on AWS. According to the architectural best practices, the application must be highly available, scalable, cost effective, with high-performance and should require minimal human intervention. You have deployed the web servers and database servers in public and private subnet of the VPC respectively. While testing the application via web browser, you noticed that the application is not accessible. Which configuration settings you must do to tackle this problem?</p> <p>Choose 2 options from below:</p> | <p>あなたの会社はAWSでウェブアプリケーションをホストしています。アーキテクチャ上のベストプラクティスによれば、アプリケーションは高可用性、スケーラビリティ、コスト効率、高性能でなければならず、人間の介入を最小限に抑える必要があります。VPCの公開サブネットとプライベートサブネットにそれぞれWebサーバーとデータベースサーバーを展開しています。Webブラウザ経由でアプリケーションをテストしているときに、アプリケーションにアクセスできないことに気付きました。この問題を解決するには、どの設定を行う必要がありますか？</p> <p>以下の2つのオプションを選択してください：</p>	ma:	x:Configure a NAT instance in your VPC and create a default route via the NAT instance and associate it with all subnets. Configure a DNS A record that points to the NAT instance public IP address. <br>  VPCでNATインスタンスを設定し、NATインスタンス経由でデフォルトルートを作成し、すべてのサブネットに関連付けます。NATインスタンスのパブリックIPアドレスを指すDNS Aレコードを設定します。|<p><br></p><p>Option A is incorrect because (a) NAT instance is ideally used to route traffic from a private subnet to the internet via a public subnet, (b) NAT instance is not managed by AWS and requires to be configured and maintained by the user; hence, adding to the overhead, and (c) if not scaled, can cause performance bottleneck. NAT Gateway is a preferred option over NAT instances.</p><p>Option B is recommending us to use AWS CloudFront and configure the distributions Origin to the web server and then use a AWS Route 53 ‘CNAME’ for the CloudFront Distribution. Even though CloudFront is highly available and is accessible to the Internet, it would work better if the Origin for the AWS CloudFront Distribution was pointed to an AWS ELB rather than to the Web Server itself.&nbsp;</p><p>Since the Origin would only be a Web Server, if this server goes offline for a period of time, the web site would become unavailable the content is not cached at the Edge location or if the TTL for the content expires.&nbsp;</p><p>So, Option B is incorrect as well.</p><p>Option C is CORRECT. Because, (a) if the web servers are behind an ELB, the load on the web servers will be uniformly distributed. Hence, if any of the web servers goes offline or becomes non-responsive, the traffic would be routed to other online web servers; making the application highly available, and (b) You can use Route53 to set the ALIAS record that points to the ELB endpoint.</p><p><img src="https://s3.amazonaws.com/awssap/1_01_1.png" alt="" width="522" height="534" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p>Option D is CORRECT. Because in Route53, you can either resolve the DNS query via creating an ALIAS record pointing to the ELB endpoint or an A record pointing to the IP Addresses of the application. As the EIPs are static (will not be changed) and can be assigned to new web servers if any of the web servers becomes offline, the EIPs can be used in the A record. The health check would ensure that Route53 checks the health of the record set before the failover to other web servers.</p><p><img src="https://s3.amazonaws.com/awssap/1_01_2.png" alt="" width="476" height="746" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Option E is incorrect because AWS does not recommend to assign IP Addresses to ELB. The public IP addresses get automatically assigned to the ELB’s. You should always use the DNS name of the ELB. </p><p><br></p><p><a href="https://aws.amazon.com/elasticloadbalancing/"></a></p>	x:Configure a CloudFront distribution and configure the origin to point to the private IP addresses of your Web servers. Configure a Route53 CNAME record to your CloudFront distribution. <br>  CloudFrontディストリビューションを構成し、WebサーバーのプライベートIPアドレスを指すように起点を構成します。 CloudFrontディストリビューションにRoute53 CNAMEレコードを設定します。	o:Place all your web servers behind ELB. Configure a Route53 ALIAS-Record to point to the ELB DNS name. <br>  すべてのWebサーバーをELBの背後に置きます。 ELB DNS名を指すようにRoute53 ALIAS-Recordを設定します。	o:Assign EIP's to all web servers. Configure a Route53 A-Record set with all EIPs with health checks and DNS failover. <br>  すべてのWebサーバーにEIPを割り当てます。 ヘルスチェックとDNSフェールオーバーを備えたすべてのEIPでRoute53 Aレコードセットを設定します。	x:Configure ELB with an EIP. Place all your Web servers behind ELB. Configure a Route53 A record that points to the EIP. <br>  ELBをEIPで設定します。すべてのWebサーバーをELBの背後に置きます。ルートを設定する53 EIPを示すレコード。
Test1-02. <p>You have developed an application that processes massive amount of process logs generated by web site and mobile app. This application requires the ability to analyze petabytes of unstructured data using Amazon Elastic MapReduce. The resultant data is stored on Amazon S3. You have deployed c4.8xlarge Instance type, whose CPUs are mostly idle during the data processing. Which of the below options would be the most cost-efficient way to reduce the runtime of the log processing job?</p> | <p> Webサイトとモバイルアプリケーションで生成された大量のプロセスログを処理するアプリケーションを開発しました。このアプリケーションでは、Amazon Elastic MapReduceを使用して、ペタバイトの非構造化データを分析する機能が必要です。結果のデータはAmazon S3に保存されます。c4.8xlargeインスタンス・タイプをデプロイしました。このタイプのCPUは、データ処理中にほとんどアイドル状態です。以下のうち、ログ処理ジョブの実行時間を短縮する最もコスト効率のよい方法はどれですか？</p>	sa:	Use smaller instances that have higher aggregate I/O performance. <br>  集約されたI / Oパフォーマンスのより小さいインスタンスを使用します。|<p>Option A is incorrect even though storing the files on S3 storage class such as RRS would reduce the cost. The problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.&nbsp;</p><p>Option B is incorrect as adding more of c4.8xlarge instance type in the task instance group would create more idle resources, which is - in fact - more costly.</p><p>Option C is CORRECT because, since the CPU’s are mostly idle, it means that you have provisioned a larger instance which is under-utilized. A better cost-efficient solution would be to use smaller instances. For batch processing jobs such as the one mentioned in this scenario, you can use multiple t2 instances - which support the concept of CPU bursts - are ideal for situations where there are bursts of CPU during certain periods of time only.</p><p><span style="font-size: 1rem;">Option D is incorrect even though storing the files on S3 storage class such as RRS would reduce the cost. The problem in the scenario is that the provision of a large instance is wasted due to it being idle most of the time.&nbsp;</span></p><p><span style="font-size: 1rem;">For more information on resizing of the EC2 instances, please visit the URL given below:</span></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html</a><br></p><p><br></p>	Add additional c4.8xlarge instances by introducing a task instance group. The network performance of 10 Gigabit per EC2 instance would increase the processing speed; thus reducing the load on the EMR cluster. <br>  タスクインスタンスグループを導入して、さらにc4.8xlargeインスタンスを追加します。 EC2インスタンスあたり10ギガビットのネットワークパフォーマンスは、処理速度を向上させます。 EMRクラスターへの負荷が軽減されます。	Create log files with smaller size and store them on Amazon S3. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults. <br> より小さなサイズのログファイルを作成し、Amazon S3に保存します。 ライフサイクルポリシーをS3バケットに適用して、ファイルを最初にRRSに、次にAmazon Glacier Vaultに移動させます。	Create fewer, larger log files. Compress and store them on Amazon S3 bucket. Apply the life cycle policy to the S3 bucket such that the files would be first moved to RRS and then to Amazon Glacier vaults. <br>  少数の大きなログファイルを作成します。 Amazon S3バケットに圧縮して保存します。 ライフサイクルポリシーをS3バケットに適用して、ファイルを最初にRRSに、次にAmazon Glacier Vaultに移動させます。
Test1-03. <p>Your department creates regular analytics reports from your company’s log files. All log data is collected in Amazon S3 and processed by daily Amazon Elastic MapReduce (EMR) jobs that generate daily PDF reports and aggregated tables in CSV format for an Amazon Redshift data warehouse. You notice that the average EMR hourly usage is more than 25% but less than 50%.</p><p>Your CFO requests you to optimize the cost structure for this system. Which of the following alternatives will lower costs without compromising the average performance of the system or data integrity for the raw data?</p> | <p>部門では、会社のログファイルから定期的な分析レポートを作成します。すべてのログデータはAmazon S3で収集され、Amazon Redshiftデータウェアハウス用のCSV形式の日別PDFレポートと集計テーブルを生成する日々のAmazon Elastic MapReduce（EMR）ジョブによって処理されます。</p> <p>お客様のCFOは、このシステムのコスト構造を最適化するようお客様に依頼しています。以下の代替案のどれが、システムの平均パフォーマンスや生データのデータ保全性を損なうことなくコストを削減できますか？</p>	sa:	Use Reduced Redundancy Storage (RRS) for PDF and CSV files in S3. Use a combination of Spot instances and Reserved Instances for Amazon EMR jobs. Use Reserved instances for Amazon Redshift. <br>  S3のPDFおよびCSVファイルのRedundancy Redundancy Storage（RRS）を使用します。 Amazon EMRジョブには、スポットインスタンスとリザーブドインスタンスの組み合わせを使用します。 Amazon Redshiftの予約インスタンスを使用します。|<p>There are 3 main considerations in this question - Cost optimization, Average Performance should not be compromised, and Data integrity should be preserved.&nbsp;</p><p>Since the log files are getting stored on S3, the Reduced Redundancy Storage (RRS) provides the reduce storage cost (Cost Optimization) as well as preserves the data integrity. Note that the RRS has reduced durability (99.99% compared to 99.999999999% of S3), but does not compromise with data integrity.&nbsp;</p><p>To maintain the average performance of data processing, instances must be reserved so that there would be guaranteed availability of the compute resources. Additionally, you can use spot instances which are more cost effective compared to reserved instances to increase the overall performance of processing of the log files. The average hourly usage is more than 17% - which according to the Amazon EMR Best Practices - further reassures that reserved instances should be used (see the page 29 of the document given in the <i>More information</i> section).</p><p>Option A is incorrect because if only spot instances are used for data processing, the average performance could be hampered as the availability of spot instance is not always guaranteed (The availability of spot instances depend on the available capacity and spot price. AWS can terminate the spot instances if the spot price exceeds your maximum price).</p><p><span style="font-size: 1rem;">Option B is CORRECT because (a) You need to store only the PDF and CSV files in RRS, not the entire data as these files can be generated by the EMR jobs in case of loss of data, and (b) The combination of the reserved and spot will guarantee the average performance and it will help reduce cost (by not reserving all the instances, only reserving the required instances). Moreover, AWS RRS would reduce cost and guarantee data integrity as discussed above.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because (a) You should not store the entire data on RRS as this storage class is only for the data that is noncritical, and reproducible, and (b) If only spot instances are used for data processing, the average performance could be hampered as the availability of spot instance is not always guaranteed (The availability of spot instances depend on the available capacity and spot price. AWS can terminate the spot instances if the spot price exceeds your maximum price).<br></span></p><p><span style="font-size: 1rem;">Option D is incorrect because you should not store the entire data on RRS as this storage class is only for the data that is noncritical, and reproducible. If the data is lost, the PDF and CSV files cannot be regenerated.</span></p><div></div><br>For more information on Amazon EMR Best Practices, please refer to the link below:<br><a href="https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf</a><br><br><p></p>	Use Reduced Redundancy Storage (RRS) for PDF and CSV data in Amazon S3. Add Spot instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift. <br>  Amazon S3のPDFおよびCSVデータにRedundancy Redundancy Storage（RRS）を使用します。 SpotインスタンスをAmazon EMRジョブに追加します。 Amazon Redshift用の予約済みインスタンスを使用します。	Use Reduced Redundancy Storage (RRS) for all data in Amazon S3. Add Spot Instances to Amazon EMR jobs. Use Reserved Instances for Amazon Redshift. <br>  Amazon S3のすべてのデータに対してRedundancy Redundancy Storage（RRS）を使用します。 SpotインスタンスをAmazon EMRジョブに追加します。 Amazon Redshift用の予約済みインスタンスを使用します。	Use Reduced Redundancy Storage (RRS) for all data in S3. Use a combination of Spot instances and Reserved Instances for Amazon EMR jobs. Use Spot Instances for Amazon Redshift. <br>  S3のすべてのデータに対してRedundancy Redundancy Storage（RRS）を使用します。 Amazon EMRジョブには、スポットインスタンスとリザーブドインスタンスの組み合わせを使用します。 Amazon Redshiftにスポットインスタンスを使用する。
Test1-04. <p>You are the new IT architect in a company that operates a mobile sleep tracking application. When activated at night, the mobile app is sending collected data points of 1 KB every 5 minutes to your middleware. The middleware layer takes care of authenticating the user and writing the data points into an Amazon DynamoDB table. Every morning, you scan the table to extract and aggregate last night’s data on a per-user basis, and store the results in Amazon S3. Users are notified via Amazon SMS mobile push notifications that new data is available, which is parsed and visualized by the mobile app. Currently, you have around 100k users. You have been tasked to optimize the architecture of the middleware system to lower the cost. What would you recommend?</p> <p>Choose 2 options from below:<br><br></p> | <p>モバイルスリープトラッキングアプリケーションを運営する会社の新しいITアーキテクトです。夜間にアクティブにすると、モバイルアプリは5分ごとに1 KBのデータポイントをミドルウェアに送信します。ミドルウェア層は、ユーザーを認証し、データポイントをAmazon DynamoDBテーブルに書き込む処理を行います。毎朝、テーブルをスキャンして、昨夜のデータをユーザー単位で抽出して集計し、その結果をAmazon S3に格納します。ユーザーはAmazon SMSのモバイルプッシュ通知を介して、新しいデータが利用可能であることを通知されます。これは、モバイルアプリケーションによって解析され視覚化されます。現在、約100,000人のユーザーがいます。ミドルウェア・システムのアーキテクチャーを最適化してコストを削減する任務がありました。</p> <p>下記から2つのオプションを選択してください：<br> <br> </p>	ma:	o:Create a new Amazon DynamoDB table each day and drop the one for the previous day after its data is on Amazon S3. <br>  新しいAmazon DynamoDBテーブルを毎日作成し、そのデータがAmazon S3に入ってから前日のデータを削除します。|<p>Option A is CORRECT because (a) The data stored would be old/obsolete anyways and need not be stored; hence, lowering the cost, and (b) Storing the data in DynamoDB is expensive; hence, you should not keep the tables with the data not needed.&nbsp;</p><p><span style="font-size: 1rem;">Option B is incorrect because&nbsp;(a) Storing the data in DynamoDB is more expensive than S3, and (b) giving the app access to the DynamoDB to read the data is an operational overhead.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) it uses SQS which reduce the provisioned output cutting down on the costs, and (b) acts as a buffer that absorbs sudden higher load, eliminating going over the provisioned capacity.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because the data is only read once before its stored to S3. The cache would only be useful if you read things multiple times. Also, in this scenario optimizing "write" operations is most desired, not "read" ones.</span></p><div><span style="font-size: 1rem;">Option E is incorrect because (a) Amazon Redshift cluster is primarily used for OLAP transactions, not OLTP; hence, not suitable for this scenario, and (b) moving the storage to Redshift cluster means deploying large number of EC2 instances that are continuously running, which is not a cost-effective solution.</span></div><div><span style="font-size: 1rem;"><br></span></div><div><span style="font-size: 1rem;">For complete guidelines on working with DynamoDB, please visit the below URL:</span></div> <p><a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html" target="_blank"><strong>http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GuidelinesForTables.html</strong></a></p>	x:Have the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3. <br>  Amazon S3に格納されているJSONファイルの代わりに、モバイルアプリがAmazon Dynamo DBに直接アクセスできるようにします。	o:Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput. <br>  Amazon SQSキューを導入して、Amazon DynamoDBテーブルへの書き込みをバッファし、プロビジョニングされた書き込みスループットを削減します。	x: Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput. <br>  Amazon Elasticacheを導入してAmazon DynamoDBテーブルからの読み取りをキャッシュし、プロビジョニングされた読み取りスループットを削減します。	x:Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3. <br>  Amazon DynamoDBとAmazon S3の両方を置き換えるAmazon Redshiftクラスタにデータを直接書き込みます。
Test1-05. <p>Your website is serving on-demand training videos to your workforce. Videos are uploaded monthly in high-resolution MP4 format. Your workforce is distributed globally often on the move and using company-provided tablets that require the HTTP Live Streaming (HLS) protocol to watch a video. Your company has no video transcoding expertise and it required you may need to pay for a consultant. How would you implement the most cost-efficient architecture without compromising high availability and quality of video delivery’?</p> | <p>あなたのウェブサイトは、オンデマンドのトレーニングビデオをあなたの労働力に提供しています。動画は毎月高解像度のMP4形式でアップロードされます。あなたの従業員は、ビデオを見るためにHTTPライブストリーミング（HLS）プロトコルを必要とする会社提供のタブレットを使用して、世界中で頻繁に配信されます。あなたの会社にはビデオトランスコーディングの専門知識はなく、コンサルタントに支払う必要があるかもしれません。高い可用性とビデオ配信の品質を損なうことなく、最も費用対効果の高いアーキテクチャをどのように実装しますか？</p>	sa:	Elastic Transcoder to transcode original high-resolution MP4 videos to HLS. Use S3 to host videos with Lifecycle Management to archive original files to Glacier after a few days. Use CloudFront to serve HLS transcoded videos from S3. <br>  Elastic Transcoderを使用してオリジナルの高解像度MP4ビデオをHLSにトランスコードします。 S3を使用してLifecycle Managementでビデオをホストし、数日後に元のファイルを氷河にアーカイブします。 CloudFrontを使用して、S3からHLSトランスコードされた動画を配信します。|<p><br></p><p>There are four most important design considerations here: (a)&nbsp;video transcoding expertise, (b) global distribution of the content, (c) cost-effective solution, and (d) no compromise with the high availability and quality of the video delivery.</p><p>Amazon Elastic Transcoder is a media transcoding service in the cloud. It is designed to be a highly scalable, easy to use and a cost-effective way for developers and businesses to convert (or “transcode”) media files from their source format into versions that will playback on various devices like smartphones, tablets, and PCs.<br></p><p><br></p><p>Option A is CORRECT because (a) it uses Amazon Elastic Transcoder that converts from MP4 to HLS, (b) S3 Object Lifecycle Management reduces the cost by archiving the files to Glacier, and (c) CloudFront - which is a highly available service - enables the global delivery of the video without compromising the video delivery speed or quality.</p><p><span style="font-size: 1rem;">Option B is incorrect because (a) it necessitates the overhead of infrastructure provisioning. i.e deploying of EC2 instances, auto scaling, SQS queue / pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost efficient solution.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because the use of EBS snapshots is not a cost effective solution compared to S3 Object Lifecycle Management.</span></p><p>Option D is incorrect because&nbsp;(a) it necessitates the overhead of infrastructure provisioning. i.e deploying of EC2 instances, auto scaling, SQS queue / pipeline, (b) setting up of EC2 instances to handle global delivery of content is not a cost efficient solution, and (d)&nbsp;the use of EBS snapshots is not a cost effective solution compared to S3 Object Lifecycle Management.</p><p><br></p><p>For more information on Elastic transcoder, please visit the below URL:<br><a href="https://aws.amazon.com/elastictranscoder/" target="_blank">https://aws.amazon.com/elastictranscoder/</a></p><p>Cloudfront can be then used to deliver the content to the users from its various edge locations.<br><br></p>	A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue. Use S3 to host videos with Lifecycle Management to archive all files to Glacier after a few days. Use CloudFront to serve HLS transcoding videos from Glacier. <br>  タスクを配布するためにSQSを使用してEC2上で実行されるビデオトランスコードパイプライン、およびキューの長さに応じてノードの数を調整する自動スケーリング S3を使用してLifecycle Managementでビデオをホストすると、数日後にすべてのファイルを氷河にアーカイブすることができます。 CloudFrontを使用して、氷河からHLSコード変換ビデオを配信します。	Elastic Transcoder to transcode original high-resolution MP4 videos to HLS EBS volumes to host videos and EBS snapshots to incrementally backup original rues after a few days. CloudFront to serve HLS transcoded videos from EC2. <br>  Elastic Transcoderを使用してオリジナルの高解像度MP4ビデオをHLS EBSボリュームにトランスコードしてビデオをホストし、EBSスナップショットを使用して数日後に元のルートを増分バックアップします。 CloudFrontは、EC2からHLSトランスコードされたビデオを配信します。	A video transcoding pipeline running on EC2 using SQS to distribute tasks and Auto Scaling to adjust the number of nodes depending on the length of the queue EBS volumes to host videos and EBS snapshots to incrementally backup original files after a few days CloudFront to serve HLS transcoded videos from EC2. <br>  タスクを配布するためにSQSを使用するEC2上で実行されるビデオトランスコーディングパイプライン、および数日後に元のファイルを増分バックアップするEBSスナップショットをホストするキューEBSボリュームの長さに応じてノードの数を調整する自動スケーリング。 CloudFrontは、EC2からHLSトランスコードされたビデオを配信します。
Test1-06. <p>You’ve been hired to enhance the overall security posture for a very large e-commerce site. They have a well architected multi-tier application running in a VPC that uses ELBs in front of both the web and the app tier with static assets served directly from S3. They are using a combination of RDS and DynamoDB for their dynamic data and then archiving nightly into S3 for further processing with EMR. They are concerned because they found questionable log entries and a flood of superfluous requests for accessing the resources. You suspect that someone is attempting to gain unauthorized access. Which approach provides a cost-effective scalable mitigation to this kind of attack?</p> | <p>大規模な電子商取引サイトのセキュリティの全体的な姿勢を強化するために雇われました。彼らは、VPCでうまく構築されたマルチティアアプリケーションを持っています。このアプリケーションは、S3から直接提供される静的資産を使ってWeb層とアプリケーション層の前にELBを使用します。彼らは、動的データ用にRDSとDynamoDBの組み合わせを使用しています。その後、夜間にS3にアーカイブして、EMRでさらに処理します。彼らは疑わしいログエントリとリソースにアクセスするための余分な要求の氾濫を発見したため、懸念しています。誰かが不正なアクセスを試みていると思われます。どのようなアプローチが、この種の攻撃に対する費用効果の高いスケーラブルな軽減策ですか？</p>	sa:	Add a WAF tier by creating a new ELB and an AutoScaling group of EC2 Instances running a host-based WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would pass the traffic to the current web tier. The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group <br>  新しいELBを作成し、ホストベースのWAFを実行するEC 2インスタンスの自動スケーリンググループを作成して、WAFティアを追加します。彼らはRoute 53を新しいWAF層ELBに解決するようにリダイレクトします。WAF層は、トラフィックを現在のWeb層に渡します。Web層セキュリティグループは、WAF層セキュリティグループからのトラフィックのみを許可するように更新することができます|<p><br></p><p>In such scenarios where you are designing a solution to prevent the DDoS attack (indicated by the flood of superfluous request for accessing the resources and suspicious activity) , always think of using Web Access Firewall (WAF).</p><p>AWS WAF is a web application firewall that helps protect your web&nbsp;applications from common web exploits that could affect application&nbsp;availability, compromise security, or consume excessive resources. AWS&nbsp;WAF gives you control over which traffic to allow or block to your web&nbsp;applications by defining customizable web security rules. You can use AWS&nbsp;WAF to create custom rules that block common attack patterns, such as SQL&nbsp;injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond&nbsp;quickly to changing traffic patterns.&nbsp;</p><p><br></p><p>Option A is incorrect because, although this option could work, the setup is very complex and it is not a cost effective solution.</p><p>Option B is incorrect because, (a) even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and (b) it does not prevent the attack from the new source of threat.</p><p>Option C is CORRECT because (a) WAF Tiers acts as the first line of defense, it filters out the known sources of attack and blocks common attack patterns, such as SQL&nbsp;injection or cross-site scripting, (b) the ELB of the application is not exposed to the attack, and most importantly (c) this pattern - known as "WAF Sandwich" pattern - has WAF layer with EC2 instances are placed between two ELBs - one that faces the web, receives all the traffic, and sends them to WAF layer to filter out the malicious requests, and sends the filtered non-malicious requests, another ELB - which receives the non-malicious requests and send them to the EC2 instances for processing. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/1_06_1.png" alt="" width="1261" height="702" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Option D is incorrect because there is no such thing as&nbsp;Advanced Protocol Filtering feature for ELB.</p><p></p><p><br></p><p>For more information on WAF, please visit the below URL:</p><p></p><a href="https://aws.amazon.com/waf/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/waf/</a><br><p></p><br><p></p>	Add previously identified host file source IPs as an explicit INBOUND DENY NACL to the web tier subnet. <br>  以前に識別されたホストファイルのソースIPを明示的なINBOUND DENY NACLとしてWeb層サブネットに追加します。	Recommend that they lease space at a DirectConnect partner location and establish a 1G DirectConnect ?connection to their VPC. Then they would establish Internet connectivity into their space, filter the traffic in hardware Web Application Firewall (WAF) and then pass the traffic through the DirectConnect connection into their application running in their VPC. <br>  VPCに接続するDirective Directorでスペースをリースすることを推奨します。その後、彼らは自分のスペースにインターネット接続を確立し、ハードウェアWebアプリケーションファイアウォール（WAF）でトラフィックをフィルタリングし、DirectConnect接続を介してVPCで動作するアプリケーションにトラフィックをフィルタします。	Remove all but TLS 1 & 2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality. <br>  Web層ELBからTLS 1および2を除くすべてを削除し、Advanced Protocol Filteringを有効にします。これにより、ELB自体がWAF機能を実行できるようになります。
Test1-07. <p>You currently operate a web application in the AWS US-East region. The application runs on an auto-scaled layer of EC2 instances and an RDS Multi-AZ database. Your IT security compliance officer has tasked you to develop a reliable and durable logging solution to track changes made to your EC2, IAM and RDS resources. The solution must ensure the integrity and confidentiality of your log data. Which of the below solutions would you recommend?</p> | <p>現在AWS US-East地域でWebアプリケーションを運用しています。このアプリケーションは、EC2インスタンスとRDS Multi-AZデータベースの自動スケーリングされたレイヤー上で実行されます。ITセキュリティコンプライアンスオフィサーは、EC2、IAM、およびRDSリソースの変更を追跡するための信頼性の高い耐久性のあるロギングソリューションを開発するように任命しました。ソリューションでは、ログデータの整合性と機密性を確保する必要があります。どれをお勧めしますか？</p>	sa:	Create a new CloudTrail trail with one new S3 bucket to store the logs and with the option that applies trail to all regions selected. Use IAM roles S3 bucket policies and Multi Factor Authentication (MFA) to delete on the S3 bucket that stores your logs. <br>  1つの新しいS3バケットでログを保存し、選択したすべてのリージョンにトレールを適用するオプションを使用して、新しいCloudTrailトレイルを作成します。 S3バケットポリシーとログを格納するS3バケットを削除するのに、MFA（Multi Factor Authentication）を使用します。|<p><br></p><p>For the scenarios where the application is tracking (or needs to track) the changes made by any AWS service, resource, or API, always think about AWS CloudTrail service.</p><p>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticated AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets.&nbsp;</p><p>The most important points in this question are (a) Use of a single&nbsp;<span id="docs-internal-guid-743fa474-6e4a-7b28-c400-5f2859d1e47e">S3 bucket, (b) CloudTrail with the&nbsp;option that applies trail to all regions enabled, (b) Data integrity, and (d) Confidentiality.</span></p><p>Option A is CORRECT because (a) it uses AWS CloudTrail with the&nbsp;option that applies trail to all regions enabled, (b) a single new S3 bucket and IAM Roles so that it has the confidentiality, (c)&nbsp; MFA on Delete on S3 bucket so that it maintains the data integrity. See the AWS CloudTrail setting below which sets the option that applies trail to all regions:</p><p><img src="https://s3.amazonaws.com/awssap/1_07_1.png" alt="" width="808" height="184" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Options B is incorrect because (a) although&nbsp;it uses AWS CloudTrail,&nbsp;the option that applies trail to all regions is not enabled, and (b) SNS notifications can be a overhead in this situation.</p><p>Option C is incorrect because (a) as an existing S3 bucket is used, it may already be accessed to the user, hence not maintaining the confidentiality, and (b) it is not using IAM roles.</p><p>Option D is incorrect because (a)&nbsp;although&nbsp;it uses AWS CloudTrail,&nbsp;the option that applies trail to all regions is not enabled, and (b) three S3 buckets are not needed.</p><p><br></p><p>For more information on Cloudtrail, please visit the below URL:</p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events" target="_blank">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a><br></p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p>	Create a new CloudTrail with one new S3 bucket to store the logs. Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs. <br>  1つの新しいS3バケットで新しいCloudTrailを作成し、ログを保存します。 ログファイル配信通知を管理システムに送信するようにSNSを設定します。 ログを格納するS3バケットにIAMロールとS3バケットポリシーを使用します。	Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the option that applies trail to all regions selected. Use S3 ACLs and Multi Factor Authentication (MFA) to delete on the S3 bucket that stores your logs <br>  既存のS3バケットを使用して新しいCloudTrailトレイルを作成し、ログを保存し、選択したすべてのリージョンにトレールを適用するオプションを使用します。 S3 ACLとログを格納するS3バケットを削除するのに、MFA（Multi Factor Authentication）を使用します。	Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs. <br>  AWS管理コンソール用、AWS SDK用、コマンドラインツール用のログを格納する3つの新しいS3バケットを備えた3つの新しいCloudTrailトレイルを作成します。 ログを格納するS3バケットにIAMロールとS3バケットポリシーを使用します。
Test1-08. <p>An enterprise wants to use a 3rd party SaaS application hosted by another AWS account. The SaaS application needs to have access to issue several API commands to discover Amazon EC2 resources running within the enterprise’s account. The enterprise has internal security policies that require any outside access to their environment must conform to the principles of least privilege and there must be controls in place to ensure that the credentials used by the SaaS vendor cannot be used by any other third party. <br></p><p>Which of the following options would meet all of these conditions?</p> | <p>企業は、別のAWSアカウントでホストされているサードパーティのSaaSアプリケーションを使用したいと考えています。SaaSアプリケーションは、エンタープライズアカウント内で実行されているAmazon EC2リソースを検出するためのAPIコマンドをいくつか発行する必要があります。企業には、社内のセキュリティポリシーがあり、その環境への外部アクセスは最小限の特権の原則に従わなければならず、SaaSベンダーが使用する資格情報を他の第三者が使用できないようにするためのコントロールが必要です。<br> </p> <p>これらの条件をすべて満たす以下のオプションはどれですか？</p>	sa:	Create an IAM role for cross-account access that allows the SaaS provider’s account to assume the role and assign it a policy that allows only the actions required by the SaaS application. <br>  クロスアカウントアクセス用のIAMロールを作成すると、SaaSプロバイダのアカウントはそのロールを引き受け、SaaSアプリケーションが要求するアクションのみを許可するポリシーを割り当てることができます。|<p>When a user, a resource, an application, or any service needs to access any AWS service or resource, always prefer creating appropriate role that has least privileged access or only required access, rather than using any other credentials such as keys.</p><p>Option A is incorrect because you should never share your access and secret keys.</p><p>Option B is incorrect because (a) when a user is created, even though it may have the appropriate policy attached to it, its security credentials are stored in the EC2 which can be compromised, and (b) creation of the appropriate role is always the better solution rather than creating a user.</p><p>Option C is CORRECT because AWS role creation allows cross-account access to the application to access the necessary resources. See the image and explanation below:</p><p><span style="font-size: 1rem;">Many SaaS platforms can access AWS resources via a Cross-account access created in AWS. If you go to Roles in your identity management, you will see the ability to add a cross-account role.</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_08_1.png" alt="" width="1140" height="476" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>Option D is incorrect because the role is to be assigned to the application and it's resources, not the EC2 instances.</p><p>For more information on the cross-account role, please visit the&nbsp;<span style="font-size: 1rem;">below URL:</span></p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p>	Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application. Create a new access and secret key for the user and provide these credentials to the SaaS provider. <br>  エンタープライズアカウント内でIAMユーザーを作成すると、SaaSアプリケーションが要求するアクションのみを許可するユーザーポリシーをIAMユーザーに割り当てます。 ユーザの新しいアクセスと秘密鍵を作成し、これらの資格情報をSaaSプロバイダに提供します。	From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account <br>  AWS Management Consoleから、Security Credentialsページに移動し、アカウントのアクセスと秘密鍵を取得します。	Create an IAM role for EC2 instances, assign it a policy which allows only the actions required for the Saas application to work, provide the role ARM to the SaaS provider, to be used when launching their application instances. <br>  EC2インスタンスのIAMロールを作成し、Saasアプリケーションに必要なアクションのみを許可するポリシーを割り当て、SaaSプロバイダにARMのロールを提供し、アプリケーションインスタンスを起動するときに使用するポリシーを割り当てます。
Test1-09. <p><span style="font-size: 1rem;">You are designing a data leak prevention solution for your VPC environment. You want your VPC Instances to be able to access software depots and distributions on the Internet for product updates. The depots and distributions are accessible via the third party via their URLs. You want to explicitly deny any other outbound connections from your VPC instances to hosts on the internet.</span></p> <p>Which of the following options would you consider?</p> | <p> <span style = "font-size：1rem;">VPC環境用のデータ漏洩防止ソリューションを設計しています。 VPCインスタンスが製品アップデートのためにインターネット上のソフトウェアデポとディストリビューションにアクセスできるようにします。 ディポとディストリビューションは、URLを介して第三者からアクセスできます。 VPCインスタンスからインターネット上のホストへの他のアウトバウンド接続を明示的に拒否したいとします。</ span> </p> <p>次のオプションのどれを検討しますか？</p>	sa:	Configure a web proxy server in your VPC and enforce URL-based rules for outbound access. Remove default routes. <br>  VPC内にWebプロキシサーバを設定し、アウトバウンドアクセス用のURLベースのルールを適用します。既定のルートを削除します。|<p>There are 3 main considerations in this scenario: (a) the instances in your VPC needs internet access, (b) the access should be restricted for product updates only, and (c) all other outbound connection requests must be denied.</p><p><span style="font-size: 1rem;">With such scenarios, you should not put your instances in public subnet as they would have access to internet without any restrictions. So, you should put them in a private subnet, and since there is a need of a logic for filtering the requests from client machines, configure a proxy server.</span></p><p><span style="font-size: 1rem;"><b>What is a Proxy Server?</b></span></p><p><span style="font-size: 1rem;">Proxy server is a server that acts as a mediator between client(s) that sends requests and server that receives the requests and replies back. If any client requires any resources, it connects to the proxy server, and the proxy server evaluates the request based on its filtering rules. If the requests are valid, it connects to the server which receives the request and replies back. The proxy server also maintains cache; i.e., if any subsequent requests from same or other clients are received, it returns the result from the cache, saving the trip to and from the server. Hence, proxy servers tend to improve the performance. See the diagram below:</span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/1_09_1.jpg" alt="" width="600" height="300" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p><p><span style="font-size: 1rem;">Option A is CORRECT because a proxy server (a) filters requests from the client, and allows only those that are related to the product updates, and (b) in this case helps filtering all other requests except the ones for the product updates.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because a security group cannot filter request based on URLs and you cannot specify deny rules.</span></p><p>O<span style="font-size: 1rem;">ption C is incorrect because even though moving the instances in a private subnet is a good idea, the routing table does not have the filtering logic, it only connects the subnets with internet gateway.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because you are trying to deny the outbound traffic from your VPC. There is no need for any traffic to be allowed in.</span></p><p>An example of setting up a proxy server can be found via the below URL:</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/articles/6463473546098546" target="_blank">https://aws.amazon.com/articles/6463473546098546</a></span></li></ul><p></p>	Implement security groups and configure outbound rules to only permit traffic to software depots. <br>  セキュリティグループを実装し、アウトバウンドルールを構成して、ソフトウェアデポへのトラフィックのみを許可します。	Move all your instances into private VPC subnets. Remove default routes from all routing tables and add specific routes to the software depots and distributions only. <br>  すべてのインスタンスをプライベートVPCサブネットに移動します。すべてのルーティングテーブルからデフォルトルートを削除し、特定のルートをソフトウェアデポと配布にのみ追加します。	Implement network access control lists to allow traffic from specific destinations, with an implicit deny as a rule. <br>  暗黙の拒否をルールとして、特定の宛先からのトラフィックを許可するネットワークアクセス制御リストを実装します。
Test1-10. <p>An administrator is using Amazon CloudFormation to deploy a three-tier web application that consists of a web tier and application tier that will utilize Amazon DynamoDB for storage. While creating the CloudFormation template which of the following would allow the application instance access to the DynamoDB tables without exposing API credentials?</p> | <p>管理者は、Amazon CloudFormationを使用して、Amazon DynamoDBをストレージに利用するWeb層とアプリケーション層で構成される3層Webアプリケーションを配備しています。CloudFormationテンプレートを作成すると、次のうちどれがアプリケーションインスタンスがAPIの資格情報を公開せずにDynamoDBテーブルにアクセスできるようになりますか？</p>	sa:	Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and reference the Role in the instance profile property of the application instance. <br>  Dynamo DBテーブルの読み書きに必要な権限を持つIDおよびアクセス管理ロールを作成し、アプリケーションインスタンスのインスタンスプロファイルプロパティでロールを参照します。|<p>The scenario requires the instance to have access to DynamoDB tables without having to use the API credentials. In such scenarios, always think of creating IAM Roles rather than IAM Users.</p><p><span style="font-size: 1rem;">Option A is incorrect because the IAM Role is not associated to the application by referencing an instance profile, it has to be used as an instance profile property.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, and (b) using IAM Role is more secured way of accessing the resources than using IAM Users with security credentials.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) it uses IAM Role with the appropriate permissions to access the resource, and (b) it references that Role in the instance profile property of the application instance. See an example given below:</span></p><p><img src="https://s3.amazonaws.com/awssap/1_10_1.png" alt="" role="presentation" class="atto_image_button_text-bottom" width="635" height="622"><br></p><p>&nbsp;</p><p><span style="font-size: 1rem;">Option D is incorrect because (a) you should never expose the Access and Secret Keys while accessing the AWS resources, (b) using IAM Role is more secured way of accessing the resources than using IAM Users with security credentials.</span></p><p><span style="font-size: 1rem;">For more information on granting access to AWS resources via EC2 instance profile property, please visit the below URL:</span></p><p><span style="font-size: 1rem;"><a href="https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html" target="_blank">https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html</a><br></span></p><p>For more information on adding IAM roles in CloudFormation templates, please visit the below URL:</p><p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html" target="_blank">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html</a></p>	Use the Parameter section in the Cloud Formation template to have the user input Access and Secret Keys from an already created IAM user that has me permissions required to read and write from the required DynamoDB table. <br>  クラウドフォーメーションテンプレートのパラメータセクションを使用して、必要なDynamoDBテーブルの読み書きに必要な権限を持つ、すでに作成されたIAMユーザからのアクセスキーとシークレットキーをユーザに入力させる。	Create an Identity and Access Management Role that has the required permissions to read and write from the required DynamoDB table and associate the Role to the application instances by referencing an instance profile. <br>  DynamoDBテーブルから読み書きするために必要な権限を持つIDとアクセス管理ロールを作成し、インスタンスプロファイルを参照してロールをアプリケーションインスタンスに関連付けます。	Create an identity and Access Management user in the CloudFormation template that has permissions to read and write from the required DynamoDB table, use the GetAtt function to retrieve the Access and secret keys and pass them to the application instance through user-data. <br>  DynamoDBテーブルを読み書きする権限を持つCloudFormationテンプレート内のIDとアクセス管理ユーザーを作成し、GetAtt関数を使用してAccessおよび秘密キーを取得し、ユーザーデータを使用してアプリケーションインスタンスに渡します。
Test1-11. <p>An AWS customer is deploying an application that is composed of an auto scaling group of EC2 Instances. The customer's security policy requires that every outbound connection from these instances to any other service within the customers Virtual Private Cloud must be authenticated using a unique X.509 certificate that contains the specific instance ID. In addition, an X.509 certificate must be designed by the AWS Key Management Service (KMS) in order to be trusted for authentication.</p> <p>Which of the following configurations will support these requirements?<br><br></p> | <p> AWSのお客様は、EC2インスタンスの自動スケーリンググループで構成されるアプリケーションを導入しています。 顧客のセキュリティポリシーでは、これらのインスタンスから顧客のVirtual Private Cloud内の他のサービスへのすべてのアウトバウンド接続を、特定のインスタンスIDを含む一意のX.509証明書を使用して認証する必要があります。 さらに、X.509証明書は、認証のために信頼されるために、AWS Key Management Service（KMS）によって設計されなければなりません。</ p> <p>これらの要件をサポートする次の構成はどれですか？</p> <p> </p>	sa:	Configure the Auto Scaling group to send an SNS notification of the launch of a new instance to the trusted key management service. Have the AWS KMS generate a signed certificate and send it directly to the newly launched instance. <br> Auto Scalingグループを設定して、新しいインスタンスの起動に関するSNS通知をトラステッドキー管理サービスに送信します。 AWS KMSに署名証明書を生成させ、新しく起動されたインスタンスに直接送信します。|<p><br></p><p>This scenario requires (a) a x.509 certificate per instance created in the auto scaling group, (b) the certificate should be unique that contains the instance id, and (c) this certificate should be generated by a key management service (authoritative service).</p><p><span style="font-size: 1rem;">Option A is incorrect because (a) storing the signed certificate in S3 is bad idea as it will not be unique for each the instance id, and (b) S3 is not a key management service and does cannot generate such certificates.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because you need to generate the instance id first before generating the certificate that will be unique for that instance id. Therefore, embedding a certificate in image and then launching the instance will not be useful at all.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) once the instance is launched in the auto scaling group, it notifies the key management service to generate a signed certificate, (b) the key management service is trusted, and (c) once the certificate is generated, it is directly sent to the newly created instance; hence, the workflow is logical.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because (a) the onus is on the EC2 instances to generate the signed certificate, (b) the requirement is to use a key management service to generate the signed certificate, and (c)&nbsp; AWS KMS does not have any feature to 'poll' any service.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on AWS KMS, please visit the below URL:</p><p></p><a href="https://d0.awsstatic.com/whitepapers/KMS-Cryptographic-Details.pdf" target="_blank" style="font-size: 1rem;">https://d0.awsstatic.com/whitepapers/KMS-Cryptographic-Details.pdf</a><br><br><p></p>	Embed a certificate into the Amazon Machine Image that is used by the Auto Scaling group Have the launched instances, generate a certificate signature request with the instance’s assigned instance-id to the AWS KMS for signature. <br>  Amazonマシンイメージに証明書を埋め込む起動されたインスタンスを作成し、インスタンスに割り当てられたインスタンスIDを使用してAWS KMSに署名用の証明書署名要求を生成します。	Configure an IAM Role that grants access to an Amazon S3 object containing a signed certificate and configure an Auto Scaling group to launch instances with this role. Have the instances bootstrap, get the certificate from Amazon S3 upon first boot. <br>  署名付き証明書を含むAmazon S3オブジェクトへのアクセスを許可するIAMロールを設定し、このロールを使用してインスタンスを起動するAuto Scalingグループを設定します。 インスタンスをブートストラップし、最初の起動時にAmazon S3から証明書を取得します。	Configure the launched instances to generate a new certificate upon first boot. Have the AWS KMS poll the Auto Scaling group for associated instances and send new instances a certificate signature (that contains the specific instance-id). <br>  起動したインスタンスを設定して、最初の起動時に新しい証明書を生成します。 AWS KMSに関連するインスタンスのAuto Scalingグループをポーリングさせ、新しいインスタンスに証明書の署名（特定のインスタンスIDを含む）を送信します。
Test1-12. <p>You are given a task with moving a legacy application from a virtual machine running inside your datacenter to an Amazon VPC. Unfortunately, this app requires access to a number of on-premise services and no one who configured the app still works for your company. Even worse, there’s no documentation for it. What will allow the application running inside the VPC to reach back and access its internal dependencies without being reconfigured?</p> <p>Choose 3 options the below:<br></p> | <p>データセンター内で実行されている仮想マシンからレガシーアプリケーションをAmazon VPCに移動するタスクが与えられます。 残念ながら、このアプリは多数のオンプレミスサービスにアクセスする必要があり、アプリを設定したユーザーの誰もあなたの会社ではまだ動作していません。 さらに悪いことに、それに関する文書はありません。 VPC内で実行されているアプリケーションが、再構成せずに内部の依存関係にアクセスしてアクセスできるようにするにはどうすればよいでしょうか？</p> <p>次の3つのオプションを選択します。<br> </p>	ma:	o:An AWS Direct Connect link between the VPC and the network housing the internal services. <br>  VPCとネットワーク間のAWS Direct Connectリンクが内部サービスを提供します。|<p></p><div><div>The scenario requires you to connect your on-premise server/instance with Amazon VPC. When such scenarios are presented, always think about services such as Direct Connect, VPN, and VM Import and Export as they help either connecting the instances from different location or importing them from one location to another.</div><div><br></div><div>Option A is CORRECT because Direct Connect sets up a dedicated connection between on-premise data-center and Amazon VPC, and provides you with the ability to connect your on-premise servers with the instances in your VPC.</div><div><br></div><div>Option B is incorrect as you normally create a VPN connection based off of a customer gateway and a virtual private gateway (VPG) in AWS.&nbsp;</div><div><br></div><div>Option C is incorrect as EIPs are not needed as the instances in the VPC can communicate with on-premise servers via their private IP address.</div><div><br></div><div>Option D is CORRECT because, there should not be a conflict between IP address of on-premise servers and the instances in VPC for them to communicate.</div><div><br></div><div>Option E is incorrect because, Route53 is not useful in resolving on-premise dependency.</div><div><br></div><div>Option F is CORRECT because VM Import Export service helps you to import the virtual machine images from the data center to AWS platform as EC2 instances<span style="font-size: 1rem;">&nbsp;and export them back to your on-premises environment. This offering allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances.</span></div></div><br><p></p>	x:An Internet Gateway to allow a VPN connection. <br>  VPN接続を許可するインターネットゲートウェイ。	x:An Elastic IP address on the VPC instance <br>  VPCインスタンス上の弾性IPアドレス	o:An IP address space that does not conflict with the one on-premises <br>  IPアドレス空間が社内のものと競合しない	x:Entries in Amazon Route 53 that allow the Instance to resolve its dependencies’ IP addresses <br>  インスタンスがその依存関係のIPアドレスを解決できるようにするAmazon Route 53のエントリ	o: A VM Import of the current virtual machine. <br>  現在の仮想マシンのVMインポート。
Test1-13. <p>Your company has recently extended its data center into a VPC on AWS to add burst computing capacity as needed. Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You don’t want to create new IAM users for each member and make those users sign in again to the AWS Management Console. Which option below will meet the needs of your NOC members?</p> | <p>貴社は最近、データセンターをAWS上のVPCに拡張し、必要に応じてバーストコンピューティング能力を追加しました。Network Operations Centerのメンバーは、必要に応じてAWS Management Consoleにアクセスし、Amazon EC2インスタンスを管理できる必要があります。メンバーごとに新しいIAMユーザーを作成し、そのユーザーがAWS Management Consoleに再度サインインすることは望ましくありません。下記のどのオプションがあなたのNOCメンバーのニーズを満たすのですか？</p>	sa:	Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバーにAWSシングルサインオン（SSO）エンドポイント経由でAWS管理コンソールへのフェデレーションアクセスを許可する|<p><br></p><p>This scenario has two requirements: (a) temporary access to AWS resources be given to certain users or application (NOC members in this case), and (b) you are not supposed to create new IAM users for the NOC members to log into AWS console.&nbsp;</p><p><span style="font-size: 1rem;">This scenario is handled by a concept named "Federated Access". Read this for more information on federated access:&nbsp;<a href="https://aws.amazon.com/identity/federation/" target="_blank">https://aws.amazon.com/identity/federation/</a> .</span></p><p><span style="font-size: 1rem;">Read this article for more information on how to establish the federated access to the AWS resources:</span></p><p><a href="https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/" target="_blank">https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/</a><br></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because OAuth 2.0 is not applicable in this scenario as we are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google etc.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because the key point here is that you need to give access to AWS Management Console to only the members of your Network Operations Center using on premises SSO to avoid signing in again. The users should not be using Facebook or Google IDs to login.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) it gives a federated access to the NOC members to AWS resources by using SAML 2.0 identity provider, and (b) it uses on-premise single sign on (SSO) endpoint to authenticate users and gives them access tokens prior to providing the federated access.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because, even though it uses SAML 2.0 identity provider, one of the requirements is not to let users sign in to AWS console using any security credentials.</span></p><p><span style="font-size: 1rem;">See this diagram that explains the Federated Access using SAML 2.0.&nbsp;</span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/1_13_1.png" alt="" width="1216" height="702" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p>	Use web Identity Federation to retrieve AWS temporary security credentials to enable your members to sign in to the AWS Management Console. <br>  Web ID Federationを使用してAWSの一時的なセキュリティ資格情報を取得し、メンバーがAWS Management Consoleにサインインできるようにします。	Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your members to sign in to the AWS Management Console. <br>  OAuth 2.0を使用して、メンバがAWS管理コンソールにサインインできるように、一時的なAWSセキュリティ資格情報を取得します。	Use your on-premises SAML 2.0-compliant identity provider (IDP) to retrieve temporary security credentials to enable members to sign in to the AWS Management Console. <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバがAWS管理コンソールにサインインできるように一時的なセキュリティ資格情報を取得します。
Test1-14. <p>You are designing an SSL/TLS solution that requires HTTPS clients to be authenticated by the Web server using client certificate authentication. The solution must be resilient. Which of the following options would you consider for configuring the web server infrastructure?</p> <p>Choose 2 option from the below</p> | <p>クライアント証明書認証を使用してHTTPSクライアントをWebサーバーで認証する必要があるSSL / TLSソリューションを設計しています。ソリューションは弾力性がなければなりません。</p> <p>下記の2つのオプションを選択してください。</p>	ma:	o:Configure ELB with TCP listeners on TCP/443 and place the Web servers behind it. <br>  TCP / 443でTCPリスナーを使用してELBを構成し、その背後にWebサーバーを配置します。|<p></p><div><div></div></div><div><div>This scenario requires you to setup the web servers in such a way that the HTTPS clients must be authenticated by the client-side certificate (not the server side certificate). There are two ways of architecting this - with ELB and without ELB. (a) With ELB, if you use HTTPS listener, you have to deploy the server side certificate - which is not desired. So, you need to use the TCP listener so that the HTTPS client requests do not terminate at the ELB, they just pass through ELB and terminate at the web server instances. (b) Alternatively, without ELB, you can directly use the web server to communicate with the clients, or set up a Route53 Record Set with the public IP address of the web server(s) such that the client requests would be directly routed to the web server(s).</div><div><br></div><div>Option A is CORRECT because it uses the TCP (443) listener so that the HTTPS client requests do not terminate at the ELB, they just pass through the ELB and terminate at the web server instances.</div><div><br></div><div>Option B is CORRECT because it uses Route53 Record Set with the public IP address of the web server(s) such that the client requests would be directly routed to the web server(s).</div><div><br></div><div>Option C is incorrect because if you use HTTPS listener, you must deploy an SSL/TLS certificate on your load balancer, i.e. authentication via the client certificate is not currently supported.</div><div><br></div><div>Option D is incorrect because this setting is currently not supported.</div></div><p></p>	o:Configure your Web servers with EIP’s. Place the Web servers in a Route53 Record Set and configure health checks against all Web servers. <br>  EIPでWebサーバーを構成します。WebサーバーをRoute 53 Record Setに配置し、すべてのWebサーバーに対してヘルスチェックを構成します。	x:Configure ELB with HTTPS listeners, and place the Web servers behind it. <br>  HTTPSリスナでELBを設定し、Webサーバをその背後に置きます。	x:Configure your web servers as the origins for a CloudFront distribution. Use custom SSL certificates on your CloudFront distribution. <br>  WebサーバーをCloudFrontの配布元として設定します。CloudFrontディストリビューションでカスタムSSL証明書を使用します。
Test1-15. <p>You are designing a connectivity solution between on-premises infrastructure and Amazon VPC. Your servers on-premises will be communicating with your VPC instances. You will be establishing IPSec tunnels over the internet. You will be using VPN gateways and terminating the IPsec tunnels on AWS-supported customer gateways. <br></p><p>Which of the following objectives would you achieve by implementing an IPSec tunnel as outlined above? <br></p><p>Choose 4 answers from the below:<br></p> | <p>オンプレミスインフラストラクチャとAmazon VPC間の接続ソリューションを設計しています。オンプレミスのサーバーがVPCインスタンスと通信します。インターネット経由でIPSecトンネルを確立します。VPNゲートウェイを使用し、AWS対応のカスタマゲートウェイ上でIPsecトンネルを終了します。<br> </p> <p>上記で概説したIPSecトンネルを実装することで、次の目的はどれですか？<br> </p> <p>以下の4つの回答を選択してください：<br> </p>	ma:	x:End-to-end protection of data in transit <br>  通過中のデータのエンドツーエンド保護|<p><span style="font-size: 1rem;">IPSec is designed to provide authentication, integrity, and confidentiality of the data that is being transmitted. IPSec operates at network layer of the OSI model. Hence, it only protects the data that is in transit over the internet. For the full security of the data transmission it is very essential that both the sender and receiver need to be IPSec-aware.</span></p><p>See the diagram of this scenario:</p><p><img src="https://s3.amazonaws.com/awssap/1_15_1.png" alt="" width="575" height="430" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p>Option A is incorrect because (a) IPSec operates at network layer of the OSI model. Hence, it only protects the data that is in transit over the internet, and (b) both the source and the destination (client and server) may not be IPSec aware.</p><p><span style="font-size: 1rem;">Option B is incorrect because the identity authentication of the origin of the data has to be done at the application layer, not the network layer.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because the data that is transiting via the IPSec tunnel is encrypted.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because IPSec protects the data that is in transit over the internet (fundamental responsibility of IPSec tunnel).</span></p><p><span style="font-size: 1rem;">Option E is CORRECT because in this scenario, the IPSec tunnel is established between VPN gateway (VPG) and Customer Gateway (CGW) whose identity gets authenticated during the setup of the IPSec tunnel.</span></p><p><span style="font-size: 1rem;">Option F is CORRECT because - as mentioned earlier - integrity of the data that is transiting via the IPSec tunnel is always preserved (fundamental responsibility of IPSec tunnel).</span></p><p><span style="font-size: 1rem;">For more information on IPSec tunnel, please refer to:</span></p><p><span style="font-size: 1rem;"><a href="http://techgenix.com/securing_data_in_transit_with_ipsec/" target="_blank">http://techgenix.com/securing_data_in_transit_with_ipsec/</a><br></span></p><p>The below link provides an article on the general working of an IPSec tunnel which outlines the advantages of an IPSec tunnel which includes:</p><p><a href="http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html" target="_blank">http://www.firewall.cx/networking-topics/protocols/870-ipsec-modes.html</a></p><p><br></p>	x:End-to-end Identity authentication <br>  エンドツーエンドのID認証	o:Data encryption across the Internet <br>  インターネットを介したデータの暗号化	o:Protection of data in transit over the Internet <br>  インターネット上を通過するデータの保護	o:Peer identity authentication between VPN gateway and customer gateway <br>  VPNゲートウェイと顧客ゲートウェイ間のピア識別認証	o: Data integrity protection across the Internet <br>  インターネットを介したデータの完全性の保護
Test1-16. <p>You are designing an intrusion detection prevention (IDS/IPS) solution for a customer's web application in a single VPC. You are considering the options for implementing IDS/IPS protection for traffic coming from the Internet. Which of the following options would you consider?</p> <p>Choose 2 options from the below<br></p> | <p>お客様のWebアプリケーション用の侵入検知防止（IDS / IPS）ソリューションを単一のVPCで設計しています。インターネットからのトラフィックに対してIDS / IPS保護を実装するためのオプションを検討しています。次のオプションのどれを検討しますか？</p> <p>下記の2つのオプションを選択してください。<br> </p>	ma:	o:Implement IDS/IPS agents on each Instance running In VPC <br>  VPCで動作する各インスタンスでIDS / IPSエージェントを実装する|<p>The main responsibility of Intrusion Detection Systems (IDS) / Intrusion Prevention Systems (IPS) is to (a) detect the vulnerabilities in your EC2 instances, (b) protect your EC2 instances from attacks, and (c) respond to intrusion or attacks against your EC2 instances.</p><p><span style="font-size: 1rem;">The IDS is an appliance that is installed on the EC2 instances that continuously monitors the VPC environment to see if any malicious activity is happening and alerts the system administration if such activity is detected. IPS, on the other hand, is an appliance that is installed on the EC2 instances that monitors and analyzes the incoming and outgoing network traffic for any malicious activities and prevents the malicious requests from reaching to the instances in the VPC.</span></p><p><span style="font-size: 1rem;">This scenario is asking you how you can setup IDS/IPS in your VPC. There are few well known ways: (a) install the IDS/IPS agents on the EC2 instances of the VPC, so that the activities of that instance can be monitored, (b) set up IDS/IPS on a proxy server/NAT through which the network traffic is flowing, or (c) setup a Security-VPC that contains EC2 instances with IDS/IPS capability and peer that VPC with your VPC and always accept the traffic from Security-VPC only.</span></p><p><span style="font-size: 1rem;">Option A is CORRECT because&nbsp; it implements the IDS/IPS agents on each EC2 instances in the VPC.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because promiscuous mode is not supported by AWS.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because ELB with SSL is does not have the intrusion detection/prevention capability.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because a reverse proxy server through which the traffic from instances inside VPC flows outside of it, has the IDS/IPS agent installed.</span></p><p><span style="font-size: 1rem;">For more information on intrusion detection systems in AWS, please refer to the below link:</span></p><p><a href="https://awsmedia.s3.amazonaws.com/SEC402.pdf" target="_blank">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a></p>	x:Configure an instance in each subnet to switch its network interface card to promiscuous mode and analyze network traffic. <br>  各サブネットでインスタンスを設定して、ネットワークインターフェイスカードをプロミスキャスモードに切り替え、ネットワークトラフィックを分析します。	x:Implement Elastic Load Balancing with SSL listeners In front of the web applications <br>  SSLリスナーによるロードバランシングの実装Webアプリケーションの前で	o:Implement a reverse proxy layer in front of web servers and configure IDS/IPS agents on each reverse proxy server. <br>  Webサーバーの前にリバースプロキシレイヤを実装し、各リバースプロキシサーバーでIDS / IPSエージェントを構成します。
Test1-17. <p>You are designing a photo-sharing mobile app. The application will store all pictures in a single Amazon S3 bucket. Users will upload pictures from their mobile device directly to Amazon S3 and will be able to view and download their own pictures directly from Amazon S3. You want to configure security to handle potentially millions of users in the most secure manner possible. What should be done by your server-side application, when a new user registers on the photo-sharing mobile application?</p> |  <p>あなたは写真共有モバイルアプリをデザインしています。 アプリケーションはすべての画像を1つのAmazon S3バケットに保存します。 ユーザーはモバイルデバイスからAmazon S3に直接写真をアップロードし、自分の写真をAmazon S3から直接表示してダウンロードすることができます。 可能な限り最も安全な方法で、潜在的に何百万人ものユーザーを処理するようにセキュリティを構成する必要があります。 新しいユーザーが写真共有モバイルアプリケーションに登録したときに、サーバー側アプリケーションで何をすべきか？</p>	sa:	Record the user's Information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses their mobile app create temporary credentials using the AWS Security Token Service ‘AssumeRole’ function, store these credentials in the mobile app's memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app. <br>  ユーザーの情報をAmazon RDSに記録し、適切な権限でIAMに役割を作成します。 ユーザーがモバイルアプリケーションを使用して、AWSセキュリティトークンサービス 'AssumeRole'機能を使用して一時的な資格情報を作成するときは、これらの資格情報をモバイルアプリのメモリに保存し、それらを使用してAmazon S3にアクセスします。 次回ユーザーがモバイルアプリを実行したときに新しい認証情報を生成します。|<p><br></p><p>This scenario requires the mobile application to have access to S3 bucket. There are potentially millions of users and a proper security measure should be taken. In such question, where mobile applications needs to access AWS Resources, always think about using funtions such as "AssumeRole", "AssumeRoleWithSAML", and "AssumeRoleWithWebIdentity". See the following diagram that explains the flow of actions while using "AssumeRole".</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/1_17_1.png" alt="" role="presentation" class="img-responsive atto_image_button_middle" height="359" width="638"><br></p><p><br></p><p>You can let users sign in using a well-known third-party identity provider such as login with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider. You can exchange the credentials from that provider for temporary permissions to use resources in your AWS account. This is known as the&nbsp;<em>web identity federation</em>&nbsp;approach to temporary access. When you use web identity federation for your mobile or web application, you don't need to create custom sign-in code or manage your own user identities. Using web identity federation helps you keep your AWS account secure because you don't have to distribute long-term security credentials, such as IAM user access keys, with your application.</p><p><br></p><p>Option A is incorrect because you should always grant the short term or temporary credentials for the mobile application. This option asks to create a long term credentials.</p><p><span style="font-size: 1rem;">Option B is CORRECT because (a) it creates an IAM Role with appropriate permissions, (b) it generates temporary security credentials using STS "AssumeRole" function, and (c) it generates new credentials when the user runs the app the next time.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because, even though the set up is very similar to option B, it does not create IAM Role with proper permissions which is an essential step.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create a IAM Role so that the app can access the AWS Resource via "AssumeRole" function.</span></p><p><span style="font-size: 1rem;">Option E is incorrect because, it asks to create an IAM User, not the IAM Role - which is not a good solution. You should create a IAM Role so that the app can access the AWS Resource via "AssumeRole" function.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on AWS temporary credentials, please refer to the below link:</p> <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html" target="_blank" style="background-color: rgb(255, 255, 255); font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a><br><a href="https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html" target="_blank" style="background-color: rgb(255, 255, 255); font-size: 1rem;">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a><br><br>	Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3. <br>  適切な権限を持つAWS Security Token Serviceを使用して、長期の認証情報を作成します。 これらの資格情報をモバイルアプリに保存し、それらを使用してAmazon S3にアクセスします。	Record the user’s Information In Amazon DynamoDB. When the user uses their mobile app create temporary credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app’s memory and use them to access Amazon S3. Generate new credentials the next time the user runs the mobile app. <br>  Amazon DynamoDBでユーザーの情報を記録します。 ユーザーがモバイルアプリを使用する場合、適切な権限を持つAWS Security Token Serviceを使用して一時的な認証情報を作成します。 これらの資格情報をモバイルアプリのメモリに保存し、それらを使用してAmazon S3にアクセスします。 次回ユーザーがモバイルアプリを実行したときに新しい認証情報を生成します。	Create IAM user. Assign appropriate permissions to the IAM user Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3. <br>  IAMユーザーを作成します。 IAMユーザーに適切な権限を割り当てるIAMユーザーのアクセスキーと秘密キーを生成し、モバイルアプリに格納し、これらの資格情報を使用してAmazon S3にアクセスします。	Create an IAM user. Update the bucket policy with appropriate permissions for the IAM user. Generate an access Key and secret Key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3. <br>  IAMユーザーを作成します。IAMユーザーのアクセスキーと秘密鍵を生成し、モバイルアプリケーションに格納し、これらの資格情報を使用してAmazon S3にアクセスします。
Test1-18. <p>You have an application running on an EC2 Instance which will allow users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?</p> | <p> EC2インスタンス上で動作するアプリケーションを使用して、事前署名されたURLを使用してプライベートS3バケットからファイルをダウンロードできるようにします。URLを生成する前に、アプリケーションはS3でファイルの存在を確認する必要があります。アプリケーションがAWS認証を使用してS3バケットに安全にアクセスするにはどうすればよいですか？</p>	sa:	Create an IAM role for EC2 that allows list access to objects in the S3 bucket. Launch the instance with the role, and retrieve the role’s credentials from the EC2 Instance metadata <br>  S3バケット内のオブジェクトへのリストアクセスを許可するEC2のIAMロールを作成します。 ロールを使用してインスタンスを起動し、EC2インスタンスのメタデータからロールの資格情報を取得します|<p><br></p><p>An IAM&nbsp;<em>role</em>&nbsp;is similar to a user. In that, it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have any credentials (password or access keys) associated with it. Instead, if a user is assigned to a role, access keys are created dynamically and provided to the user.</p><p>You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.</p><p>Whenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User. The reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service. This has security concerns. Whereas, when an IAM Role is created, it has all the necessary policies attached to it. So, the use of access key and secret key is not needed. This is the preferred approach.</p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because you should not use the account access keys , instead you should use the IAM Role.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because, (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on IAM roles, please visit the below URL:</p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p>	Create an IAM user for the application with permissions that allow list access to the S3 bucket launch the instance as the IAM user and retrieve the IAM user’s credentials from the EC2 instance user data. <br>  S3バケットへのリストアクセスがIAMユーザーとしてインスタンスを起動し、EC2インスタンスユーザーデータからIAMユーザーの資格情報を取得する権限を持つアプリケーション用のIAMユーザーを作成します。	Use the AWS account access Keys. The application retrieves the credentials from the source code of the application. <br>  AWSアカウントのアクセスキーを使用します。 アプリケーションは、アプリケーションのソースコードから資格情報を取得します。	Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. <br>  S3バケットへのリストアクセスを許可する権限を持つアプリケーションのIAMユーザーを作成します。 アプリケーションは、アプリケーションユーザーのみに読み取りアクセスを許可する権限を持つ一時ディレクトリからIAMユーザー資格情報を取得します。
Test1-19. <p>You are designing a social media site and are considering how to mitigate distributed denial-of-service (DDoS) attacks. Which of the below are viable mitigation techniques?</p> <p>Choose 3 options from the below</p> | <p>あなたはソーシャルメディアサイトを設計しており、分散型サービス拒否（DDoS）攻撃を軽減する方法を検討しています。以下のうちどれが実行可能な緩和手法ですか？</p> <p>以下の3つのオプションを選択してください。</p>	ma:	x:Add multiple elastic network interfaces (ENIs) to each EC2 instance to increase the network bandwidth. <br>  各EC 2インスタンスに複数のENI（elastic network interface）を追加してネットワーク帯域幅を拡大する。|<p>This question is asking you to select some of the most recommended and widely used DDoS mitigation techniques.</p><p><b>What is a DDoS Attack?</b></p><p>A Distributed Denial of Service (DDoS) attack is an attack orchestrated by distributed multiple sources that makes your web application unresponsive and unavailable for the end users.</p><p><span style="font-size: 1rem;"><b>DDoS Mitigation Techniques</b></span></p><p>Some of the recommended techniques for mitigating the DDoS attacks are&nbsp;</p><p>(i) build the architecture using the AWS services and offerings that have the capabilities to protect the application from such attacks. e.g. CloudFront, WAF, Autoscaling, Route53, VPC etc.</p><p>(ii) defend the infrastructure layer by over-provisioning capacity, and deploying DDoS mitigation systems.</p><p>(iii) defend the application layer by using WAF, and operating at scale by using autoscale so that the application can withstand the attack by scaling and absorbing the traffic.</p><p>(iv) minimizing the surface area of attack</p><p>(v) obfuscating the AWS resources</p><p><span style="font-size: 1rem;">Option A is incorrect because ENIs do not help in increasing the network bandwidth.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because having dedicated instances performing at maximum capacity will not help mitigating the DDoS attack. What is needed is instances behind auto-scaling so that the traffic can be absorbed while actions are being taken on the attack and the application can continue responding to the clients.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) CloudFront is AWS managed service and it can scale automatically, (b) helps absorbing the traffic, and (c) it can help putting restriction based on geolocation. i.e. if the attack is coming from IPs from specific location, such requests can be blocked.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because (a) ELB helps distributing the traffic to the instances that are part of auto-scaling (helps absorbing the traffic), and (b) Amazon RDS is an Amazon managed service which can withstand the DDoS attack.</span></p><p><span style="font-size: 1rem;">Option E is CORRECT because CloudWatch can help monitoring the network traffic as well as CPU Utilization for suspicious activities.</span></p><p><span style="font-size: 1rem;">Option F is incorrect because adding and removing rules of firewall is not going to mitigate the DDoS attack.</span></p><p>It is very important to read the AWS Whitepaper on Best Practices for DDoS Resiliency.</p><p><a href="https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a><br></p><ul></ul><p></p>	x:Use dedicated instances to ensure that each instance has the maximum performance possible. <br>  専用インスタンスを使用して、各インスタンスが最大のパフォーマンスを発揮できるようにします。	o:Use an Amazon CloudFront distribution for both static and dynamic content. <br>  静的コンテンツと動的コンテンツの両方にAmazon CloudFrontディストリビューションを使用します。	o:Use an Elastic Load Balancer with auto scaling groups at the web, App. Restricting direct internet traffic to Amazon Relational Database Service (RDS) tiers. <br>  Web、アプリケーションで自動スケーリンググループを持つElastic Load Balancerを使用します。AmazonのRelational Database Service（RDS）層への直接インターネットトラフィックを制限します。	o:Add alert Amazon CloudWatch to look for high network in and CPU utilization. <br>  Amazon CloudWatchにアラートを追加して、ネットワークとCPU使用率の高いネットワークを探します。	x: Create processes and capabilities to quickly add and remove rules to the instance OS firewall. <br>  インスタンスOSファイアウォールにルールを迅速に追加および削除するためのプロセスと機能を作成します。
Test1-20. <p>A benefits enrollment company is hosting a 3-tier web application running in a VPC on AWS which includes a NAT (Network Address Translation) instance in the public Web tier. There is enough provisioned capacity for the expected workload for the new fiscal year benefit enrollment period plus some extra overhead. Enrollment proceeds nicely for two days and then the web tier becomes unresponsive, upon investigation using CloudWatch and other monitoring tools. It is discovered that there is an extremely large and unanticipated amount of inbound traffic coming from a set of 15 specific IP addresses over port 80 from a country where the benefits company has no customers. The web tier instances are so overloaded that benefit enrolment administrators cannot even SSH into them. Which activity would be useful in defending against this attack?</p> | <p>特典登録会社は、パブリックWeb層にNAT（Network Address Translation）インスタンスを含むAWSのVPCで動作する3層Webアプリケーションをホストしています。新しい会計年度の給付登録期間に予想されるワークロードに十分なプロビジョニングされた能力と、余分なオーバーヘッドがあります。CloudWatchやその他の監視ツールを使用した調査では、2日間の登録がうまくやってからWeb層が反応しなくなります。有益な会社に顧客がいない国からのポート80を介した15の特定のIPアドレスのセットから来るインバウンドトラフィックが非常に大きく予想外に多いことが判明しました。Web層インスタンスは非常にオーバーロードされているため、登録管理者はSSHを使用することはできません。	sa:	Create an inbound NACL (Network Access control list) associated with the web tier subnet with deny rules to block the attacking IP addresses <br>  攻撃者のIPアドレスをブロックする拒否ルールを持つWeb層サブネットに関連付けられた受信ネットワークアクセス制御リスト（NACL）を作成する|<p>In this scenario, the attack is coming from a set of certain IP addresses over specific port from a specific country. You are supposed to defend against this attack.&nbsp;</p><p><span style="font-size: 1rem;">In such questions, always think about two options: Security groups and Network Access Control List (NACL). Security Groups operate at the individual instance level, whereas NACL operates at subnet level. You should always fortify the NACL first, as it is encounter first during the communication with the instances in the VPC.</span></p><p><span style="font-size: 1rem;">Option A is incorrect because IP addresses cannot be blocked using route table or IGW.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because changing the EIP of NAT instance cannot block the incoming traffic from a particular IP address.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because (a) you cannot deny port access using security groups, and (b) by default all requests are denied; you open access for particular IP address or range. You cannot deny access for particular IP addresses using security groups.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because (a) you can add deny rules in NACL and block access to certain IP addresses. See an example below:</span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/1_20_1.png" alt="" width="762" height="326" role="presentation" class="img-responsive atto_image_button_middle"><br></span></p>	Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route Table with the new EIP. <br>  Web層サブネット内のNATインスタンスのEIP（Elastic IP Address）を変更し、新しいEIPでMain Route Tableを更新します。	Create 15 Security Group rules to block the attacking IP addresses over port 80 <br>  ポート80を介して攻撃するIPアドレスをブロックする15のセキュリティグループルールを作成する	Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (internet Gateway) <br>  Web層に関連付けられたカスタムルートテーブルを作成し、インターネットゲートウェイ（IGW）からの攻撃IPアドレスをブロックします。
Test1-21. <p>Your fortune 500 company has undertaken a TCO analysis evaluating the use of Amazon S3 versus acquiring more hardware. The outcome was that all employees would be granted access to use Amazon S3 for storage of their personal documents. Which of the following will you need to consider so you can set up a solution that incorporates single sign-on from your corporate AD or LDAP directory and restricts access for each user to a designated user folder in a bucket?</p> <p>Choose 3 options from the below</p> | <p>あなたのFortune 500企業は、Amazon S3の使用と、より多くのハードウェアの購入を評価するTCO分析を実施しました。その結果、すべての従業員に、Amazon S3を使用して個人用ドキュメントを保管するためのアクセス権が付与されました。企業のADディレクトリまたはLDAPディレクトリからのシングルサインオンを組み込んだソリューションを設定し、各ユーザーのアクセスをバケット内の指定されたユーザーフォルダに限定するソリューションを設定できるよう、次のうちどれを検討する必要がありますか？</p> <p >以下の3つのオプションを選択してください</p>	ma:	o:Setting up a federation proxy or identity provider <br>  フェデレーションプロキシまたはアイデンティティプロバイダを設定する|<p>In questions like this where an application, or user needs to be given access using Single Sign On (SSO), following steps are very important:</p><p>(i) setting up a identity provider for federated access</p><p>(ii) authenticating users using corporate data store / active directory-user-attributes/</p><p>(iii) getting temporary access tokens / credentials using AWS STS</p><p>(iv) creating the IAM Role that has the access to the needed AWS Resources</p><p><span style="font-size: 1rem;">Option A is CORRECT because as mentioned above, setting up a identity provider for federated access is needed.</span></p><p><span style="font-size: 1rem;">Option B is CORRECT because as mentioned above, getting temporary access tokens / credentials using AWS STS is needed.</span></p><p>Option C is incorrect because tagging each folder in bucket does not help in this scenario.</p><p><span style="font-size: 1rem;">Option D is CORRECT because as mentioned above, creating the IAM Role that has the access to the needed AWS Resources is needed.</span></p><p><span style="font-size: 1rem;">Option E is incorrect because you should be creating IAM Roles rather than IAM Users.&nbsp;</span></p><p><span style="font-size: 1rem;">The diagram below showcases how authentication is carried out when having an identity broker. This is an example of a SAML connection , but the same concept holds true for getting access to an AWS resource.</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_21_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="765" height="380"></p><p>For more information on federated access, please visit the below link:</p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p>	o:Using AWS Security Token Service to generate temporary tokens <br>  AWSセキュリティトークンサービスを使用して一時的なトークンを生成する	x:Tagging each folder in the bucket <br>  バケット内の各フォルダにタグを付ける	o:Configuring IAM role <br>  IAMロールの設定	x:Setting up a matching IAM user for every user in your corporate directory that needs access to a folder in the bucket <br>  社内ディレクトリ内のバケット内のフォルダにアクセスする必要があるすべてのユーザーに対応するIAMユーザーを設定する
Test1-22. <p>Your company policies require encryption of sensitive data at rest. You are considering the possible options for protecting data while storing it at rest on an EBS data volume, attached to an EC2 instance. Which of these options would allow you to encrypt your data at rest?</p> <p>Choose 3 options from the below<br></p> |  会社のポリシーでは、機密データの暗号化が必要です。 EC2インスタンスに接続されたEBSデータボリュームにデータを格納しながらデータを保護するためのオプションを検討しています。 あなたのデータを安心して暗号化できるこれらのオプションはどれですか？3つ選んでください。<br></p>	ma:	o:Implement third party volume encryption tools <br>  第三者のボリューム暗号化ツールを実装する|<p>You can encrypt the data at rest by either using a native data encryption, using a third party encrypting tool, or just encrypt the data before storing on the volume.</p><p><span style="font-size: 1rem;">Option A CORRECT because it uses third party volume encryption tool.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because EBS volumes are not encrypted by default.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT as it encypts the data before storing it on EBS.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT as it uses the native data encryption.</span></p><p><span style="font-size: 1rem;">Option E is incorrect as SSL/TLS is used for the security of the data in transit, not at rest.</span></p><p><br></p>	x:Do nothing as EBS volumes are encrypted by default <br>  デフォルトでEBSボリュームが暗号化されるため、何もしない	o:Encrypt data inside your applications before storing it on EBS <br>  EBSに保存する前に、アプリケーション内のデータを暗号化する	o:Encrypt data using native data encryption drivers at the file system level <br>  ネイティブデータ暗号化ドライバを使用してファイルシステムレベルでデータを暗号化する	x:Implement SSL/TLS for all services running on the server <br>  サーバー上で実行されているすべてのサービスに対してSSL / TLSを実装する
Test1-23. <p>You are migrating a legacy client-server application to AWS. The application responds to a specific DNS domain (e.g. www.example.com) and has a 2-tier architecture, with multiple application servers and a database server. Remote clients use TCP to connect to the application servers. The application servers need to know the IP address of the clients in order to function properly and are currently taking that information from the TCP socket. A decision is made to use multi-AZ RDS MySQL instance for the database. During the migration, you can change the application code but you have to file a change request.</p> <p>How would you implement the architecture on AWS In order to maximize scalability and high-ability?<br><br></p> | <p>従来のクライアント/サーバーアプリケーションをAWSに移行しています。 アプリケーションは特定のDNSドメイン（www.example.comなど）に応答し、複数のアプリケーションサーバーとデータベースサーバーを備えた2層アーキテクチャを備えています。 リモートクライアントはTCPを使用してアプリケーションサーバーに接続します。 アプリケーションサーバーは、正常に機能するためにクライアントのIPアドレスを知る必要があり、現在TCPソケットからその情報を取得しています。 データベースに複数のAZ RDS MySQLインスタンスを使用することが決定されました。 移行中にアプリケーションコードを変更できますが、変更要求を提出する必要があります。</p> <p> スケーラビリティと高性能を最大限に引き出すためにAWSでアーキテクチャをどのように実装しますか？ </p>	sa:	File a change request to implement Proxy Protocol Support. In the application use an ELB with a TCP Listener and Proxy Protocol enabled to distribute load on two application servers in different AZs. <br>  プロキシプロトコルサポートを実装するための変更要求を提出してください。 アプリケーションでは、TCPリスナーとプロキシプロトコルを有効にしたELBを使用して、異なるAZの2つのアプリケーションサーバーに負荷を分散します。|<p><br>AWS ELB has support for Proxy Protocol. It simply depends on a humanly readable header with the client's connection information to the TCP data sent to your server.&nbsp;As per the AWS documentation, the Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections. Because load balancers intercept traffic between clients and your instances, the access logs from your instance contain the IP address of the load balancer instead of the originating client. You can parse the first line of the request to retrieve your client's IP address and the port number.</p><p><span style="font-size: 1rem;">Option A is CORRECT because it implements the proxy protocol and uses ELB with TCP listener.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because, although implementing cross-zone load balancing provides high availability, it is not going to give the IP address of the clients. T</span><span style="font-size: 1rem;">he answer for B is still wrong because it states to use TCP forwarding, which does not support X-Forwarded-For.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because Route53 latency based routing does not give the IP address of the clients.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because Route53 Alias record does not give the IP address of the clients.</span></p><p>For more information on ELB enabling support for TCP, please refer to the links given below:<br><a href="https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/" target="_blank">https://aws.amazon.com/blogs/aws/elastic-load-balancing-adds-support-for-proxy-protocol/</a></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html" target="_blank">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html</a></p>	File a change request to Implement Cross-Zone support in the application Use an ELB with a TCP Listener and Cross-Zone Load Balancing enabled, two application servers in different AZs. <br>  アプリケーションでクロスゾーンサポートを実装するための変更要求を提出します。異なるリスナーの2つのアプリケーションサーバーであるTCPリスナーとクロスゾーンロードバランシングが有効なELBを使用します。	File a change request to implement Latency Based Routing support in the application. Use Route 53 with Latency Based Routing enabled to distribute load on two application servers in different AZs. <br>  アプリケーションに遅延ベースルーティングのサポートを実装するための変更要求を提出します。 遅延時間ベースのルーティングを有効にしたルート53を使用すると、異なるAZの2つのアプリケーションサーバーに負荷を分散できます。	File a change request to implement Alias Resource Support in the application, use Route 53 Alias Resource Record to distribute load on two application servers in different AZs. <br>  アプリケーションでエイリアスリソースサポートを実装するための変更要求を提出します。ルート53エイリアスリソースレコードを使用して、異なるAZの2つのアプリケーションサーバーに負荷を分散します。
Test1-24. <p>You have a periodic Image analysis application that gets some files. The input stream analyzes them and for each file, it writes some data to an output stream to a number of files. The number of files in input per day is high and concentrated in a few hours of the day. Currently, you have a server on EC2 with a large EBS volume that hosts the input data and the results it takes almost 20 hours per day to complete the process</p> <p>What services could be used to reduce the elaboration time and improve the availability of the solution?<br><br></p> | <p>ファイルを取得する定期的な画像解析アプリケーションがあります。入力ストリームはそれらを解析し、ファイルごとにいくつかのデータを出力ストリームにいくつかのファイルに書き込みます。1日あたりの入力ファイル数は高く、1日の数時間で集中します。現在、入力データをホストする大きなEBSボリュームを持つEC2上のサーバーと、処理を完了するために1日約20時間かかる結果があります。</p> <p>どのサービスを使用して詳細な時間を短縮し、ソリューションの可用性を向上させることができますか？<br> <br> </p>	sa:	Use S3 to store I/O files. The use SQS to distribute elaboration commands to a group of hosts working in parallel. Then use Auto scaling to dynamically size the group of hosts depending on the length of the SQS queue. <br>  S3を使用してI / Oファイルを格納します。 SQSを使用して、並行して動作するホストのグループに精巧化コマンドを配布します。 自動スケーリングを使用して、SQSキューの長さに応じてホストのグループのサイズを動的に調整します。|<p></p><p>The scenario in this question is that (a) there any EC2 instances that need to process high number of input files, (b) currently the processing takes 20 hrs a day, which needs to be reduced, (c) the availability needs to be improved.</p><p><span style="font-size: 1rem;">Looking at all the option, it appears that there are two choices to be made. (1) between S3 and EBO with PIOPS, and (2) between SQS and SNS.</span></p><p><span style="font-size: 1rem;">First, let's see whether we should choose S3 or EBS with PIOPS. It appears that all the options have auto-scaling in common. i.e. there will be multiple EC2 instances working in parallel on the input data.&nbsp; This should reduce the overall elaboration time, satisfying one of the requirements. Since a single EBS volume cannot be attached to multiple instances, using EBS volume seems an illogical choice. Moreover, S3 provides high availability, which satisfies the other requirement.&nbsp;</span></p><p><span style="font-size: 1rem;">Second, SQS is a great option to do the autonomous tasks and can queue the service requests and can be scaled to meet the high demand. SNS is a mere notification service and would not hold the tasks. Hence, SQS is certainly the correct choice.&nbsp;</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is CORRECT because, as mentioned above, it provides high availability, and can store the massive amount of data. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps distributing the commands/tasks to the group of EC2 instances.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because, as mentioned above, neither EBS nor SNS is a valid choice in this scenario.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because, as mentioned above, SNS is not a valid choice in this scenario.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because, as mentioned above, EBS is not a valid choice in this scenario.</span></p><p><span style="font-size: 1rem;"><br></span></p>	Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SNS to distribute elaboration commands to a group of hosts working in parallel and Auto Scaling to dynamically size the group of hosts depending on the number of SNS notifications. <br>  I / Oファイルを格納するには、プロビジョニングIOPS（PIOPS）でEBSを使用します。 SNSを使用して、並行して動作するホストのグループに精巧化コマンドを配布し、自動スケーリングを使用して、SNS通知の数に応じてホストのグループを動的にサイズ変更します。	Use S3 to store I/O files, SNS to distribute elaboration commands to a group of hosts working in parallel. Auto scaling to dynamically size the group of hosts depending on the number of SNS notifications. <br>  S3を使用してI / Oファイル、SNSを格納して、並行して動作するホストのグループに精緻化コマンドを配布します。SNS通知の数に応じてホストのグループのサイズを動的に変更する自動スケーリング	Use EBS with Provisioned IOPS (PIOPS) to store I/O files. Use SQS to distribute elaboration commands to a group of hosts working in parallel. Use Auto Scaling to dynamically size the group of hosts depending on the length of the SQS queue. <br>  I / Oファイルを格納するには、プロビジョニングされたIOPS（PIOPS）でEQを使用します。SQSを使用して、並行して動作するホストのグループにエラボレーションコマンドを配布します。自動スケーリングを使用して、SQSキューの長さに応じてホストのグループのサイズを動的に調整します。
Test1-25. <p>You require the ability to analyze a customer’s clickstream data on a website so they can do the behavioral analysis. Your customer needs to know what sequence of pages and ads their customer clicked on. This data will be used in real time to modify the page layouts as customers click through the site to increase stickiness and advertising click-through. Which option meets the requirements for captioning and analyzing this data?</p> | <p>行動分析を行うために、ウェブサイト上の顧客のクリックストリームデータを分析する機能が必要です。顧客がクリックしたページと広告の順序を知る必要があります。このデータは、顧客がサイトをクリックして粘着性および広告のクリックスルーを増やすように、ページレイアウトを変更するためにリアルタイムで使用されます。このデータのキャプションと分析の要件を満たすオプションはどれですか？</p>	sa:	Push web clicks by session to Amazon Kinesis and analyze behavior using Kinesis workers <br>  Amazon KinesisにセッションごとにWebクリックをプッシュし、Kinesisワーカーを使用して動作を分析する|<p>Whenever the question presents a scenario where the application needs to do analysis on real time data such as clickstream (i.e.massive real-time data analysis), most of the time the best option is Amazon Kinesis. It is used to collect and process large&nbsp;<a href="https://aws.amazon.com/streaming-data/" target="_blank">streams</a>&nbsp;of data records in real time.</p><p>You'll create data-processing applications, known as&nbsp;<em>Amazon Kinesis Streams applications</em>. A typical Amazon Kinesis Streams application reads data from an&nbsp;<em>Amazon Kinesis stream</em>&nbsp;as data records. These applications can use the Amazon Kinesis Client Library, and they can run on Amazon EC2 instances. The processed records can be sent to dashboards, used to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services</p><p>The below diagrams from the aws documentation shows how you can create custom streams in Amazon Kinesis.</p><p><img src="https://s3.amazonaws.com/awssap/1_25_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="1268" height="299"><br></p><p><img src="https://s3.amazonaws.com/awssap/1_25_2.png" alt="" width="1102" height="474" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p>For more information on Kinesis, please visit the below link:</p><p><a href="http://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">http://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p>	Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce <br>  WeblogsのURLをURLでログに記録してAmazon S3に保存し、Elastic MapReduceで分析する	Write click events directly to Amazon Redshift and then analyze with SQL <br>  クリックイベントをAmazon Redshiftに直接書き込んでからSQLで分析する	Publish web clicks by session to an Amazon SQS queue and periodically drain these events to Amazon RDS and analyze with sql. <br>  セッションごとのWebクリックをAmazon SQSキューに公開し、これらのイベントをAmazon RDSに定期的に流し込み、sqlで分析します。
Test1-26. <p>An AWS customer runs a public blogging website. The site users upload two million blog entries a month. The average blog entry size is 200 KB. The access rate to blog entries drops to negligible 6 months after publication and users rarely access a blog entry 1 year after publication. Additionally, blog entries have a high update rate during the first 3 months following publication, this drops to no updates after 6 months. The customer wants to use CloudFront to improve his user’s load times. Which of the following recommendations would you make to the customer?</p> | <p> AWSのお客様が公開ブログのウェブサイトを運営しています。サイトユーザーは月に200万件のブログエントリをアップロードします。ブログエントリの平均サイズは200 KBです。ブログエントリへのアクセス率は、出版後6ヶ月で無視され、ユーザは出版後1年でブログエントリにアクセスすることはほとんどありません。さらに、ブログのエントリーは、出版後最初の3ヶ月間に更新率が高いため、6ヶ月後には更新されません。顧客は、CloudFrontを使用してユーザーの読み込み時間を改善したいと考えています。次のうち推奨するものはどれですか？</p>	sa:	Create a CloudFront distribution with S3 access restricted only to the CloudFront identity and partition the blog entry’s location in S3 according to the month it was uploaded to be used with CloudFront behaviors. <br>  S3アクセスがCloudFront IDのみに制限されたCloudFrontディストリビューションを作成し、S3でCloudFrontの動作に使用するためにアップロードされた月に従ってブログエントリの場所を分割します。|<p>The scenario here is that (a) blogs have high access/updates rate in the first 3 months of their creation, (b) this rate drops after 6 months. The main architectural consideration is that the user's load time of the blog needs to be improved.&nbsp;</p><p><span style="font-size: 1rem;">This question is based on making the best use of CloudFront's Cache Behavior. You need to understand two things about CloudFront for such scenario: (1) CloudFront is a service that is designed to give geographically distributed users the fast access to the content by maintaining the content in the cache that is maintained at multiple edge locations, and (2) using the cache-behavior of CloudFront, you can control the origin and path of the content, time to live (TTL), and control the user access using trusted signers.</span></p><p><span style="font-size: 1rem;">In this scenario, you need to control the content based on the time period at which the blog is published. i.e. when a blog is published, you need to cache the update for first 3 months, so that it can be quickly accessed by the users, and after six months from the update, the content can be removed from the cache, as it is rarely accessed. Also, you need to make sure that the content is only accessed by the CloudFront.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because maintaining two separate buckets is not going to improve the load time for the users.</span></p><p><span style="font-size: 1rem;">Option B is incorrect as the location-wise distribution is not going to improve the load time for the users.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because it (a) the content is only accessed by CloudFront, and (b) if the content is partitioned at the origin based on the month it was uploaded, you can control the cache behavior accordingly, and keep only the latest updated content in the CloudFront cache, so that it can be accessed with fast load-time; hence, improving the performance.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because, setting minimum TTL of 0 will enforce loading of the content from origin every time, even if it has not been updated over 6 months.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on Cloudfront identity, please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a><br><p></p>	Create a CloudFront distribution with “US’Europe price class for US/Europe users and a different CloudFront distribution with All Edge Locations’ for the remaining users. <br>  米国/ヨーロッパユーザーの場合は「米国」のヨーロッパ価格クラスを使用し、残りのユーザーの場合は「すべてのエッジロケーションを持つCloudFrontディストリビューション」を使用してCloudFrontディストリビューションを作成します。	Duplicate entries into two different buckets and create two separate CloudFront distributions where S3 access is restricted only to Cloud Front identity <br>  2つの異なるバケットにエントリを複製し、2つの別々のCloudFrontディストリビューションを作成します。ここでは、S3アクセスはクラウドフロントID	Create a CloudFront distribution with Restrict Viewer Access Forward Query string set to true and minimum TTL of 0. <br>  Restrict Viewer Access Forward Query文字列をtrueに設定し、最小TTLを0に設定してCloudFrontディストリビューションを作成します。
Test1-27. <p>Your company is getting ready to do a major public announcement of a social media site on AWS. The website is running on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL Extra Large DB Instance. The site performs a high number of small reads and writes per second and relies on an eventual consistency model. After comprehensive tests, you discover that there is read contention on RDS MySQL. Which are the best approaches to meet these requirements?</p> <p>Choose 2 options from the below:</p> | <p>あなたの会社は、AWS上でソーシャルメディアサイトの主要な公表を行う準備が整いました。このWebサイトは、複数AZ RDSのMySQL Extra Large DBインスタンスを使用して複数の可用性ゾーンに展開されたEC2インスタンス上で実行されています。このサイトは、毎秒の読み取りと書き込みの回数が非常に多く、最終的な一貫性モデルに依存しています。包括的なテストが終わったら、RDS MySQLに関する読解競合があることがわかります。これらの要件を満たすための最良のアプローチはどれですか？</p> <p>以下の2つのオプションを選択してください：</p>	ma:	o:Deploy ElasticCache in-memory cache running in each availability zone <br>  各可用性ゾーンで実行中のElastic Cacheインメモリキャッシュを展開する|<p>The main point to note in this question is that there is a read contention on RDS MySQL. Your should be looking for the options which will improve upon the "read" contention issues. Hint: Always see if any of the options contain (1) caching solution such as ElastiCache, (2) CloudFront, or (3) Read Replicas.&nbsp;</p><p>Option A is CORRECT because ElastiCache is a in-memory caching solution which reduces the load on the database and improves the read performance.</p><p><span style="font-size: 1rem;">Option B is incorrect because sharding does not improve read performance; however, it improves write performance, but write contention is not the issue here.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because improving the instance size may improve the read performance, but only up to a specific limit. It is not a reliable solution.</span></p><p><span style="font-size: 1rem;">Option D is CORRECT because Read Replicas are used to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Hence, improving the read performance.</span></p><p><span style="font-size: 1rem;">See more information on Read Replicas and ElastiCache below.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><b>Read Replicas</b></p><p>Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.</p><p>For more information on Read Replica’s, please visit the below link:</p><p></p><a href="https://aws.amazon.com/rds/details/read-replicas/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/rds/details/read-replicas/</a><br><br><p></p><p><b>ElastiCache</b></p><p>Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in&nbsp;the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.&nbsp;</p><p>For more information on Amazon ElastiCache, please visit the below link:</p><p></p><a href="https://aws.amazon.com/elasticache/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/elasticache/</a><br><br><p></p>	x:Implement sharding to distribute load to multiple RDS MySQL instances <br>  複数のRDS MySQLインスタンスに負荷を分散するシャーディングを実装する	x:Increase the RDS MySQL Instance size and implement provisioned IOPS <br>  RDSのMySQLインスタンスサイズを拡大し、実装されたIOPSを実装する	o:Add an RDS MySQL read replica in each availability zone <br>  各可用性ゾーンにRDS MySQL読み取りレプリカを追加する
Test1-28. <p>A company is running a batch analysis every hour on their main transactional DB running on an RDS MySQL instance to populate their central Data Warehouse running on Redshift. During the execution of the batch their transactional applications are very slow. When the batch completes they need to update the top management dashboard with the new data. The dashboard is produced by another system running on-premises that is currently started when a manually-sent email notifies that an update is required. The on-premises system cannot be modified because is managed by another team.</p> <p>How would you optimize this scenario to solve performance issues and automate the process as much as possible?</p> |  <p>ある会社は、RDS MySQLインスタンス上で稼動するメインのトランザクションDBで1時間ごとにバッチ分析を実行して、Redshiftで稼働する中央データウェアハウスにデータを移入します。 バッチの実行中、トランザクションアプリケーションは非常に遅いです。 バッチが完了すると、トップマネジメントダッシュボードを新しいデータで更新する必要があります。 ダッシュボードは、手動で送信された電子メールで更新が必要であることが通知されたときに現在開始されているオンプレミスを実行している別のシステムによって生成されます。 </ p> <p>このシナリオを最適化してパフォーマンスの問題を解決し、できるだけプロセスを自動化するにはどうすればよいでしょうか。</p>	sa:	Create a RDS Read Replica for the batch analysis and SNS to notify the on-premises system to update the dashboard. <br>  バッチ分析用にRDS読み取りレプリカを作成し、オンプレミスシステムにダッシュボードを更新するよう通知するSNSを作成します。|<p><br></p><p>There are two architectural considerations here. (1) you need to improve read performance by reducing the load on the RDS MySQL instance, and (2) automate the process of notifying to the on-premise system.</p><p><span style="font-size: 1rem;">When the scenario asks you to improve the read performance of a DB instance, always look for options such as ElastiCache or Read Replicas. And when the question asks you to automate the notification process, always think of using SNS.&nbsp;</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because Redshift is used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a solution.&nbsp;</span></p><p><span style="font-size: 1rem;">Option B is incorrect because Redshift is used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a solution.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because (a) it uses Read Replicas which improves the read performance, and (b) it uses SNS which automates the process of notifying the on-premise system to update the dashboard.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because SQS is not a service to be used for sending the notification.</span></p><p><br></p><p>For more information on Read Replica’s, please visit the below link</p><p></p><a href="https://aws.amazon.com/rds/details/read-replicas/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/rds/details/read-replicas/</a><br><br><p></p>	Replace RDS with Redshift for the batch analysis and SQS to send a message to the on-premises system to update the dashboard. <br>  バッチ分析でRDSをRedshiftに置き換えて、SQSを使用してオンプレミスシステムにメッセージを送信してダッシュボードを更新します。	Replace RDS with Redshift for the batch analysis and SNS to notify the on-premises system to update the dashboard. <br>  バッチ分析でRDSをRedshiftに置き換えて、SNSを使用してオンプレミスシステムにダッシュボードを更新するように通知します。	Create a RDS Read Replica for the batch analysis and SQS to send a message to the on-premises system to update the dashboard. <br>  バッチ分析用にRDS読み取りレプリカを作成し、オンプレミスシステムにメッセージを送信してダッシュボードを更新するSQSを作成します。
Test1-29. <p>You are implementing a URL whitelisting system for a company that wants to restrict outbound HTTPS connections to specific domains from their EC2-hosted applications. You deploy a single EC2 instance running proxy software and configure it to accept traffic from all subnets and EC2 instances in the VPC. You configure the proxy to only pass through traffic to domains that you define in its whitelist configuration.</p> <p>You have a nightly maintenance window or 10 minutes where all instances fetch new software updates. Each update is about 200MB in size and there are 500 instances In the VPC that routinely fetch updates. After a few days you notice that some machines are failing to successfully download some, but not all of their updates within the maintenance window. The download URLs used for these updates are correctly listed in the proxy’s whitelist configuration and you are able to access them manually using a web browser on the instances. What might be happening?</p> <p>Choose 2 answers form the options below:</p> | <p> EC2ホストアプリケーションから特定のドメインへのアウトバウンドHTTPS接続を制限したい会社のURLホワイトリストシステムを実装しています。プロキシソフトウェアを実行する単一のEC2インスタンスを展開し、VPC内のすべてのサブネットおよびEC2インスタンスからのトラフィックを受け入れるように設定します。</p> <p>夜間のメンテナンスウィンドウまたはすべてのインスタンスが新しいソフトウェアアップデートを取得する10分のメンテナンスウィンドウがあります。各アップデートのサイズは約200MBで、アップデートを定期的に取得するVPCでは500インスタンスあります。数日後、いくつかのマシンがメンテナンスウィンドウ内のアップデートの一部ではなく、すべてのアップデートを正常にダウンロードできていないことに気付きました。これらのアップデートに使用されるダウンロードURLは、プロキシのホワイトリスト設定に正しくリストされており、インスタンス上のWebブラウザを使用して手動でアクセスできます。</p> <p>以下のオプションから2つの回答を選択してください：</p>	ma:	o:You are running the proxy on an undersized EC2 instance type so network throughput is not sufficient for all instances to download their updates in time. <br>  小さすぎるEC 2インスタンスタイプでプロキシを実行しているため、ネットワークスループットだけではすべてのインスタンスがアップデートをダウンロードできません。|<p>This scenario contains following main points: (1) there is a single EC2 instance running proxy software that either itself acts as or connects to a NAT instance. The NAT instances are not AWS managed, they are user managed; so, it may become the bottleneck, (2) there is a whitelist maintained so that limited outside access is given to the instances inside VPC, (3) the URLs in the whitelist are correctly maintained, so whitelist is not an issue, (4) only some machines are having download problems with some updates. i.e. some updates are successful on some machines.</p><p><span style="font-size: 1rem;">This indicates that there is no setup issue, but most-likely it is the proxy instance that is a bottleneck and under-performing or inconsistently performing. As the proxy instance is not part of any auto-scaling group, it's size must be definitely the issue.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is CORRECT because due to limited size of proxy instance, it's network throughput might not be sufficient to provide service to all the VPC instances (as only some of the instances are not able to download the updates).</span></p><p>Option B is incorrect because limited storage on the proxy instance should not cause other instances any problems in downloading the updates.</p><p>Option C is incorrect because proxy instances are supposed to be in public subnet, but allocation of EIPs should not cause any issues for other instances in the VPC.</p><p>Option D is CORRECT because undersized NAT instance can be a bottleneck and can cause other instances suffer from insufficient network throughput.</p><p>Option E is incorrect because if this was the case, none of the instances would get the updates. However, some of the instances were able to get the updates, so, this cannot be the case.&nbsp;</p><p></p>	x:You have not allocated enough storage to the EC2 instance running the proxy so the network buffer is filling up causing some requests to fail. <br>  プロキシを実行しているEC 2インスタンスに十分なストレージを割り当てていないため、ネットワークバッファがいっぱいになり、一部の要求が失敗します。	x:You are running the proxy in a public subnet but have not allocated enough EIP’s to support the needed network throughput through the Internet Gateway (IGW). <br>  パブリックサブネットでプロキシを実行していますが、インターネットゲートウェイ（IGW）を介して必要なネットワークスループットをサポートするのに十分なEIPを割り当てていません。	o:You are running the proxy on a appropriate-size EC2 instance in a private subnet and its network throughput is being throttled by a NAT instance running on a t2.micro? EC2 instance. <br>  プライベートサブネット内の適切なサイズのEC2インスタンスでプロキシを実行していて、そのネットワークスループットがt2.micro EC2インスタンス上で実行されているNATインスタンスによって抑制されています。	x:The route table for the subnets containing the affected EC2 instances is not configured to direct network traffic for the software update locations to the proxy. <br>  影響を受けるEC 2インスタンスを含むサブネットのルートテーブルは、ソフトウェア更新場所のネットワークトラフィックをプロキシに転送するように構成されていません。
Test1-30. <p>To serve Web traffic for a popular product your chief financial officer and IT director have purchased 10 large heavy utilization Reserved Instances (RIs) evenly spread across two availability zones. Route 53 is used to deliver the traffic to an Elastic Load Balancer (ELB). After several months, the product grows even more popular and you need additional capacity. As a result, your company purchases two c5.2xlarge medium utilization RI. You register the two c5.2xlarge instances with your ELB and quickly find that the large instances are at 100% of capacity and the c5.2xlarge instances have a significant capacity that’s unused. Which option is the most cost-effective and uses EC2 capacity most effectively?</p> | <p>一般的な製品のWebトラフィックを処理するために、最高財務責任者とITディレクターは、2つの可用性ゾーンに均等に分散された10個の大規模利用率リザーブドインスタンス（RI）を購入しました。ルート53は、Elastic Load Balancer（ELB）にトラフィックを配信するために使用されます。数ヶ月後には、製品がさらに普及し、追加容量が必要になります。その結果、あなたの会社は2つのc5.2xlargeメディア利用率RIを購入します。2つのc5.2xlargeインスタンスをELBに登録し、大きなインスタンスの容量が100％であり、c5.2xlargeインスタンスの容量が未使用であることをすぐに確認します。どのオプションが最も費用対効果が高く、EC2のキャパシティを最も効果的に使用していますか？</p>	sa:	Use a separate ELB for each instance type and distribute load to ELBs with Route 53 weighted round robin. <br>  各インスタンスタイプに対して別々のELBを使用し、Route 53加重ラウンドロビンを使用してELBに負荷を分散します。|<p>In this question, the problem is that the newly added c5.2xlarge instances are not fully utilized. This is happening because the load is spread evenly across all the instances. There is no logic on how much traffic is to be routed to which instance types.</p><p><span style="font-size: 1rem;">Hence, there is need to add some logic where higher (more-weighted) traffic should be routed to c5.2xlarge instances and light-weighted to the other instances. Route 53's weighted routing policy does exactly this, so you should look for this option.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is CORRECT because it first creates separate ELBs, one each for set of different instance type and uses Route 53's weighted routing policy such that higher proportion of the load is routed to the ELB that has c5.2xlarge instances and smaller proportion to the one with smaller instances.</span></p><p>Option B is incorrect because shutting down c5.2xlarge instances will not be an effective use of the EC2 capacity. You have already paid for the instance. So you would lose money here.</p><p>Option C is incorrect because latency based routing may not always distribute heavy traffic to the large instance. You must use weighted routing policy.</p><p>Option D is incorrect because this option is not a good use of the existing capacity, and in fact, would add to the cost.</p><p><br></p><p>For more information on Route 53 weighted routing policy, please visit the URL below:</p><p><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted" target="_blank">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a><br></p>	Configure AutoScaling group and Launch Configuration with ELB to add up to 10 more on-demand mi large instances when triggered by Cloudwatch shut off c5.2xiarge instances. <br>  AutoScalingグループとLaunch ConfigurationをELBで設定し、Cloudwatchがc5.2xlargeインスタンスをシャットダウンしたときに最大10個のオンデマンドマイラージインスタンスを追加できます。	Route traffic to EC2 large and c5.2xlarge instances directly using Route 53 latency based routing and ?health checks shut off ELB. <br>  Route 53レイテンシベースルーティングを使用してEC2 largeおよびc5.2xlargeインスタンスにトラフィックを直接ルーティングし、ヘルスチェックはELBをシャットダウンします。	Configure ELB with two c5.2xlarge Instances and use on-demand AutoScaling group for up to two additional c5.2xlarge instances. <br>  2つのc5.2xlargeインスタンスでELBを構成し、最大2つの追加のc5.2xlargeインスタンスに対してオンデマンド自動スケーリンググループを使用します。
Test1-31. <p>A read-only news reporting site with a combined web and application tier and a database tier that receives large and unpredictable traffic demands must be able to respond to these traffic fluctuations automatically. Which AWS services should be used meet these requirements?</p> | <p>大量の予測不可能なトラフィック需要を受け取る、統合されたWebアプリケーション層とデータベース層を持つ読み取り専用のニュースレポートサイトでは、これらのトラフィックの変動に自動的に対応できる必要があります。これらの要件を満たすAWSサービスはどれですか？</p>	sa:	Stateless instances for the web and application tier that are in an auto scaling group, synchronized using Elasticache Memcached and monitored with CloudWatch. RDS configured with read replicas for the backend. <br>  自動スケーリンググループ内にあり、Elasticache Memcachedを使用して同期され、CloudWatchで監視され、バックエンド用の読み取りレプリカで構成されたRDSを用いた、Web層とアプリケーション層のステートレスインスタンス。|<p>The scenario asks for 2 things: (1) a performance improving solution for read heavy web tier and database tier. Hint: Always see if any of the options contain caching solution such as ElastiCache, CloudFront, or Read Replicas, and (2) whether to use stateless or stateful instances.</p><p><span style="font-size: 1rem;">Stateful instances are not suitable for distributed systems, <span>as they retain the state or connection</span>&nbsp;between client and web server, database remains engaged as long as the session is active. Hence, it increases the load on the server as well as database. Stateless instances, however are distributed and easy to scale in/scale out. Hence, the stateless application tend to improve the performance of a distributed application.</span></p><p><br></p><p>Option A is CORRECT because (a) it uses stateless instances, (b) the web server uses ElastiCache for read operations, (c) it uses CloudWatch which monitors the fluctuations in the traffic and notifies to auto-scaling group to scale in/scale out accordingly, and (d) it uses read replicas for RDS to handle the read heavy workload.</p><p>Option B is incorrect because (a) it uses stateful instances, and (b) it does not use any caching mechanism for web and application tier.</p><p>Option C is incorrect because (a) it uses stateful instances, (b) it does not use any caching mechanism for web and application tier, and (c) multi-AZ RDS does not improve read performance.</p><p>Option D is incorrect because multi-AZ RDS does not improve read performance.</p><p><br></p><p>For more information on ElastiCache and Read Replicas, please refer to the following links:</p><p><a href="https://aws.amazon.com/elasticache/" target="_blank">https://aws.amazon.com/elasticache/</a></p><p><a href="https://aws.amazon.com/rds/details/read-replicas/" target="_blank">https://aws.amazon.com/rds/details/read-replicas/</a><br></p>	Stateful instances for the web and application tier in an auto scaling group monitored with CloudWatch, and RDS with read replicas. <br>  読み取りレプリカを持つRDSを用いた、CloudWatchで監視される自動スケーリンググループ内のWebおよびアプリケーション層のステートフルインスタンス。	Stateful instances for the web and application tier in an auto scaling group and monitored with CloudWatch, and a multi-AZ RDS. <br>  自動スケーリンググループ内のWebおよびアプリケーション層のステートフルインスタンスで、CloudWatchおよび複数AZ RDSで監視されます。	Stateless instances for the web and application tier in an auto scaling group that are synchronized using ElastiCache Memcached, and monitored with CloudWatch, and a multi-AZ RDS. <br>  ElastiCache Memcachedを使用して同期され、マルチAZ RDSを利用し、CloudWatchで監視される自動スケーリンググループ内のWeb層およびアプリケーション層のステートレスインスタンス
Test1-32. <p>You are running a news website in the EU-west-1 region that updates every 15 minutes. The website has a worldwide audience. It uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS database. Static content resides on Amazon S3 and is distributed through Amazon CloudFront. Your Auto Scaling group is set to trigger a scale up event at 60% CPU utilization, you use an Amazon RDS extra large DB instance with 10,000 Provisioned IOPS. Its CPU utilization is around 80%. While freeable memory is in the 2 GB range. Web analytics reports show that the average load time of your web pages is around 1.5 to 2 seconds but your SEO consultant wants to bring down the average load time to under 0.5 seconds. How would you improve page load times for your users? Choose 3 options from the below</p> | <p> 15分ごとに更新されるEU-west-1地域のニュースウェブサイトを運営しています。このウェブサイトには世界中の視聴者がいます。Elastic Load BalancerとAmazon RDSデータベースの背後にあるAuto Scalingグループを使用します。静的コンテンツはAmazon S3にあり、Amazon CloudFrontを通じて配布されます。自動スケーリンググループは、60％のCPU使用率でスケールアップイベントを発生させるように設定されています.AMDS RDSの超大容量DBインスタンスに10,000のプロビジョニングIOPSを使用します。そのCPU使用率は約80％です。空きメモリは2 GBの範囲です。ウェブ解析レポートでは、ウェブページの平均ロード時間は約1.5〜2秒ですが、SEOコンサルタントは平均ロード時間を0.5秒未満に短縮したいと考えています。ユーザーのページ読み込み時間をどのように改善しますか？以下の3つのオプションを選択してください</p>	ma:	x:Lower the scale up trigger of your Auto Scaling group to 30% so it scales more aggressively. <br>  Auto Scalingグループのスケールアップトリガーを30％に下げ、より積極的にスケールを調整します。|<p>In this scenario, there are major points of consideration: (1) news website updtes every 15 minutes, (2) current average load time is high, and (3) the performance of the use of the website should be improved (i.e. read performance needs improvement). When the questions asks for performance improving solution for read heavy application, always see if any of the options contain caching solution such as ElastiCache, CloudFront, or Read Replicas.</p><p><br></p><p>Option A is incorrect because it will increase the number of web server instances, but will not reduce the load on the database; hence, will not improve the read performance.</p><p>Option B is CORRECT because it uses ElastiCache for storing sessions as well as frequent DB queries; hence reducing the load on the database. This should help increasing the read performance.</p><p>Option C is CORRECT because it uses CloudFront which is a network of globally distributed "edge-locations" that caches the content and improves the user experience.</p><p>Option D is CORRECT because scaling up the RDS instance helps improving its read and write performance.</p><p>Option E is incorrect because it would not improve read performance; in fact, this setup would add to the cost.</p><p><br></p><p>For more information on Elastic Cache, please visit the below URL</p><p><a href="https://aws.amazon.com/elasticache/" target="_blank">https://aws.amazon.com/elasticache/</a></p><p>Dynamic content management via CloudFront can also help alleviate some of the load from the actual web servers.</p><p>For more information on Cloudfront Dynamic content, please visit the below URL</p><p><a href="https://aws.amazon.com/cloudfront/dynamic-content/" target="_blank">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p>And finally, since the RDS is at 80% usage, having large instances with better I/O capability can help.</p>	o:Add an Amazon ElastiCache caching layer to your application for storing sessions and frequent DB queries <br>  ストレージセッションと頻繁なDBクエリーのためにAmazon ElastiCacheキャッシングレイヤーをアプリケーションに追加する	o:Configure Amazon CloudFront dynamic content support to enable caching of re-usable content from your site <br>  サイトから再利用可能なコンテンツをキャッシュできるようにAmazon CloudFrontの動的コンテンツサポートを設定する	o:Switch Amazon RDS database to the high memory extra-large Instance type <br>  Amazon RDSデータベースを大容量の超大規模インスタンスに切り替えます。	x:Set up a second installation in another region, and use the Amazon Route 53 latency-based routing feature to select the right region. <br>  別の地域に2番目の設置をセットアップし、Route 53のレイテンシーに基づくルーティング機能を使用して適切な地域を選択します。
Test1-33. <p>A large real-estate brokerage is exploring the option of adding a cost-effective location-based alert to their existing mobile application. The application backend infrastructure currently runs on AWS. Users who opt into this service will receive alerts on their mobile device regarding real-estate offers in proximity to their location. For the alerts to be relevant, delivery time needs to be in the low minute count. The existing mobile app has 5 million users across the US. Which one of the following architectural suggestions would you make to the customer?</p> | <p>大規模な不動産仲介業者は、既存のモバイルアプリケーションに費用効果の高いロケーションベースのアラートを追加するオプションを検討しています。アプリケーションのバックエンドインフラストラクチャは現在、AWS上で実行されています。このサービスを利用するユーザーは、自宅の近くにある不動産の提供に関するモバイルデバイスのアラートを受信します。アラートが適切であるためには、配信時間が低い分カウントである必要があります。既存のモバイルアプリには、米国全土で500万人のユーザーがいます。次のアーキテクチャ上の提案のどれをお客様に提供しますか？</p>	sa:	The mobile application will send device location using SQS. EC2 instances will retrieve the relevant offers from DynamoDB. AWS Mobile Push will be used to send offers to the mobile application <br>  モバイルアプリケーションは、SQSを使用してデバイスの場所を送信します。EC2インスタンスは、DynamoDBから関連する機能を取得します。AWSモバイルプッシュを使用してモバイルアプリケーションにオファーを送信する|<p>The scenario has following architectural considerations: (1) the users should get notifications about the real estate in the area near to their location, (2) only subscribed users should get the notification, (3) the notification delivery should be fast, (4) the architecture should be scalable, and (5) it should be cost effective.</p><p><span style="font-size: 1rem;">When the question has considerations for scalability, always think about DynamoDB as it is the most recommended database solution to handle huge amount of data/records. For automated notifications, always think about SNS.&nbsp;</span></p><p><br></p><p>Option A is incorrect because (a) setting up EC2 instances and ELB to handle 5 millions users will not be cost effective, and (b) sending the notifications via mobile earners/device providers as alerts is neither feasible nor cost effective (certainly not as cost effective as SNS).</p><p>Option B is incorrect because (a) setting up EC2 instances and ELB to handle 5 millions users will not be cost effective, (b) receiving location via Direct Connect and carrier connection is not cost effective, also it does not deal with subscriptions, and (c)&nbsp; sending the notifications via mobile carriers as alerts is not cost effective (certainly not as cost effective as SNS).</p><p>Option C is CORRECT because (a) SQS is a highly scalable, cost effective solution for carrying out utility tasks such as holding the location of millions of users, (b) it uses highly scalable DynamoDB, and (c) it uses the cost effective AWS SNS Mobile Push service to send push notification messages directly to apps on mobile devices.</p><p>Option D is incorrect because AWS SNS Mobile Push service to used for sending push notification messages to the mobile devices, not to get the location of the mobile devices.</p><p><br></p><p>For more information on AWS SNS Mobile Push, please see the diagram and link given below:</p><p><img src="https://s3.amazonaws.com/awssap/1_33_1.png" alt="" width="609" height="220" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><br></p><p><a href="https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html" target="_blank">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a><br></p>	Use AWS DirectConnect or VPN to establish connectivity with mobile carriers EC2 instances will receive the mobile applications ‘location through carrier connection. RDS will be used to store and relevant offers. EC2 instances will communicate with mobile carriers to push alerts back to the mobile application <br>  EC2インスタンスは、モバイル・キャリアとモバイル・アプリケーションとの間で通信する.EC2インスタンスは、キャリア接続を介してモバイル・アプリケーションのロケーションとの通信を確立する。	The mobile application will submit its location to a web service endpoint utilizing Elastic Load Balancing and EC2 instances. DynamoDB will be used to store and retrieve relevant offers from EC2 instances which will then communicate with mobile earners/device providers to push alerts back to mobile application. <br>  DynamoDBを使用して、EC 2インスタンスから関連アイテムを取得し、取得し、モバイル事業者/デバイスプロバイダと通信してアラートをモバイルアプリケーションにプッシュバックします。	The mobile application will send device location using AWS Mobile Push. EC2 instances will retrieve the relevant offers from DynamoDB. EC2 instances will communicate with mobile carriers/device providers to push alerts back to the mobile application. <br>  EC2のインスタンスはモバイルキャリア/デバイスプロバイダと通信してアラートをモバイルアプリケーションにプッシュバックします。
Test1-34. <p>A newspaper organization has an on-premises application which allows the public to search its back catalog and retrieve individual newspaper pages via a website written in Java. They have scanned the old newspapers into JPEGs which is of a total size of 17TB and used Optical Character Recognition (OCR) to populate a commercial search product. The hosting platform and software are now end of life. The organization wants to migrate its archive to AWS, produce a cost-efficient architecture, and still be designed for availability and durability. Which of the below options is the most appropriate?</p> | <p>新聞社の組織では、公開されているカタログを検索し、Javaで書かれたWebサイトを介して個々の新聞ページを取得できるようにするオンプレミスのアプリケーションがあります。彼らは古い新聞を17TBの合計サイズであるJPEGにスキャンし、商用検索製品を埋め込むために光学式文字認識（OCR）を使用しました。ホスティングプラットフォームとソフトウェアは終わりを告げました。組織はアーカイブをAWSに移行し、費用対効果の高いアーキテクチャを作成し、可用性と耐久性を考慮して設計したいと考えています。次のうちどれが最適ですか？</p>	sa:	Use S3 to store and serve the scanned files. Use CloudSearch for query processing, and use Elastic Beanstalk to host the website across multiple availability zones. <br>  S3を使用してスキャンしたファイルを保存し、提供します。 CloudSearchを使用してクエリ処理を行い、Elastic Beanstalkを使用して複数の可用性ゾーン間でWebサイトをホストする|<p>This question presents following scenarios: (1) type of storage that can store large amount of data (17TB), (2) the commercial search product is at the end of its life, and (3) the architecture should be cost effective, highly available, and durable.</p><p><span style="font-size: 1rem;">Tip: Whenever a storage service that can store large amount of data with low cost, high availability, and high durability, always think about using S3.&nbsp;</span></p><p><br></p><p>Option A is incorrect because even though it uses S3, it uses the commercial search software which is at the end of its life.</p><p>Option B is incorrect because striped EBS is not as durable of a solution as S3 and certainly not as cost effective as S3. Also, it has maintenance overhead.</p><p>Option C is CORRECT because (a) it uses S3 to store the images which is cost-effective, (b) instead of the commercial product that is at its end of life, it uses CloudSearch for query processing, and (c) with multi AZ implementation, it achieves high availability.</p><p>Option D is incorrect because with single AZ RDS instance, it does not have high availability.</p><p>Option E is incorrect because (a) this configuration is not scalable, and (b) it does not mention any origin for the CloudFront distribution.</p><p><br></p><p><b>Amazon CloudSearch</b></p><p>With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance. With a few clicks in the&nbsp;<a href="https://aws.amazon.com/console/">AWS Management Console</a>, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.</p><p>You can easily change your search parameters, fine tune search relevance, and apply new settings at any time. As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.</p><p>For more information on AWS CloudSearch, please visit the below link:</p><p></p><a href="https://aws.amazon.com/cloudsearch/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/cloudsearch/</a><br><br><p></p>	Model the environment using CloudFormation. Use an EC2 instance running Apache webserver and an open source search application, stripe multiple standard EBS volumes together to store the JPEGs and search index. <br>  CloudFormationを使用して環境をモデル化する。Apache Webサーバーとオープンソース検索アプリケーションは、複数の標準EBSボリュームをストライプ化して、JPEGと検索インデックスを格納します。	Use S3 to store and serve the scanned files. Install the commercial search application on EC2 Instances and configure with auto-scaling and an Elastic Load Balancer. <br>  S3を使用してスキャンしたファイルを保存し、提供します。 EC2インスタンスに商用検索アプリケーションをインストールし、自動スケーリングとElastic Load Balancerを使用して構成します。	Use a single-AZ RDS MySQL instance to store the search index for the JPEG images and use an EC2 instance to serve the website and translate user queries into SQL. <br>  単一AZのRDS MySQLインスタンスを使用してJPEGイメージの検索インデックスを格納し、EC 2インスタンスを使用してWebサイトを提供し、ユーザーのクエリをSQLに変換します。	Use a CloudFront download distribution to serve the JPEGs to the end users and Install the current commercial search product, along with a Java Container for the website on EC2 instances and use Route53 with DNS round-robin. <br>  CloudFrontのダウンロードを使用してJPEGをエンドユーザーに配信し、現在の商用検索製品をEC 2インスタンスのWebコンテナ用のJavaコンテナと共にインストールし、DNSラウンドロビンでRoute 53を使用します。
Test1-35. <p>A company is building a voting system for a popular TV show, viewers watch the performances then visit the show’s website to vote for their favorite performer. It is expected that in a short period of time after the show is finished the site will receive millions of visitors. The visitors will the first login to the site using their Amazon.com credentials and then submit their vote. After the voting is completed the page will display the vote totals. The company needs to build the site such that can handle the rapid influx of traffic while maintaining good performance but also wants to keep costs to a minimum. Which of the design patterns below should they use?</p> |  <p>ある企業が人気のあるテレビ番組の投票システムを構築しており、視聴者がその公演を見てから、ショーのウェブサイトにアクセスして、好きな出演者に投票します。 ショーが終了してから短期間でサイトには何百万人もの訪問者が訪れることが予想されます。 訪問者はAmazon.comの資格情報を使用してサイトに初めてログインし、投票を行います。 投票が完了すると、そのページに投票総数が表示されます。 同社は、優れたパフォーマンスを維持しながらトラフィックの急激な流入に対応できるだけでなく、コストを最小限に抑えることを望むようなサイトを構築する必要があります。 どのデザインパターンを使用すべきですか？</p>	sa:	Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first login with Amazon service to authenticate the user, the web servers will process the users vote and store the result into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the SQS queue. A set of application servers will then retrieve the items from the queue and store the result into a DynamoDB table. <br>  自動スケールされた一連のWebサーバーの前にCloudFrontとElastic Load Balancerを使用すると、Webサーバーは最初にAmazonサービスでログインしてユーザーを認証し、SQSキューへのアクセス権をもつIAM Roleを設定されたWebサーバーはユーザーの投票を処理し、結果をSQSキューに格納します 。 アプリケーションサーバーのセットは、キューからアイテムを取得し、その結果をDynamoDBテーブルに格納します。|<p>This scenario has following architectural considerations:(1) the application need to be scalable so that it can handle traffic coming from millions of users, (2) the application should handle rapid influx of traffic maintaining good performance, and (3) the cost should be kept to minimum.</p><p><span style="font-size: 1rem;">When the application needs to handle the data coming from millions of users, always think about using DynamoDB. Also, to provide the global users with high performance content access, you need to consider CloudFront, and&nbsp; you need to set the appropriate IAM Role for the front end / web servers to give access to DynamoDB tables.</span></p><p><br></p><p>Option A is incorrect because multi-AZ RDS is expensive solution compared to DynamoDB.</p><p>Option B is incorrect because although this would work, it is not scalable and storing all the data directly in DynamoDB would consume read and write capacity and increase the cost.</p><p>Option C is incorrect because it is not scalable and storing all the data directly in DynamoDB would consume read and write capacity and increase the cost.</p><p>Option D is CORRECT because (a) it is highly scalable, (b) creates appropriate IAM Role to access the DynamoDB database, and (c) more importantly uses SQS to hold the user data/votes such that the application does not consume read and write provisioned capacity of DynamoDB.</p><p><br></p>	Use CloudFront and the static website hosting feature of S3 with the Javascript SDK to call the login with Amazon service to authenticate the user, use IAM Roles to gain permissions to a DynamoDB table to store the users vote. <br>  CloudFrontとS3の静的Webサイトホスティング機能をJavascript SDKとともに使用して、Amazonサービスでログインしてユーザーを認証し、IAMロールを使用してDynamoDBテーブルにユーザーの投票を格納する権限を与えます。	Use CloudFront and an Elastic Load Balancer in front of an auto-scaled set of web servers, the web servers will first login with Amazon service to authenticate the user, the web servers will process the users vote and store the result into a DynamoDB table using IAM Roles for EC2 instances to gain permissions to the DynamoDB table. <br>  自動スケールされた一連のWebサーバーの前にCloudFrontとElastic Load Balancerを使用すると、WebサーバーはまずAmazonサービスでログインしてユーザーを認証し、DynamoDBテーブルへのアクセス権を取得するIAM Roleを持ったWebサーバーは、ユーザーの投票を処理し、その結果を使用して結果をDynamoDBテーブルに格納します 	Use CloudFront and an Elastic Load balancer in front of an auto-scaled set of web servers, the web servers will first login with the Amazon service to authenticate the users, then process the users vote and store the result into a multi-AZ Relational Database Service instance. <br>  自動スケールされたWebサーバーのセットの前にCloudFrontとElastic Load Balancerを使用すると、WebサーバーはまずAmazonサービスでログインしてユーザーを認証し、ユーザーの投票を処理し、その結果を複数のAZのリレーショナルデータベース サービスインスタンスに格納します。
Test1-36. <p>You are developing a new mobile application and are considering storing user preferences in AWS. This would provide a more uniform cross-device experience to users using multiple mobile devices to access the application. The preference data for each user is estimated to be 50KB in size. Additionally, 5 million customers are expected to use the application on a regular basis. The solution needs to be cost-effective, highly available, scalable and secure. How would you design a solution to meet the above requirements?</p> | <p>新しいモバイルアプリケーションを開発しており、AWSにユーザー設定を保存することを検討しています。これにより、複数のモバイルデバイスを使用してアプリケーションにアクセスするユーザーに、より均一なクロスデバイスエクスペリエンスが提供されます。各ユーザーの嗜好データは、50KBのサイズであると見積もられます。さらに、500万の顧客が定期的にアプリケーションを使用する予定です。このソリューションは、コスト効率が高く、可用性が高く、拡張性があり、安全である必要があります。上記の要件を満たすソリューションをどのように設計しますか？</p>	sa:	Setup a DynamoDB table with an item for each user having the necessary attributes to hold the user preferences. The mobile application will query the user preferences directly from the DynamoDB table. Utilize STS. Web Identity Federation, and DynamoDB Fine Grained Access Control to authenticate and authorize access. <br>  ユーザーの設定を保持するために必要な属性を持つ各ユーザーの項目を含むDynamoDBテーブルを設定します。 モバイルアプリケーションは、DynamoDBテーブルから直接ユーザープリファレンスを照会します。 STS、Web Identity Federation、およびDynamoDB Fine Grained Access Controlを使用して、アクセスの認証と承認を行います。|<p><br></p><p>This scenario has following architectural considerations:(1) the application should support millions of customers, so it should be scalable, (2) multiple mobile devices should be able to access the application, and (3) it should be cost effective, highly available and secure.</p><p><span style="font-size: 1rem;">Tip: Whenever the application needs to (a) support millions of users and scalability is most important, always think about DynamoDB, and (b) give mobile apps the access to AWS resource/service, always think about federated acccess using Web Identity Provider and "AssumeRoleWithWebIdentity" API.</span></p><p><br></p><p>Option A is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB.</p><p>Option B is CORRECT because (a) it uses DynamoDB for scalability and cost efficiency, (b) It uses federated acccess using Web Identity Provider, and (c) uses fine grained access priviledges for authenticating the access.</p><p>Option C is incorrect because (a) RDS MySQL is not as scalable and cost-effective as DynamoDB, and (b) user management and access privilege system cannot be used for controlling access.</p><p>Option D is incorrect because accessing the data via S3 would be slower compared to DynamoDB.</p><p><br></p><p>For more information on DynamoDB, please visit the below URL:</p><p><a href="https://aws.amazon.com/dynamodb/developer-resources/" target="_blank">https://aws.amazon.com/dynamodb/developer-resources/</a></p>	Setup an RDS MySQL instance in 2 availability zones to store the user preference data. Deploy a public facing application on a server in front of the database to manage security and access credentials. <br>  2つの可用性ゾーンでRDS MySQLインスタンスをセットアップして、ユーザー設定データを格納します。 パブリック向きのアプリケーションをデータベースの前のサーバーに展開して、セキュリティとアクセス資格情報を管理します。	Setup an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data .The mobile application will query the user preferences from the read replicas. Leverage the MySQL user management and access privilege system to manage security and access credentials. <br> 2つの可用性ゾーンに複数の読み取りレプリカを持つRDS MySQLインスタンスを設定して、ユーザーのプリファレンス・データを保管します。モバイル・アプリケーションは、読み取られたレプリカからユーザーのプリファレンスを照会します。 MySQLユーザー管理とアクセス特権システムを利用して、セキュリティとアクセス資格情報を管理します。	Store the user preference data in S3. Setup a DynamoDB table with an item for each user and an item attribute pointing to the user’ S3 object. The mobile application will retrieve the S3 URL from DynamoDB and then access the S3 object directly by utilizing STS, Web identity Federation, and S3 ACLs to authenticate and authorize access. <br>  ユーザー設定データをS3に格納します。 各ユーザーのアイテムとユーザーのS3オブジェクトを指すアイテム属性を持つDynamoDBテーブルを設定します。 モバイルアプリケーションはDynamoDBからS3 URLを取得し、STS、Web IDフェデレーション、およびS3 ACLを利用してS3オブジェクトに直接アクセスし、アクセスを認証して認証します。
Test1-37. <p>Your team has a Tomcat-based Java application you need to deploy into development, test and production environments. After some research, you opt to use Elastic Beanstalk due to its tight integration with your developer tools and RDS due to its ease of management. Your QA team lead points out that you need to roll a sanitized set of production data into your environment on a nightly basis. Similarly, other software teams in your organization want access to that same restored data via their EC2 instances in your VPC.What of the following would be the optimal setup for persistence and security that meets the above requirements?<br></p> | <p>チームには、開発、テスト、運用環境に展開する必要のあるTomcatベースのJavaアプリケーションがあります。いくつかの調査の後で、Elastic Beanstalkは、開発ツールとRDSとの緊密な統合のために、管理の容易さのために使用されます。QAチームの指導者は、夜間に、サニタイズされた運用データのセットを自分の環境にロールする必要があることを指摘しています。同様に、組織内の他のソフトウェアチームは、VPCのEC2インスタンスを介して同じ復元データにアクセスしたいと考えています。上記の要件を満たす永続性とセキュリティの最適な設定は、次のうちどれですか？<br> </p>	sa:	Create your RDS instance separately and pass its DNS name to your app’s DB connection string as an  environment variable. Create a security group for client machines and add it as a valid source for DB traffic to the security group of the RDS instance itself. <br>  RDSインスタンスを個別に作成し、DNS名を環境変数として環境のデータベース接続文字列に渡します。クライアントマシン用のセキュリティグループを作成し、DBトラフィックの有効なソースとしてRDSインスタンス自体のセキュリティグループに追加します。|<p>The main consideration in this question is: only the EC2 instances in your VPC you should be able to access RDS instances and the setup should support persistence.</p><p><br></p><p>Option A is incorrect because RDS instance will be part of the Elastic Beanstalk environment and would not be optimal for performance.</p><p>Option B is incorrect because you should always use the DNS endpoint of the RDS instance, not IP address as this option suggests.</p><p>Option C is CORRECT because (a) it uses RDS instance separately (not part of Beanstalk), (b) it uses DNS name of RDS for accessing it, and (c) it correctly configures the security group such that only the valid client machines have access to RDS instance.&nbsp;</p><p>Option D is incorrect because the security group is not configured correctly as it gives access to all the hosts in the application subnets.</p><p><br></p>	Create your RDS instance separately and add its IP address to your application’s DB connection strings in your code. Alter its security group to allow access to it from hosts within your VPC’s IP address block. <br>  RDSインスタンスを個別に作成し、IPアドレスをコード内のアプリケーションのデータベース接続文字列に追加します。VPCのIPアドレスブロック内のホストからアクセスできるようにセキュリティグループを変更します。	Create your RDS instance as part of your Elastic Beanstalk definition and alter its security group to allow access to it from hosts in your application subnets. <br>  Elastic Beanstalk定義の一部としてRDSインスタンスを作成し、アプリケーションサブネット内のホストからアクセスできるようにセキュリティグループを変更します。	Create your RDS instance separately and pass its DNS name to your’s DB connection string as an environment variable. Alter its security group to allow access to it from hosts in your application subnets. <br>  RDSインスタンスを個別に作成し、DNS名を環境変数に渡します。セキュリティグループを変更して、アプリケーションサブネット内のホストからアクセスできるようにします。
Test1-38. <p>You are looking to migrate your Development and Test environments to AWS. You have decided to use separate AWS accounts to host each environment. You plan to link each accounts bill to a Master AWS account using Consolidated Billing. To make sure you keep within the budget, you would like to implement a way for administrators in the Master account to have access to stop, delete and/or terminate resources in both the Dev and Test accounts. Identify which of the options will allow you to achieve this goal.</p> | <p>開発環境とテスト環境をAWSに移行しようとしています。各環境をホストするために別々のAWSアカウントを使用することに決めました。連結決済を使用して、各口座明細をマスターAWS口座にリンクする予定です。予算内に収まるように、Masterアカウントの管理者がDevアカウントとTestアカウントのリソースを停止、削除、または終了するためのアクセス権を持つ方法を実装したいと考えています。この目標を達成するためのオプションを特定します。</p>	sa:	Create IAM users in the Master account. Create cross-account roles in the Dev and Test accounts that have full Admin permissions and grant the Master account access. <br>  マスターアカウントでIAMユーザーを作成します。 管理者のフルアクセス権を持ち、マスタアカウントにアクセスできるようにするDevアカウントとTestアカウントでクロスアカウントロールを作成します。|<p><br></p><p>The scenario here is asking you to give permissions to administrators in the Master account such that they can have access to stop, delete, and terminate the resources in two accounts: Dev and Test.</p><p><span style="font-size: 1rem;">Tip: Remember that you always create roles in the account whose resources are to be accesses. In this example that would be Dev and Test. Then you create the users in the account who will be accessing the resources and give them that particular role. In this example, the Master account should create the users.</span></p><p><br></p><p>Option A is incorrect because the permissions cannot be inherited from one AWS account to another.</p><p>Option B is incorrect because cross-account role should be created in Dev and Test accounts, not Master account.</p><p>Option C is CORRECT because (a) the cross-account role is created in Dev and Test accounts, and the users are created in the Master account, that are given that role.</p><p>Option D is incorrect because consolidated billing does not give access to resources in this fashion.</p><p><br></p><p>For more information on cross account access, please visit the below URL</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></span></li></ul><p></p>	Create IAM users and a cross-account role in the Master account that grants full Admin permissions to the Dev and Test accounts. <br>  マスターアカウントでIAMユーザーとクロスアカウントロールを作成し、DevアカウントとTestアカウントに完全な管理者権限を与えます。	Create IAM users in the Master account with full Admin permissions. Create cross-account roles in the Dev and Test accounts that grant the Master account access to the resources in the account by inheriting permissions from the Master account. <br>  完全な管理者権限を持つマスターアカウントでIAMユーザーを作成します。 マスターアカウントからのアクセス許可を継承して、アカウント内のリソースへのマスターアカウントアクセスを許可する、クロスアカウントロールを開発アカウントとテストアカウントに作成します。	Link the accounts using Consolidated Billing. This will give IAM users in the Master account access to resources in the Dev and Test accounts <br>  連結請求を使用して勘定をリンクします。これにより、MasterアカウントのIAMユーザーは、DevアカウントとTestアカウントのリソースにアクセスできます
Test1-39. <p>Your customer is willing to consolidate their log streams, access logs, application logs, security logs etc. in one single system. Once consolidated, the customer wants to analyze these logs in real time based on heuristics. From time to time, the customer needs to validate heuristics, which requires going back to data samples extracted from the last 12 hours? What is the best approach to meet your customer’s requirements?</p> | <p>顧客は​​、ログストリーム、アクセスログ、アプリケーションログ、セキュリティログなどを1つのシステムで統合する意向です。一旦統合されると、ヒューリスティックに基づいてこれらのログをリアルタイムで分析することが求められます。顧客はヒューリスティックを検証する必要があります。ヒューリスティックは、過去12時間から抽出されたデータサンプルに戻る必要があります。お客様の要件を満たす最善のアプローチは何ですか？</p>	sa:	Send all the log events to Amazon Kinesis. Develop a client process to apply heuristics on the logs <br>  すべてのログイベントをAmazon Kinesisに送信します。ログにヒューリスティックを適用するクライアントプロセスを開発する|<p><br></p><p>Whenever the scenario - just like this one - wants to do real-time processing of a stream of data, always think about Amazon Kinesis. Also, remember that the records of the stream is available for 24 hours.</p><p><br></p><p>Option A is incorrect because SQS is not used for real time processing of stream of data.</p><p>Option B is CORRECT because Amazon Kinesis is best suited for application that does the real-time processing of stream of data. Also, the records of the stream is available for 24 hours in Kinesis.</p><p>Option C is incorrect because CloudTrail is not used to process the real-time data processing and EMR is used for batch-processing.</p><p>Option D is incorrect because setting autoscaling of EC2 instances is not cost-effective and EMR is used for batch-processing.</p><p><br></p><p><b>More information on Amazon Kinesis:</b></p><p>Amazon Kinesis is a platform for streaming data on AWS, making it easy to load and analyze streaming data, and also providing the ability for you to build custom streaming data applications for specialized needs.</p> <ul> <li>Use Amazon Kinesis Streams to collect and process large streams of data records in real time.</li> <li>Use Amazon Kinesis Firehose to deliver real-time streaming data to destinations such as Amazon S3 and Amazon Redshift.</li> <li>Use Amazon Kinesis Analytics to process and analyze streaming data with standard SQL.</li> </ul> <p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_39_1.png" alt="" width="1005" height="470" role="presentation" class="img-responsive atto_image_button_middle"></p><p>For more information on Kinesis, please visit the below URL:</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/documentation/kinesis/" target="_blank">https://aws.amazon.com/documentation/kinesis/</a></span></li></ul><p></p>	Send all the log events to Amazon SQS. Setup an Auto Scaling group of EC2 servers to consume the logs and apply the heuristics. <br>  すべてのログイベントをAmazon SQSに送信します。EC 2サーバーのAuto Scalingグループを設定してログを消費し、ヒューリスティックを適用します。	Configure Amazon Cloud Trail to receive custom logs, use EMR to apply heuristics the logs <br>  カスタムログを受け取るようにAmazon Cloud Trailを設定し、ヒューリスティックをログに適用するためにEMRを使用します	Setup Auto Scaling group of EC2 syslogd servers, store the logs S3 use EMR to apply heuristics on the logs <br>  セットアップEC2 syslogdサーバの自動スケーリンググループ、ログの保存S3 EMRを使用してログにヒューリスティックを適用する
Test1-40. <p>You deployed your company website using Elastic Beanstalk and you enabled log file rotation to S3. An Elastic Map Reduce job is periodically analyzing the logs on S3 to build a usage dashboard that you share with your CIO. You recently improved the overall performance of the website using Cloud Front for dynamic content delivery and your website as the origin. After this architectural change, the usage dashboard shows that the traffic on your website dropped by an order of magnitude. How will you fix your usage dashboard?</p> | <p> Elastic Beanstalkを使用して会社のWebサイトを展開し、ログファイルのローテーションをS3に有効にしました。Elastic Map Reduceジョブは、S3のログを定期的に分析して、CIOと共有する使用ダッシュボードを構築しています。最近、クラウドフロントを使用して動的コンテンツ配信とウェブサイトを起点にウェブサイト全体のパフォーマンスを向上させました。このアーキテクチャの変更後、使用ダッシュボードでは、Webサイトのトラフィックが一桁減少したことが示されます。どのように使用ダッシュボードを修正しますか？</p>	sa:	Enable Cloud Front to deliver access logs to S3 and use them as input of the Elastic Map Reduce job. <br>  クラウドフロントがS3にアクセスログを配信できるようにし、それらをElastic Map Reduceジョブの入力として使用します。|<p><br></p><p>In this scenario, you have a web site that is set up using Elastic Beanstalk. This web site delivers logs to S3, which is used by the EMR job to show the usage dashboard. Now, the architecture is changed, where CloudFront is used to deliver the dynamic content, and is using web site as the origin. The effect that is seen is that&nbsp; the dashboard now shows that the trafic to the website is reduced.</p><p><span style="font-size: 1rem;">The most likely reason for this is that the dashboard is not getting the true data of the traffic. Since it is unlikely that EMR failed to get the entire data, the most likely cause could be that the S3 may not have the logs of the entire traffic to the website. Hence, most likely reason is that the CloudFront is not sending the logs to S3.</span></p><p><br></p><p>Option A is CORRECT because, as mentioned earlier, if CloudFront delivers the logs to S3, the EMR job will pick those logs and update the dashboard.</p><p>Option B is incorrect because the CloudTrail logs would contain the logs about the access of all the AWS resources and APIs, and it will not help updating the dashboard.</p><p>Option C is incorrect because CloudWatch metrics logs would not help update the dashboard.</p><p>Option D is incorrect because rebuilding environment would not get the logs created by CloudFront in S3. Hence, the dashboard will till not be up-to-date.</p><p>Option E is incorrect because restart app servers would not get the logs created by CloudFront in S3. Hence, the dashboard will till not be up-to-date.</p><p><br></p><p><b>More information on CloudFront:</b></p><p>You can configure CloudFront to create log files that contain detailed information about every user request that CloudFront receives. These access logs are available for both web and RTMP distributions. If you enable logging, you can also specify the Amazon S3 bucket that you want CloudFront to save files in.</p><p>For more information on Cloudfront logs, please visit the below URL</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html</a></span></li></ul><p></p>	Turn on Cloud Trail and use trail log tiles on S3 as input of the Elastic Map Reduce job <br>  Cloud TrailをオンにしてS3のトレイルログタイルをElastic Map Reduceジョブの入力として使用する	Change your log collection process to use Cloud Watch ELB metrics as input of the Elastic Map Reduce job <br>  Elastic Map Reduceジョブの入力としてCloud Watch ELBメトリックを使用するようにログ収集プロセスを変更する	Use Elastic Beanstalk “Rebuild Environment” option to update log delivery to the Elastic Map Reduce job. <br>  Elastic Beanstalk "Elbuild Environment"オプションを使用して、Elastic Map Reduceジョブへのログ配信を更新します。	Use Elastic Beanstalk ‘Restart App server(s)” option to update log delivery to the Elastic Map Reduce job. <br>  Elastic Beanstalkの[Restart App server（s）]オプションを使用して、Elastic Map Reduceジョブへのログ配信を更新します。
Test1-41. <p>You are running a successful multitier web application on AWS and your marketing department has asked you to add a reporting tier to the application. The reporting tier will aggregate and publish status reports every 30 minutes from user-generated information that is being stored in your web application’s database. You are currently running a Multi-AZ RDS MySQL instance for the database tier. You also have implemented Elasticache as a database caching layer between the application tier and database tier. Select the answer that will allow you to successfully implement the reporting tier with as little impact as possible to your database.</p> | <p> AWSで成功したマルチサイトWebアプリケーションを実行していて、マーケティング部門がレポートティアをアプリケーションに追加するよう求めました。レポート層は、Webアプリケーションのデータベースに格納されているユーザー生成情報から30分ごとにステータスレポートを集計して公開します。現在、データベース層の複数AZ RDS MySQLインスタンスが実行されています。また、アプリケーション層とデータベース層の間にデータベースキャッシュ層としてElasticacheを実装しました。できるだけデータベースに影響を与えずにレポート作成層を正常に実装するための回答を選択します。</p>	sa:	Launch a RDS Read Replica connected to your Multi AZ master database and generate reports by querying the Read Replica. <br>  Multi AZマスターデータベースに接続されているRDS読み取りレプリカを起動し、読み取りレプリカを照会してレポートを生成します。|<p>In question is asking you to design a reporting layer with least impact on the database. If the reporting queries are fired on the database instance, the performance of the database instance would surely get impacted. Since querying for the report would be a read heavy operation, the best solution is to create the read replicas of the database instance and query on them rather than on the database instance. This way, the existing database instance will have the least impact.</p><p><br></p><p>Option A is incorrect because sending the logs to S3 would add to the overhead on the databse instance.</p><p>Option B is incorrect because you cannot access the standby instance.</p><p>Option C is CORRECT because it uses the Read Replicas of the database for the querying of reports.</p><p>Option D is incorrect because the querying on ElastiCache may not always give you the latest and entire data, as the cache may not always be up-to-date.</p><p><br></p><p>For more information on Read Replica’s, please visit the below URL</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/rds/details/read-replicas/" target="_blank">https://aws.amazon.com/rds/details/read-replicas/</a></span></li></ul><p></p>	Generate the reports by querying the synchronously replicated standby RDS MySQL instance maintained through Multi-AZ. <br>  Multi-AZを介して管理されている同期レプリケートされたスタンバイRDS MySQLインスタンスを照会して、レポートを生成します。	Continually send transaction logs from your master database to an S3 bucket and generate the reports off the S3 bucket using S3 byte range requests. <br>  マスタデータベースからS3バケットにトランザクションログを継続的に送信し、S3バイト範囲要求を使用してS3バケットからレポートを生成します。	Generate the reports by querying the ElasliCache database caching tier. <br>  ElasliCacheデータベースのキャッシュ層を照会して、レポートを生成します。
Test1-42. <p>A web company is looking to implement an intrusion detection and prevention system for their deployed VPC. This platform should have the ability to scale to thousands of instances running inside of the VPC. How should they architect their solution to achieve these goals?</p> | <p> Web企業は、導入されたVPCの侵入検知と防御システムを実装しようとしています。このプラットフォームには、VPCの内部で実行されている数千のインスタンスに拡張する能力が必要です。これらの目標を達成するために彼らのソリューションをどのように構築すべきですか？</p>	sa:	Create a second VPC and route all traffic from the primary application VPC through the second VPC where the scalable virtualized IDS/IPS platform resides <br>  第2のVPCを作成し、プライマリアプリケーションVPCからのすべてのトラフィックを、スケーラブルな仮想化IDS / IPSプラットフォームが存在する2番目のVPCにルーティングします|<p>This quesion is asking you to design a scalable IDS/IPS solution (easily applicable to thousands of instances).</p><p><span style="font-size: 1rem;">There are couple of ways of designing the IDS/IPS systems: (1) install the IDS/IPS agents on each instance in the VPC, and (2) create a separate Security-VPC with only IDS/IPS instances, and route the incoming traffic via this VPC to the other VPC that contains the other EC2 resources.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>Option A is incorrect because promiscous mode is not supported by AWS.</p><p>Option B is CORRECT because it creates a second VPC which contains the scalable IDS/IPS resources, and routes the trafic via these VPC to other VPC.</p><p>Option C is incorrect because the traffic should flow FROM the security VPC, not TO it.</p><p>Option D is plausible, but (a) it is not a scalable solution, (b) it is only IDS solution, not IPS solution.</p><p><br></p><p><span style="font-size: 1rem;">Please visit the URL below for a good slide deck from AWS for getting IDS in place:</span></p><p></p><a href="https://awsmedia.s3.amazonaws.com/SEC402.pdf" target="_blank" style="font-size: 1rem;">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a><br><br><p></p>	Configure an instance with monitoring software and the elastic network interface (ENI) set to promiscuous mode packet sniffing to see all traffic across the VPC. <br>  監視ソフトウェアを使用してインスタンスを設定し、無差別モードのパケットスニッフィングに設定された弾性ネットワークインターフェイス（ENI）を使用して、VPC全体のすべてのトラフィックを表示します。	Configure servers running in the VPC using the host-based ‘route’ commands to send all traffic through the platform to a scalable virtualized IDS/IPS. <br>  ホストベースの 'route'コマンドを使用してVPCで実行されているサーバを設定し、プラットフォームを介してすべてのトラフィックをスケーラブルな仮想化IDS / IPSに送信します。	Configure each host with an agent that collects all network traffic and sends that traffic to the IDS/IPS platform for inspection. <br>  すべてのネットワークトラフィックを収集し、そのトラフィックを検査のためにIDS / IPSプラットフォームに送信するエージェントで各ホストを設定します。
Test1-43. <p>A web-startup runs its very successful social news application on Amazon EC2 with an Elastic Load Balancer, an Auto-Scaling group of Java/Tomcat application-servers, and DynamoDB as a data store. The main web application best runs on m2 x large instances since it is highly memory- bound. Each new deployment requires the semi-automated creation and testing of a new AMI for the application servers which takes quite a while and is therefore only done once per week. Recently, a new chat feature has been implemented in Node.js and waits to be integrated into the architecture. First tests show that the new component is CPU bound because the company has some experience with using Chef, they decided to streamline the deployment process and use AWS OpsWorks as an application lifecycle tool to simplify management of the application and reduce the deployment cycles. What configuration in AWS OpsWorks is necessary to integrate the new chat module in the most cost-efficient and flexible way?</p> | <p> Web Startupは、Elastic Load Balancer、Java / TomcatアプリケーションサーバーのAuto-Scalingグループ、およびデータストアとしてのDynamoDBを使用して、Amazon EC2上で非常に成功したソーシャルニュースアプリケーションを実行します。メインのWebアプリケーションは、メモリに非常に拘束されているため、m2 x大きなインスタンスで最もよく動作します。新しいデプロイメントでは、アプリケーションサーバー用の新しいAMIの半自動化された作成とテストが必要です。これにはかなりの時間がかかり、したがって1週間に1回しか実行されません。最近、新しいチャット機能がNode.jsに実装され、アーキテクチャに統合されるのを待ちます。最初のテストでは、新しいコンポーネントがCPUにバインドされていることが示されています。これは、シェフの使用経験があるためです。アプリケーションの管理を簡素化し、展開サイクルを短縮するために、導入プロセスを合理化し、AWS OpsWorksをアプリケーションライフサイクルツールとして使用することに決めました。新しいチャットモジュールを最も費用対効果の高い柔軟な方法で統合するには、AWS OpsWorksのどの設定が必要ですか？</p>	sa:	Create one AWS OpsWorks stack create two AWS OpsWorks layers, create one custom recipe. <br>  1つのAWS OpsWorksスタックを作成し、2つのAWS OpsWorksレイヤを作成し、1つのカスタムレシピを作成します。|<p><br></p><p>The scenario here requires that you need to manage the application that is created with java, node.js, and DynamoDB using AWS OpsWorks. The deployment process should be streamlined and the deployment cycles should be reduced.</p><p><span style="font-size: 1rem;">As the java and node.js have different resource requirements, it makes sense to deploy them on different layers. It would make the maintenance easier as well.</span></p><p><br></p><p>Option A is incorrect because it would be a better solution to create two separate layers: one for Java and one for node.js.</p><p>Option B is CORRECT because only one stack would be sufficient, and two layers (one for Java and one for node.js) would be required for handling separate requirements. One custom recipe for DynamoDB would be required.</p><p>Option C is incorrect because two OpsWork stacks are unnecessary.</p><p>Option D is incorrect because two OpsWork stacks are unnecessary.</p><p><br></p><p><b>More information on AWS OpsWork Stack</b></p><p>An AWS OpsWorks Stack defines the configuration of your entire application: the load balancers, server software, database, etc. You control every part of the stack by building layers that define the software packages deployed to your instances and other configuration details such as Elastic IPs and security groups. You can also deploy your software onto layers by identifying the repository and optionally using Chef Recipes to automate everything Chef can do, such as creating directories and users, configuring databases, etc. You can use OpsWorks Stacks’ built-in automation to scale your application and automatically recover from instance failures. You can control who can view and manage the resources that are used by your application, including ssh access to the instances that your application uses.</p><p>For more information on Ops work, please visit the below URL</p><p></p><a href="https://aws.amazon.com/opsworks/stacks/faqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/opsworks/stacks/faqs/</a><br><br><p></p>	Create one AWS OpsWorks stack, create one AWS OpsWorks layer, create one custom recipe. <br>  1つのAWS OpsWorksスタックを作成し、1つのAWS OpsWorksレイヤを作成し、1つのカスタムレシピを作成します。	Create two AWS OpsWorks stacks create two AWS OpsWorks layers, create one custom recipe. <br>  2つのAWS OpsWorksスタックを作成し、2つのAWS OpsWorksレイヤを作成し、1つのカスタムレシピを作成します。	Create two AWS OpsWorks stacks create two AWS OpsWorks layers, create two custom recipe. <br>  2つのAWS OpsWorksスタックを作成して2つのAWS OpsWorksレイヤーを作成し、2つのカスタムレシピを作成します。
Test1-44. <p>Your firm has uploaded a large amount of aerial image data to S3. In the past, in your on-premises environment, you used a dedicated group of servers to process this data and used Rabbit MQ – An open source messaging system to get job information to the servers. Once processed the data would go to the tape and be shipped offsite. Your manager told you to stay with the current design, and leverage AWS archival storage and messaging services to minimize cost. Which of the following options is correct?</p> | <p>大量の航空写真データをS3にアップロードしました。これまで、社内環境では、専用のサーバーグループを使用してこのデータを処理し、Rabbit MQを使用しました。オープンソースのメッセージングシステムを使用してサーバーにジョブ情報を取得しました。処理されると、データはテープに送られ、オフサイトで出荷されます。マネージャーは、現行の設計を維持し、AWSアーカイブストレージとメッセージングサービスを活用してコストを最小限に抑えるように指示しました。次のうち正しいものはどれですか？</p>	sa:	Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Glacier. <br>  SQSのメッセージを処理するためにスポットインスタンスを使用するキューの深さによってトリガされた自動スケーリングされたワーカーを設定します。データが処理されたら、S3オブジェクトのストレージクラスをGlacierに変更します。|<p>The most suggestive hint in this question is that it asks you to leverage AWS archival storage and messaging services. Hence, you should look for options Glacier and SQS.</p><p><br></p><p>Option A is incorrect because (a) RRS is not an archival storage option, and (b) since auto scaling is not mentioned, you cannot use CloudWatch alarms to terminate the idle EC2 instances.</p><p>Option B is CORRECT because (a) it uses SQS to process the messages, (b) it uses Glacier as the archival storage solution - which is cost optimized.</p><p>Option C is incorrect because RRS is not an archival storage option; instead, use Glacier as it is a low cost archival solution (cost lower than RRS).</p><p>Option D is incorrect as SNS cannot be use to process the messages. i.e. it cannot replace the functionality that was getting provided by RabbitMQ.</p><p><br></p>	Use SQS for passing job messages. Use Cloud Watch alarms to terminate EC2 worker instances when they become idle. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage. <br>  クラウドウォッチアラームを使用して、EC2ワーカーインスタンスがアイドル状態になったときにEC2ワーカーインスタンスを終了します。データが処理されたら、S3オブジェクトのストレージクラスをRedundancy Redundancy Storageに変更します。	Setup Auto-Scaled workers triggered by queue depth that use spot instances to process messages in SQS. Once data is processed, change the storage class of the S3 objects to Reduced Redundancy Storage (RRS). <br>  セットアップ自動 -  SQSのメッセージを処理するためにスポットインスタンスを使用するキューの深さによってトリガされた、スケーリングされたワーカー。データが処理されたら、S3オブジェクトのストレージクラスをRedundancy Redundancy Storage（RRS）に変更します。	Use SNS to pass the job messages. Use Cloud Watch alarms to terminate spot worker instances when they become idle. Once data is processed, change the storage class of the S3 object to Glacier. <br>  クラウドウォッチアラームを使用して、スポットワーカーインスタンスがアイドル状態になったときに終了させます。データが処理されたら、S3オブジェクトのストレージクラスをGlacierに変更します。
Test1-45. <p>A corporate web application is deployed within an Amazon Virtual Private Cloud (VPC) and is connected to the corporate data center via an IPsec VPN. The application must authenticate against the on-premises LDAP server. After authentication, each logged-in user can only access an Amazon Simple Storage Space (S3) keyspace specific to that user. Which two approaches can satisfy these objectives?</p> <p>Choose 2 options from the below<br></p> |  <p>企業Webアプリケーションは、Amazon Virtual Private Cloud（VPC）内に展開され、IPsec VPN経由で企業のデータセンターに接続されます。 アプリケーションは、オンプレミスLDAPサーバーに対して認証する必要があります。 認証後、各ログインユーザーは、そのユーザーに固有のAmazon Simple Storage Space（S3）キースペースにのみアクセスできます。 どちらのアプローチがこれらの目標を満たすことができますか？</ p> <p>下記から2つのオプションを選択してください<br></p>	ma:	x:Develop an identity broker that authenticates against IAM Security Token Service (STS) to assume a IAM role in order to get temporary AWS security credentials The application calls the identity broker to get AWS temporary security credentials with access to the appropriate S3 bucket. <br>  一時的なAWSセキュリティ資格を取得するためにIAMセキュリティトークンサービス（STS）に対して認証するIDブローカーを開発します。アプリケーションはIDブローカーを呼び出して、適切なS3バケットにアクセスしてAWS一時的セキュリティー資格情報を取得します。|<p></p><div><span style="font-size: 1rem;"><br></span></div><div><span style=""><div style="">There are two architectural considerations here: (1) The users must be authenticated via the on-premise LDAP server, and (2) each user should have access to S3 only.</div><div style=""><br></div><div style="">With this information, it is important to first authenticate the users using LDAP, get the IAM Role name, then get the temporary credentials from STS, and finally access the S3 bucket using those credentials. And second, create an IAM Role that provides access to S3.</div><div style=""><br></div><div style="">Option A is incorrect because the users need to be authenticated using LDAP first, not STS. Also, the temporary credentials to log into AWS are provided by STS, not identity broker.</div><div style=""><br></div><div style="">Option B is CORRECT because it follows the correct sequence. It authenticates users using LDAP, gets the security token from STS, and then accesses the S3 bucket using the temporary credentials.</div><div style=""><br></div><div style="">Option C is CORRECT because it follows the correct sequence. It develops an identity broker that authenticates users against LDAP, gets the security token from STS, and then accesses the S3 bucket using the IAM federated user credentials.</div><div style=""><br></div><div style="">Option D is incorrect because you cannot use the LDAP credentials to log into IAM.</div></span></div><div><span style="font-size: 1rem;"><br></span></div><div><span style="font-size: 1rem;"><br></span></div><div><span style="font-size: 1rem;">An example diagram of how this works from the AWS documentation is given below.</span></div> <p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_45_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="650" height="621"></p><p>&nbsp;</p><p>For more information on federated access, please visit the below link:</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a><br><p></p>	o:The application authenticates against LDAP and retrieves the name of an IAM role associated with the user. ?The application then calls the IAM Security Token Service (STS) to assume that IAM role. The application can use the temporary credentials to access the appropriate S3 bucket. <br>  アプリケーションはLDAPに対して認証を行い、そのユーザーに関連付けられたIAMロールの名前を取得します。 次に、アプリケーションはIAMセキュリティトークンサービス（STS）を呼び出して、そのIAMロールを想定します。 アプリケーションは、一時資格情報を使用して適切なS3バケットにアクセスできます。	o:Develop an identity broker that authenticates against LDAP and then calls IAM Security Token Service (STS) to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket. <br>  LDAPに対して認証を行い、IAMセキュリティトークンサービス（STS）を呼び出してIAMフェデレーションされたユーザー資格情報を取得するIDブローカーを開発します。 アプリケーションはIDブローカーを呼び出して、IAMフェデレーションされたユーザー資格情報を適切なS3バケットにアクセスできるようにします。	x:The application authenticates against LDAP the application then calls the AWS identity and Access?Management (IAM) Security service to log in to IAM using the LDAP credentials the application can use the IAM temporary credentials to access the appropriate S3 bucket. <br>  アプリケーションはLDAPに対して認証され、アプリケーションはAWS IDとアクセス管理（IAM）セキュリティサービスを呼び出してLDAP資格情報を使用してIAMにログインし、アプリケーションはIAM一時資格情報を使用して適切なS3バケットにアクセスできます。
Test1-46. <p>Your company is planning to develop an application in which the front end is in .Net and the backend is in DynamoDB. There is expectant of a high load on the application. How could you ensure the scalability and cost-effectiveness of the application to reduce the load on the DynamoDB database? Choose an answer from the below options.<br></p> | <p>フロントエンドが.Net、バックエンドがDynamoDBのアプリケーションを開発する予定です。アプリケーションに負荷がかかることが予想されます。DynamoDBデータベースの負荷を軽減するために、アプリケーションのスケーラビリティとコスト効率をどのように確保できますか？以下のオプションから回答を選択します。<br> </p>	sa:	Use SQS to hold the database requests instead of overloading the DynamoDB database. Then have a service that asynchronously pull the messages and write them to DynamoDB. <br>  SQSを使用して、DynamoDBデータベースのオーバーロードではなく、データベース要求を保持します。次に、メッセージを非同期にプルしてDynamoDBに書き込むサービスを用意します。|<p><br></p><p>This question is asking for an option that can be used to reduce the load on DynamoDB database. The option has to be scalable.</p><p><span style="font-size: 1rem;">In such scenario, the best option to use is SQS, because it is scalable and cost efficient as well.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because adding more databases is not going to reduce the load on existing DynamoDB database. Also, this is not a cost efficient solution.</span></p><p><span style="font-size: 1rem;">Option B is incorrect because increasing the write capacity is an expensive option.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because it uses SQS to assist in taking over the load from storing the data in DynamoDB, and it is scalable as well as cost efficient.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because MultiAZ configuration is not going to help reduce the load, in fact it will affect the performance as the records in DynamoDB would get replicated in multiple availability zones.</span></p><p><br></p><p><b>More information on SQS:</b></p><p>When the idea comes for scalability then SQS is the best option. Normally DynamoDB is scalable, but since one is looking for a cost effective solution, the messaging in SQS can assist in managing the situation mentioned in the question.</p><p>Amazon Simple Queue Service (SQS) is a fully-managed message queuing service for reliably communicating among distributed software components and microservices - at any scale. Building applications from individual components that each perform a discrete function improves scalability and reliability, and is best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available</p><p><br></p><p>For more information on SQS, please refer to the below URL:<br> <a href="https://aws.amazon.com/sqs/" target="_blank">https://aws.amazon.com/sqs/</a></p><p><br></p>	Increase write capacity of Dynamo DB to meet the peak loads. <br>  ピーク負荷を満たすためにDynamo DBの書き込み容量を増やします。	Add more DynamoDB databases to handle the load. <br>  DynamoDBデータベースを追加して負荷を処理します。	Launch DynamoDB in Multi-AZ configuration with a global index to balance writes. <br>  グローバルインデックスを使用して複数のAZ構成でDynamoDBを起動し、書き込みのバランスをとります。
Test1-47. <p>Your company has 2 departments that want to use Redshift. One department uses a process that takes 3 hours to analyze the data whereas the second department just takes a few minutes. What can you do to ensure that there is no performance impact and delete to the second’s department’s queries? Choose an answer from the below options.</p> | <p> Redshiftを使用する2つの部門があります。1つの部門では、データの分析に3時間かかりますが、2番目の部門では数分かかります。パフォーマンスへの影響がなく、第2の部門のクエリへの削除を確実にするために、あなたは何ができますか？以下のオプションから回答を選択してください。</p>	sa:	Create two separate workload management groups and assign them to respective departments <br>  2つの別々のワークロード管理グループを作成し、それらの部門に割り当てる|<p><span style="font-size: 1rem;">Whenever the question gives you scenario where, in Redshift, there are two processes - one fast and one slow, and you are asked to ensure that there is no impact on the queries of a process, always think about creating two separate workload management groups.</span></p><p><br></p><p>Option A is incorrect because Redshift does not have read replicas.</p><p>Option B is incorrect because this will affect the performance of the current Redshift cluster.</p><p>Option C is incorrect because queries cannot be paused in Redshift.</p><p>Option D is CORRECT because the best solution - without any effect on performance - is to create two separate workload management groups - one for each department and run the queries on them. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/1_47_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><b style="font-size: 1rem;"><br></b></p><p><b style="font-size: 1rem;">More information on Amazon Redshift Workload Management</b></p><p>Amazon Redshift Workload Management (WLM) enables users to flexibly manage priorities within workloads so that short, fast-running queries won't get stuck in queues behind long-running queries.</p><p>Amazon Redshift WLM creates query queues at runtime according to&nbsp;service classes, which define the configuration parameters for various types of queues, including internal system queues and user-accessible queues. From a user perspective, a user-accessible service class and a queue are functionally equivalent. For consistency, this documentation uses the term&nbsp;queue&nbsp;to mean a user-accessible service class as well as a runtime queue.</p><p>For more information on redshift workload management, please refer to the below url<br> <a href="http://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html</a></p>	Start another Redshift cluster from snapshot for the second department if current Redshift cluster is busy processing long queries <br>  現在のRedshiftクラスタが長いクエリを処理している場合は、2番目の部門のスナップショットから別のRedshiftクラスタを開始します	Pause long queries and resume the queries afterwards <br>  長いクエリを一時停止し、後でクエリを再開する	Create a read replica of the Red shift instance and run second department’s queries on read replica <br>  Redシフトインスタンスの読み取りレプリカを作成し、読み取りレプリカで第2部門のクエリを実行する
Test1-48. <p>What can be done in Cloudfront to ensure that as soon as the content is changed in the source, it is delivered to the client? Choose an answer from the options below options.</p> | <p>クラウドフロントでは、コンテンツがソースで変更されるとすぐにクライアントに配信されるようにするために何ができるのですか？下記のオプションから回答を選択してください。</p>	sa:	Set TTL to 0 seconds. <br>  TTLを0秒に設定します。|<p><br></p><p></p><p>In CloudFront, to enforce the delivery of content to the user as soon as it gets changed by the origin, the time to live (TTL) should be set to 0.</p><p>&nbsp;</p><p>Option A is incorrect because invalidate is used to remove the content from CloudFront edge locations cache before it expires. The next time a viewer requests the object, CloudFront fetches the content from the origin; whereas, setting TTL to 0 enforces CloudFront to deliver the latest content as soon as origin updates it.</p><p>Option B is incorrect because setting TTL to 10 will keep the content in cache for some time even though origin updates it.</p><p>Option C is CORRECT because setting TTL to 0 will enforce the delivery of content to the user as soon as it gets changed by the origin.</p><p>Option D is incorrect as CloudFront surely serves dynamic content.</p><p>Option E is incorrect as you do not have to contact AWS support center for this scenario.</p><br><p></p><p><b>More information on TTL in CloudFront&nbsp;</b></p><p>You can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. The low TTL is also given in the AWS documentation.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_48_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="158" width="796"></p><p><br></p><p>For more information on CloudFront dynamic content, please refer to the below URL:<br> <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html</a></p><p><br></p>	Set TTL to 10 seconds. <br>  TTLを10秒に設定します。	Use fast invalidate feature provided in CloudFront. <br>  CloudFrontで提供される高速無効化機能を使用します。	Dynamic content cannot be served from the CloudFront. <br>  動的コンテンツをCloudFrontから配信することはできません。	You have to contact AWS support center to enable this feature. <br>  この機能を有効にするには、AWSサポートセンターに連絡する必要があります。
Test1-49. <p>An application has been set up with Autoscaling and EC2 instances in multiple AZ’s. When you look at the load balancer logs you notice that EC2 instances in one of the AZ’s are not receiving requests. What can be wrong here?</p> | <p>複数のAZでAutoscalingとEC2インスタンスが設定されたアプリケーションです。ロードバランサのログを見ると、AZのEC2インスタンスがリクエストを受信して​​いないことがわかります。ここで何が間違っていますか？</p>	sa:	Availability zone is not added to Elastic load balancer <br>  可用性ゾーンはエラスティックロードバランサに追加されません|<p><br></p><p></p><p>In order to make sure that all the EC2 instances behind a cross-zone ELB receive the requests, make sure that all the applicable availability zones (AZs) are added to that ELB.</p><p>&nbsp;</p><p>Option A is incorrect because autoscaling can work with multiple AZs.</p><p>Option B is incorrect because autoscaling can be enabled for multi AZ in any single region, not just N. Virginia. (see the image below)</p><p>Option C is CORRECT because most likely the reason is that the AZ – whose EC2 instances are not receiving requests – is not added to the ELB.&nbsp;</p><p>Option D is incorrect because instances need not be added manually to AZ (they should already be there!). </p><p><img src="https://s3.amazonaws.com/awssap/1_49_1.jpg" alt="" width="970" height="635" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p></p><p><b>More information on adding AZs to ELB</b></p><p>When you add an Availability Zone to your load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. Load balancer nodes accept traffic from clients and forward requests to the healthy registered instances in one or more Availability Zones.</p><p>For more information on adding AZ’s to ELB, please refer to the below url<br> <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html" target="_blank">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html</a></p>	Autoscaling can be enabled for multi AZ only in North Virginia region <br>  ノースバージニア州の複数のAZのみでオートスケーリングを有効にすることができます	Autoscaling only works for single availability zone <br>  オートスケーリングは単一の可用性ゾーンでのみ機能します	Instances need to manually added to availability zone <br>  インスタンスを手動で可用性ゾーンに追加する必要がある
Test1-50. <p>You want to migrate an EC2 instance from one region to another and use the same PEM keys. How will you accomplish this?</p> | <p>ある地域から別の地域へEC2インスタンスを移行し、同じPEMキーを使用したいとします。これをどのように達成しますか？</p>	sa:	Using import key-pair feature using AWS web console <br>  AWS Webコンソールを使用したインポートキーペア機能の使用|<p>Key pairs across regions is not possible. In order to use key pairs across regions you need to import the key pairs in the respective regions.</p><p>You need to go the respective region and from the EC2 dashboard , click on Import Key pair and choose the relevant key pair.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_50_1.png" alt="" role="presentation" class="atto_image_button_text-bottom" width="639" height="182"></p><p></p><p><br></p><p>Option A is incorrect because key pair is region specific – not global.</p><p>Option B is incorrect because keys cannot be copied across different regions, they need to be imported.</p><p>Option C is CORRECT because import key pair functionality enables migrating an EC2 instance from one region to another and use the same PEM key.</p><p>Option D is incorrect because PEM keys cannot be copied to another region as part of the AMI.</p><br><p></p><p>For more information on bringing your own key pair, please refer to the below URL:<br> <a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-feature-bring-your-own-keypair/" target="_blank">https://aws.amazon.com/blogs/aws/new-amazon-ec2-feature-bring-your-own-keypair/</a></p>	Use copy key command line API to transfer key to different regions <br>  コピーキーのコマンドラインAPIを使用して、異なる地域にキーを転送する	Key pair is not a region level concept, all the keys are available globally <br>  キーペアはリージョンレベルの概念ではなく、すべてのキーがグローバルに使用可能です	Copy AMI of your EC2 machine between regions and start an instance from that AMI <br>  リージョン間でEC 2マシンのAMIをコピーし、そのAMIからインスタンスを開始する
Test1-51. <p>Which feature of S3 needs to be enabled for a resource in a bucket in one domain to access a resource in a bucket in another domain? Choose an answer from the below options.<br></p> | <p>あるドメインのバケット内のリソースで、別のドメインのバケット内のリソースにアクセスするには、S3のどの機能を有効にする必要がありますか？以下のオプションから回答を選択します。<br> </p>	sa:	You can configure your bucket to explicitly enable cross-origin requests from the other domain. <br>  バケットを構成して、他のドメインからのクロスオリジン要求を明示的に有効にすることができます。|<p><br></p><p></p><p>Option A is CORRECT because CORS enables a resource in one bucket access a resource in another.</p><p>Option B is incorrect because you do not need to modify bucket policy.</p><p>Option C is incorrect because you do not need to modify ACL policy.</p><p>Option D is incorrect as cross bucket access is possible via CORS configuration.</p><br><p></p><p><b>More information on CORS:</b></p><p>Cross-Origin Resource Sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support in Amazon S3, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p><p>For more information on Cross origin resource sharing, please refer to the below url<br> <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html" target="_blank">http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>	Modify bucket policy to allow cross domain access. <br>  クロスドメインアクセスを許可するバケットポリシーを変更する。	Modify the ACL policy to allow cross domain access. <br>  クロスドメインアクセスを許可するようにACLポリシーを変更します。	This is not possible <br>  これは不可能です
Test1-52. <p>As a solution architect professional you have been requested to ensure that monitoring can be carried out for EC2 instances which are located in different AWS regions? Which of the below options can be used to accomplish this.</p> | <p>ソリューションアーキテクトのプロフェッショナルとして、さまざまなAWS地域にあるEC2インスタンスのモニタリングを確実に行うことが求められていますか？これを達成するために使用できるオプションは次のうちどれですか？</p>	sa:	Have one single dashboard to report metrics to CloudWatch from different region <br>  1つのダッシュボードで異なる地域のCloudWatchに指標を報告する|<p>You can monitor AWS resources in multiple regions using a single CloudWatch dashboard. For example, you can create a dashboard that shows CPU utilization for an EC2 instance located in the US-west-2&nbsp;region with your billing metrics, which are located in the&nbsp;us-east-1&nbsp;region.</p><p>Please see the snapshot below which shows how a global dashboard looks like:</p><p><img src="https://s3.amazonaws.com/awssap/1_52_1.png" alt="" width="1820" height="523" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p><span id="docs-internal-guid-1ae5823d-834e-ea51-f7da-4d29fb3e6bde"></span></p><p dir="ltr" style="">Option A is incorrect because you can monitor AWS resources in multiple regions using a single CloudWatch dashboard.</p><p dir="ltr" style="">Option B is incorrect because you do not need to explicitly register any instances from different regions.</p><p dir="ltr" style="">Option C is CORRECT because you can monitor AWS resources in multiple regions using a single CloudWatch dashboard.</p><p dir="ltr" style="">Option D is incorrect because as mentioned in option C, the monitoring of EC2 instances is possible using a single dashboard created from CloudWatch matrix.</p><br><p></p><p>For more information on Cloudwatch dashboard, please refer to the below URL:<br></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cross_region_dashboard.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cross_region_dashboard.html</a></span></li></ul> <p></p>	Register instances running on different regions to CloudWatch <br>  異なる地域で実行されているインスタンスをCloudWatchに登録する	Create separate dashboards in every region <br>  各地域に別々のダッシュボードを作成する	This is not possible <br>  これは不可能です
Test1-53. <p>As an AWS Administrator, you have set up an ELB within a couple of Availability Zones. You have set up a web application on this setup. You notice that the traffic is not being evenly distributed across the AZ’s. What can be done to alleviate this issue? Choose an answer from the below options.</p> | <p> AWS管理者は、いくつかの可用性ゾーン内にELBを設定しています。この設定でWebアプリケーションを設定しました。トラフィックがAZ全体に均等に分散していないことがわかります。この問題を緩和するためには何ができますか？以下のオプションから回答を選択してください。</p>	sa:	Disable sticky sessions on the ELB. <br>  ELBのスティッキセッションを無効にします。|<p></p><p><br></p><p>The traffic is not evenly distributed across the instances in multiple AZs. That means the traffic is going to only specific EC2 instances. This happens when either the instances which are not receiving the traffic are unhealthy, or the instances that are receiving the traffic are holding onto the session. </p><p>This scenario does not mention about any unhealthy instances. So, it is most likely related to instances holding onto sessions. This means the ELB has sticky sessions enabled.</p><p>&nbsp;</p><p>Option A is CORRECT because this situation occurs when ELB has sticky sessions or session affinity enabled.</p><p>Option B is incorrect because reducing the frequency of health checks will not force the even distribution of the traffic.</p><p>Option C is incorrect because if sticky sessions are enabled, increasing the number of instances in each AZ will not help receiving the traffic at all. In fact, more instances will remain idle now.</p><p>Option D is incorrect because recreating ELB again will not resolve this issue.</p><br><p></p><p><b>More information on ELB Sticky Sessions:</b></p><p>The load balancer uses a special cookie to track the instance for each request to each listener. When the load balancer receives a request, it first checks to see if this cookie is present in the request. If so, the request is sent to the instance specified in the cookie. If there is no cookie, the load balancer chooses an instance based on the existing load balancing algorithm. A cookie is inserted into the response for binding subsequent requests from the same user to that instance. The stickiness policy configuration defines a cookie expiration, which establishes the duration of validity for each cookie.</p><p>This could be a reason as to why the sessions are going to a certain AZ.</p><p>For more information on ELB sticky sessions, please refer to the below URL<br> <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html" target="_blank">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html</a></p>	Reduce the frequency of the health checks <br>  ヘルスチェックの頻度を減らす	Increase the amount of instances hosting the web application in each AZ. <br>  各AZでWebアプリケーションをホストするインスタンスの量を増やします。	Recreate the ELB again. <br>  ELBを再作成してください。
Test1-54. <p>As an AWS Cloud Architect professional , In Cloudfront what is the Origin Protocol policy that must be chosen to ensure that the communication with the origin is done either via http or https. Choose an answer from the options below</p> | <p> AWSクラウドアーキテクトのプロフェッショナルとして、クラウドフロントでは、起点との通信がhttpまたはhttps経由で行われるようにするために選択する必要があるオリジンプロトコルポリシーとは何ですか。下記のオプションから回答を選択してください。</p>	sa:	Match Viewer <br>  マッチビューア|<p>Its clearly given in the aws documentation that the Origin Protocol Policy should be set accordingly.</p><p><br></p><p><span id="docs-internal-guid-1ae5823d-8355-0e69-01c9-4f676b584b64"></span></p><p dir="ltr" style="">Option A, B, and D are all incorrect because the answer is Match Viewer</p><p dir="ltr" style="">Option C is CORRECT because if the Origin Protocol Policy is set to Match Viewer, the CloundFront communicates with the origin using HTTP or HTTPS depending on the protocol of the viewer request.</p><br><img src="https://s3.amazonaws.com/awssap/1_54_1.png" alt="" width="747" height="445" role="presentation" class="img-responsive atto_image_button_text-bottom" style="font-size: 1rem;"><p></p><p>For more information on Cloudfront CDN please see the below link:</p><p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html</a></p>	HTTPS <br>  HTTPS	HTTP <br>  HTTP	None of the above <br>  上記のどれでもない
Test1-55. <p>A company has 2 VPC’s in the same region. How can you connect the VPC’s so that EC2 instances in one VPC can communicate with the other VPC? Choose an answer from the below options.<br></p> | <p>同じ地域に2つのVPCがあります。あるVPCのEC2インスタンスが他のVPCと通信できるように、VPCをどのように接続できますか？以下のオプションから回答を選択します。<br> </p>	sa:	Create a VPC peering connection between each VPC. <br>  各VPC間でVPCピアリング接続を作成します。|<p><br></p><p><span id="docs-internal-guid-1ae5823d-835b-1ad5-6f8e-f44bdf1a0567"></span></p><p dir="ltr" style="">Option A is incorrect because migration of the resources is unnecessary in this case.</p><p dir="ltr" style="">Option B is CORRECT because VPC peering is the best way of connecting the EC2 instances in two VPCs in the same region.</p><p dir="ltr" style="">Option C is incorrect because you cannot create Direct Connection between VPCs.</p><p dir="ltr" style="">Option D is incorrect because you cannot create IPSec tunnel between VPCs.</p><p dir="ltr" style=""><br></p><b>More information on VPC peering:</b><p></p><p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region.</p><p>For more information on VPC Peering please see the below link</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p>	Migrate each VPC resources from one VPC using migration tools such as Import/Export, Snapshot, AMI Copy, and S3 sharing. <br>  インポート/エクスポート、スナップショット、AMIコピー、S3共有などの移行ツールを使用して、各VPCリソースを1つのVPCから移行します。	Create a Direct Connect connection from one VPC endpoint to the other VPC. <br>  あるVPCエンドポイントから別のVPCへのダイレクトコネクト接続を作成します。	Create an OpenVPN instance in one VPC and establish an IPSec tunnel between VPCs. <br>  1つのVPCでOpenVPNインスタンスを作成し、VPC間でIPSecトンネルを確立します。
Test1-56. <p>You are designing a multi-platform web application for AWS. The application will run on EC2 instances and will be accessed from PCs, tablets, and smartphones. Supported accessing platforms are Windows. MACOS,&nbsp;IOS, and Android. Separate sticky session and SSL certificate setups are required for different platform types. Which of the following describes the most cost-effective and performance efficient architecture setup?</p> | <p> AWS用のマルチプラットフォームWebアプリケーションを設計しています。このアプリケーションはEC2インスタンス上で実行され、PC、タブレット、スマートフォンからアクセスされます。サポートされているアクセスプラットフォームはWindowsです。MACOS、IOS、Androidをサポートしています。異なるプラットフォームタイプには、個別のスティッキセッションとSSL証明書の設定が必要です。次のうち最も費用対効果の高い、パフォーマンス効率の良いアーキテクチャ設定について説明してください。</p>	sa:	Assign multiple ELBs to an EC2 instance or group of EC2 instances running the common components of the web application, one ELB for each platform type Session stickiness and SSL termination are done at the ELBs. <br>  Webアプリケーションの共通コンポーネントを実行するEC 2インスタンスまたはEC 2インスタンスのグループに複数のELBを割り当てます。各プラットフォームタイプごとに1つのELBセッションスティッキーおよびSSL終了は、ELBで行われます。|<p><span id="docs-internal-guid-1ae5823d-836e-4031-b1b1-865885760c4b"></span></p><p dir="ltr" style="">In this scenario, the main architectural considerations are (1) web application has EC2 instances running multiple platforms such as Android, iOS etc., and (2) separate sticky session and SSL certificate setups are required for different platforms.</p><p dir="ltr" style="">The best approach is to create 3 separate ELBs, per platform type.</p><br><p dir="ltr" style="">Option A is incorrect because it is not cost effective to handle such hybrid architecture.</p><p dir="ltr" style="">Option B is incorrect because if you create a single ELB for all these EC2 instances, distributing the load based on the platform type and managing the different sticky session and SSL certificate requirements will be very cumbersome and may not be feasible at all.</p><p dir="ltr" style="">Option C is incorrect because ELB cannot handle multiple SSL certificates.</p><p dir="ltr" style="">Option D is CORRECT because (a) it creates separate ELBs for each platform type, so the distribution of the load based on platform type becomes much more convenient and effective, and (b) each ELB can handle its own sticky session and SSL termination logic. See the image below:</p><p dir="ltr" style=""><img src="https://s3.amazonaws.com/awssap/1_56_1.png" alt="" width="388" height="391" role="presentation" class="img-responsive atto_image_button_middle"><br></p><br><p></p><p>For more information on ELB, please visit the below URL</p><p></p><a href="https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/faqs/</a><br><br><p></p>	Set up one ELB for all platforms to distribute load among multiple instance under it. Each EC2 instance  implements ail functionality for a particular platform. <br>  すべてのプラットフォームに対して1つのELBを設定し、複数の複数のインスタンスを負荷分散する。各EC 2インスタンスは、特定のプラットフォーム用の機能を実装します。	Set up two ELBs. The first ELB handles SSL certificates for all platforms and the second ELB handles session stickiness for all platforms for each ELB run separate EC2 instance groups to handle the web application for each platform. <br>  2つのELBを設定します。最初のELBはすべてのプラットフォームのSSL証明書を処理し、2番目のELBは各ELBのすべてのプラットフォームのセッションスティッキーを処理して、各プラットフォームのWebアプリケーションを処理する別々のEC 2インスタンスグループを実行します。	Setup a hybrid architecture to handle session state and SSL certificates on-premise and separate EC2 Instance groups running web applications for different platform types running in a VPC. <br>  VPCで実行されているさまざまなプラットフォームタイプのWebアプリケーションを実行しているEC 2インスタンスグループと、セッション状態とSSL証明書を処理するハイブリッドアーキテクチャをセットアップします。
Test1-57. <p>You're consulting for a company that is migrating its legacy application to the AWS cloud. In order to apply high availability, you've decided to implement Elastic Load Balancer and Auto Scaling services to serve traffic to this legacy application.</p><p><span style="font-size: 1rem;">The legacy application is not a standard HTTP web application but is a custom application with custom codes that is run internally for the employees of the company you are consulting.</span></p><p><span style="font-size: 1rem;">The ports required to be open are port 80 and port 8080. Which listener configuration would you create? Choose an answer from the options below:</span></p> | <p>レガシーアプリケーションをAWSクラウドに移行している企業のコンサルティングをしています。高可用性を適用するには、この従来のアプリケーションにトラフィックを提供するためにElastic Load BalancerとAuto Scalingサービスを実装することに決めました。</p> <span style = "font-size：1rem;">アプリケーションは標準のHTTP Webアプリケーションではありませんが、コンサルティングする会社の従業員のために内部で実行されるカスタムコードを持つカスタムアプリケーションです。</ span> </p> <p> <span style = "font-size： 1rem; ">開く必要があるポートはポート80とポート8080です。どのリスナー設定を作成しますか？下記のオプションから回答を選択してください：</ span> </p>	sa:	Configure the load balancer with the following ports: TCP:80 and TCP:8080 and the instance protocol to TCP:80 and TCP:8080 <br>  TCP：80、TCP：8080、インスタンスプロトコル：TCP：80、TCP：8080|<p><span id="docs-internal-guid-1ae5823d-8378-527b-65fb-6cf5a1b0bb39"></span></p><p dir="ltr" style="">The application in this scenario is a legacy based application that is built on TCP and works on ports 80 and 8080. It requires that the traffic should be routed correctly.</p><p dir="ltr" style="">Option A is CORRECT, because for the ELB to route the traffic correctly, it should be configured with ports TCP:80 and TCP 8080. For the backends as well, the ports that should be configured must be TCP:80 an TCP:8080.</p><p dir="ltr" style="">Option B, C, and D are all incorrect as both the ELB and instance protocol must be configured for ports TCP:80 and TCP:8080.</p><br><p></p><p><b>More information on ELB</b></p><p>Since the application is a custom application and not a standard HTTP application, hence you need to have the TCP ports open. Hence option A is the right option.</p><p>Before you start using Elastic Load Balancing, you must configure one or more&nbsp;<em>listeners</em>&nbsp;for your Classic Load Balancer. A listener is a process that checks for connection requests. It is configured with a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instance) connections.</p><p>Elastic Load Balancing supports the following protocols:</p> <ul> <li>HTTP</li> <li>HTTPS (secure HTTP)</li> <li>TCP</li> <li>SSL (secure TCP)</li> </ul> <p>&nbsp;</p><p>For more information on listener configuration for ELB please see the below link:</p><p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html" target="_blank">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html</a></p><p><br></p>	Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to HTTPs:80 and HTTPs:8080 <br>  HTTP：80、HTTP：8080、インスタンスプロトコル：HTTPs：80、HTTPs：8080	Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to TCP:80 and TCP:8080 <br>  HTTP：80、HTTP：8080、インスタンスプロトコル：TCP：80、TCP：8080	Configure the load balancer with the following ports: HTTP:80 and HTTP:8080 and the instance protocol to HTTP:80 and HTTP:8080 <br>  HTTP：80、HTTP：8080、インスタンスプロトコル：HTTP：80、HTTP：8080
Test1-58. <p>As an AWS Cloud Architect professional you have been instructed to share files via S3. But since these files are confidential, they cannot be accessed directly and need to be accessed via Cloudfront. Which of the below additional configurations need to be carried out to complete this requirement?</p> | <p> AWSクラウドアーキテクトのプロとして、S3経由でファイルを共有するように指示されています。しかし、これらのファイルは機密情報なので、直接アクセスすることはできず、Cloudfront経由でアクセスする必要があります。この要件を満たすために実行する必要がある追加の構成はどれですか？</p>	sa:	Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI. <br>  CloudFrontのOrigin Access Identity（OAI）を作成し、そのOAIへのS3バケット内のオブジェクトへのアクセスを許可します。|<p><br></p><p><span id="docs-internal-guid-8ca17aa6-8733-1c12-24bf-9604cad2e66d"></span></p><p dir="ltr" style="">There are two main points (1) the files should not be accessed directly via S3 as they are confidential, and (2) the files should be accessible via CloudFront.</p><p></p><p>If you want to use CloudFront signed URLs or signed cookies to provide access to objects in your Amazon S3 bucket, you probably also want to prevent users from accessing your Amazon S3 objects using Amazon S3 URLs. If users access your objects directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies, for example, control over the date and time that a user can no longer access your content and control over which IP addresses can be used to access content. In addition, if users access objects both through CloudFront and directly by using Amazon S3 URLs, CloudFront access logs are less useful because they're incomplete. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/1_58_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because it does not give CloudFront the exclusive access to S3 bucket.</span></p><p><span id="docs-internal-guid-8ca17aa6-8732-411b-ea6a-7c7bf20a2879"></span></p><p dir="ltr" style="">Option B is CORRECT because it gives CloudFront the exclusive access to S3 bucket, and prevents other users from accessing the public content of S3 directly via S3 URL.</p><p dir="ltr" style="">Option C is incorrect because you do not need to create any individual policies for each bucket.</p><p dir="ltr" style="">Option D is incorrect because (a) creating a bucket policy is unnecessary and (b) it does not prevent other users from accessing the public content of S3 directly via S3 URL.</p><br><span style="font-size: 1rem;">For more information on Origin Access Identity please see the below link</span><p></p><p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>	Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM User. <br>  CloudFrontのIDとアクセス管理（IAM）ユーザーを作成し、S3バケット内のオブジェクトへのアクセスをIAMユーザーに許可します。	Create individual policies for each bucket the documents are stored in and in that policy grant access to CloudFront only. <br>  ドキュメントが格納されているバケットごとにポリシーを作成し、そのポリシーでCloudFrontへのアクセスのみを許可します。	Create an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN). <br>  CloudFrontの配布IDをプリンシパルとして、ターゲットバケットをAmazonリソース名（ARN）としてリストするS3バケットポリシーを作成します。
Test1-59. <p></p><span id="docs-internal-guid-846a0cce-30a7-9e63-c56c-d35f8f7c1921"><p dir="ltr" style="">You are responsible for a legacy web application whose server environment is approaching end of life You would like to migrate this application to AWS as quickly as possible, since the application environment currently has the following limitations:</p><p dir="ltr" style="">The VM’s single 10GB VMDK is almost full. The virtual network interface still uses the 10Mbps driver, which leaves your 100Mbps WAN connection completely underutilized. It is currently running on a highly customized Windows VM within a VMware environment; You do not have the installation media. This is a mission critical application with an RTO (Recovery Time Objective) of 8 hours. RPO (Recovery Point Objective) of 1 hour. How could you best migrate this application to AWS while meeting your business continuity requirements?</p></span><br><p></p> | <p> </p> <span id = "docs-internal-guid-846a0cce-30a7-9e63-c56c-d35f8f7c1921"> <p dir = "ltr" style = "">レガシーWebアプリケーションは、サーバー環境の寿命が近づいているアプリケーション環境に現在の制限があるため、このアプリケーションをできるだけ早くAWSに移行したいと考えています。</p> <p dir = "ltr" style = ""> VMの単一10GBのVMDKがほぼ満杯です。仮想ネットワークインターフェイスでは10Mbpsのドライバが使用されているため、100MbpsのWAN接続が完全に利用されません。現在、VMware環境内で高度にカスタマイズされたWindows VM上で実行されています。インストールメディアはありません。これは、RTO（復旧時間目標）が8時間のミッションクリティカルなアプリケーションです。	sa:	Use the EC2 VM Import Connector for vCenter to import the VM into EC2. <br>  vCenter用のEC 2 VMインポートコネクタを使用して、VMをEC 2にインポートします。|<p><br></p><p><span id="docs-internal-guid-846a0cce-30a8-21bd-4f6e-76e2d4c973cf"></span></p><p dir="ltr" style="">Option A is CORRECT because with EC2 VM Import Connector installed, you can import the virtual machines from the VMware vSphere infrastructure into Amazon EC2 using the GUI.</p><p dir="ltr" style="">Option B is incorrect because AWS Import/Export is used to transfer large amount of data via to the AWS, not importing the VMs</p><p dir="ltr" style="">Option C is incorrect because the backup that is taken and stored on S3 may not be directly restored as an EC2 instance, and (b) it may not meet the RPO of 1 hour as this process will be slow for large number of servers.</p><p dir="ltr" style="">Option D is incorrect because (a) it is applicable to only instance store-backed Windows instance and the data on the volumes other than the root device volume does not get preserved, and (b) this API is not applicable to the Windows instances that are backed by EBS volumes.</p><p dir="ltr" style="">&nbsp;</p><p dir="ltr" style="">For more information on EC2 VM Import Connector, please see the URL below:</p><a href="https://aws.amazon.com/blogs/aws/ec2-vm-import-connector/" target="_blank">https://aws.amazon.com/blogs/aws/ec2-vm-import-connector/</a><br><br><p></p>	Use AWS Import/Export to import the VM as an ESS snapshot and attach to EC2. <br>  AWS Import / Exportを使用して、VMをESSスナップショットとしてインポートし、EC 2に添付します。	Use S3 to create a backup of the VM and restore the data into EC2. <br>  S3を使用してVMのバックアップを作成し、データをEC2に復元します。	Use the EC2’s bundle-instance API to import an image of the VM into EC2. <br>  EC 2のバンドルインスタンスAPIを使用して、VMのイメージをEC 2にインポートします。
Test1-60. <p>An administrator in your company has created a VPC with an IPv4 CIDR block 10.0.0.0/24. Now they want to expand the VPC CIDR because there is a requirement to host more resources in that VPC. Which of the below requirement can be used to accomplish this? Choose an answer from the below options.<br></p> | <p>あなたの会社の管理者がIPv4 CIDRブロック10.0.0.0/24を持つVPCを作成しました。現在、VPC CIDRを拡張するには、そのVPCでより多くのリソースをホストする必要があるためです。これを達成するために以下の要件のどれを使用できますか？以下のオプションから回答を選択します。<br> </p>	sa:	Expand your existing VPC by adding secondary IPv4 IP ranges (CIDRs) to your VPC <br>  VPCにセカンダリIPv4 IP範囲（CIDR）を追加して、既存のVPCを拡張します。|<p dir="ltr" style=""> Remember for the exam: In AWS, the CIDR of a VPC <b>can be</b> modified after its creation.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because you can change the CIDR of VPC by adding upto 4 secondary IPv4 IP CIDRs to your VPC.</p><p dir="ltr" style="">Option B is CORRECT because you can expand your existing VPC by adding up to four secondary IPv4 IP ranges (CIDRs) to your VPC.</p><p dir="ltr" style="">Option C is incorrect because deleting the subnets is unnecessary.</p><p dir="ltr" style="">Option D is incorrect because this configuration would peer the VPC, it will not alter the existing VPC’s CIDR.</p><br>For more information on VPC and its FAQs, please refer to the following link:<p></p><p><a href="https://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-now-allows-customers-to-expand-their-existing-vpcs/</a><br></p><p><a href="https://aws.amazon.com/vpc/faqs/" target="_blank">https://aws.amazon.com/vpc/faqs/</a><br></p><p><br></p>	You cannot change a VPC's size. Currently, to change the size of a VPC you must terminate your existing VPC and create a new one. <br>  VPCのサイズは変更できません。現在、VPCのサイズを変更するには、既存のVPCを終了して新しいVPCを作成する必要があります。	Delete all the subnets in the VPC and expand the VPC. <br>  VPC内のすべてのサブネットを削除し、VPCを展開します。	Create a new VPC with a greater range and then connect the older VPC to the newer one. <br>  より広い範囲で新しいVPCを作成し、古いVPCを新しいVPCに接続します。
Test1-61. <p>A company has a legacy based software which needs to be transferred to the AWS cloud. The legacy based software has a dependency on the license which is based on the MAC Address. What would be a possible solution to ensure that the legacy based software will work properly always and not lose the MAC address at any point in time? Choose an answer from the below options.<br></p> | <p>企業にはAWSクラウドに移行する必要のあるレガシーベースのソフトウェアがあります。レガシーベースのソフトウェアは、MACアドレスに基づいてライセンスに依存します。レガシーベースのソフトウェアが常に適切に動作し、いつでもMACアドレスを失わないようにするには、可能な解決策はありますか？以下のオプションから回答を選択します。<br> </p>	sa:	Use a VPC with instances having an elastic network interface attached that has a fixed MAC Address. <br>  固定MACアドレスを持つ弾性ネットワークインターフェイスが接続されているインスタンスでVPCを使用します。|<p><br></p><p><span id="docs-internal-guid-8ca17aa6-8787-e58e-d6e9-475e5ae60575"></span></p><p dir="ltr" style="">Option A is incorrect because you cannot map a static IP address to a MAC address.</p><p dir="ltr" style="">Option B is incorrect because putting license server in private subnet would not resolve the dependency on the license that is based on a MAC address.</p><p dir="ltr" style="">Option C is incorrect because MAC addresses cannot be tied to subnets.</p><p dir="ltr" style="">Option D is CORRECT because you should use Elastic Network Interface that is associated with a fixed MAC address. This will ensure that the legacy license based software would always work and not lose the MAC address any point in future.</p><p dir="ltr" style=""><br></p><p><span style="font-size: 1rem;">For more information on Elastic Network Interfaces, please refer to the URL below:</span></p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a><br></p>	Use a VPC with a private subnet for the license and a public subnet for the EC2. <br>  ライセンスにはプライベートサブネットを、EC 2にはパブリックサブネットを持つVPCを使用します。	Use a VPC with a private subnet and configure the MAC address to be tied to that subnet. <br>  プライベートサブネットでVPCを使用し、そのサブネットに結び付けるMACアドレスを設定します。	Make sure any EC2 Instance that you deploy has a static IP address that is mapped to the MAC address. <br>  静的IPアドレスを展開したEC 2インスタンスがMACアドレスにマップされていることを確認します。
Test1-62. <p><span id="docs-internal-guid-846a0cce-31bc-22f5-1095-ba9a085a11b6"></span></p><p dir="ltr" style="">You have recently joined a startup company building sensors to measure street noise and air quality in urban areas. The company has been running a pilot deployment of around 100 sensors for 3 months. Each sensor uploads 1KB of sensor data every minute to a backend hosted on AWS. During the pilot, you measured a peak or 10 IOPS on the database, and you stored an average of 3GB of sensor data per month in the database. The current deployment consists of a load-balanced auto scaled ingestion layer using EC2 instances and a PostgreSQL RDS database with 500GB standard storage. The pilot is considered a success and your CEO has managed to get the attention from some potential investors. The business plan requires a deployment of at least 100K sensors which needs to be supported by the backend. You also need to store sensor data for at least two years to be able to compare year over year improvements. To secure funding, you have to make sure that the platform meets these requirements and leaves room for further scaling.</p><br><p dir="ltr" style="">Which setup win meet the requirements?</p><br><p></p> | <p> <span id = "docs-internal-guid-846a0cce-31bc-22f5-1095-ba9a085a11b6"> </ span> </p> <p dir = "ltr" style = "">市街地の街路騒音や大気の質を測定するセンサーを開発しています。同社は、約100個のセンサーを3ヶ月間パイロット展開しています。各センサーは、毎分1KBのセンサーデータをAWSでホストされているバックエンドにアップロードします。パイロットの間に、データベースでピークまたは10 IOPSを測定し、月に平均3GBのセンサーデータをデータベースに保存しました。現在の展開は、EC2インスタンスを使用する負荷分散された自動拡張摂取レイヤーと、500GB標準ストレージを備えたPostgreSQL RDSデータベースで構成されています。パイロットは成功とみなされ、あなたのCEOはいくつかの潜在的な投資家から注目を集めることができました。事業計画では、少なくとも100,000個のセンサを配置する必要があり、バックエンドでサポートする必要があります。また、年々の改善点を比較できるようにするには、センサーデータを少なくとも2年間保管する必要があります。資金調達を確保するためには、プラットフォームがこれらの要件を満たし、さらなる拡張の余地があることを確認する必要があります。</p> <br> <p dir = "ltr" style = ""> p> <br> <p> </p>	sa:	Ingest data into a DynamoDB table and move old data to a Redshift cluster. <br>  Dynamo DBテーブルにデータを取り込み、古いデータをRedshiftクラスタに移動します。|<br><p dir="ltr" style="">Option A &amp; D are incorrect because RDS instance will not support the storage of the data for 2 years.</p><p dir="ltr" style="">Option B is incorrect because it does not mention how the ingestion of large data will be handled and how it will get scaled.</p><p dir="ltr" style="">Option C is CORRECT because (a) DynamoDB can handle the large data ingestion, and (b) Redshift can store the data for two years for comparing the improvements.</p></span><b>Note:&nbsp;</b>During the pilot deployment we have come across an average of 3GB data per month for 100 sensors. We are using a postgres sql of 500GB storage.<br>The actual requirement is 100,000 sensors which will then produce 3000GB data per month and we need to store it for 24 months which&nbsp; is not practical with the current RDS instance.&nbsp;<br>Even the 3 TB is also not enough for a period of 24 months.<br><p><a href="https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces.html" target="_blank"></a></p><p></p>	Replace the RDS instance with a 6 node Redshift cluster with 96TB of storage. <br>  96 TBのストレージを備えた6ノードRedshiftクラスタでRDSインスタンスを置き換えます。	Add an SQS queue to the ingestion layer to buffer writes to the RDS instance. <br>  処理層にSQSキューを追加して、RDSインスタンスへの書き込みをバッファします。	Keep the current architecture but upgrade RDS storage to 3TB and 10K provisioned IOPS. <br>  現在のアーキテクチャを維持しながら、RDSストレージを3TBおよび10KプロビジョニングIOPSにアップグレードします。
Test1-63. <p>There is a requirement for a web-based application hosted on AWS to talk to Redshift tables? Which of the below options best suited to have this in place from a security standpoint?</p> | <p> Redshiftテーブルと話すには、AWSでホストされているWebベースのアプリケーションが必要ですか？セキュリティの立場からこれを実現するには、以下のオプションのどれが最適ですか？</p>	sa:	Use roles that allow a web identity federated user to assume a role that allows access to the RedShift table by providing temporary credentials. <br>  Web IDフェデレーションユーザーが、一時的な資格情報を提供することによってRedShiftテーブルへのアクセスを許可する役割を引き受ける役割を使用します。|<p><span id="docs-internal-guid-8ca17aa6-87a3-9245-5c6f-501e5f71d1e7"></span></p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Tip: When a service, user, or application needs to access any AWS resource, always prefer creating an IAM Role over creating an IAM User.</p><br><p dir="ltr" style="">Option A is incorrect because embedding keys in the application to access AWS resource is not a good architectural practice as it creates security concerns.</p><p dir="ltr" style="">Option B is incorrect because HSM certificate is used by Redshift cluster to connect to the client's HSM in order to store and retrieve the keys used to encrypt the cluster databases.</p><p dir="ltr" style="">Option C is incorrect because read-only policy is insufficient and embedding keys in the application to access AWS resource is not a good architectural practice as it creates security concerns.</p><p dir="ltr" style="">Option D is CORRECT because (a) IAM role allows the least privileged access to the AWS resource, (b) web identity federation ensures the identity of the user, and (c) the user is given temporary credentials to access the AWS resource.</p><br><p></p><p>For more information on IAM policies please refer to the below link:</p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a><br></p><p></p><br><p></p><p>Next for any web application, you need to use web identity federation. Hence option D is the right option. This along with the usage of roles is highly stressed in the aws documentation.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_63_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="734" height="137"></p><p>For more information on web identity federation please refer to the below link:</p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><br></p>	Create a HSM client certificate in Redshift and authenticate using this certificate. <br>  RedshiftでHSMクライアント証明書を作成し、この証明書を使用して認証します。	Create a RedShift read-only access policy in IAM and embed those credentials in the application. <br>  RedShift読み取り専用アクセスポリシーをIAMで作成し、それらの資格情報をアプリケーションに埋め込みます。	Create an IAM user and generate encryption keys for that user. Create a policy for RedShift read-only access. Embed the keys in the application. <br>  IAMユーザーを作成し、そのユーザーの暗号化キーを生成します。RedShift読み取り専用アクセスのポリシーを作成します。アプリケーションにキーを埋め込みます。
Test1-64. <p>Your company has just set up a new central server in a VPC. There is a requirement for other teams who have their servers located in different VPC’s in the same region to connect to the central server. Which of the below options is best suited to achieve this requirement?</p> | <p>あなたの会社は、VPCに新しいセントラルサーバーを設定しました。同じ地域の異なるVPCに設置されたサーバーを持つ他のチームが、中央サーバーに接続する必要があります。次のうち、この要件を満たすのに最適なのはどれですか？</p>	sa:	Set up VPC Peering between the central server VPC and each of the teams VPCs. <br>  中央サーバVPCと各チームVPC間のVPCピアリングを設定します。|<p></p><p dir="ltr" style="display: inline !important;"><br>Option A is CORRECT because VPC Peering allows multiple VPCs to route traffic between them using the private IP addresses of the EC2 instances.</p><p dir="ltr" style="">Option B is incorrect because you cannot setup DirectConnect between different VPCs.</p><p dir="ltr" style="">Option C is incorrect because you cannot setup IPSec tunnel between different VPCs.</p><p dir="ltr" style="">Option D is incorrect as the correct solution is to use VPC Peering.</p><br><span style="font-size: 1rem;">A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single region.</span><br> <p>VPC peering needs to have the basic functionality that the CIDR’s should not overlap, hence option D is wrong.</p><p>For more information on VPC Peering, please visit the link below:</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p><p><br></p>	Set up AWS DirectConnect between the central server VPC and each of the teams VPCs. <br>  中央サーバVPCと各チームVPCの間でAWS DirectConnectを設定します。	Set up an IPSec Tunnel between the central server VPC and each of the teams VPCs. <br>  中央サーバVPCと各チームVPCの間にIPSecトンネルを設定します。	None of the above options will work. <br>  上記のオプションのどれも動作しません。
Test1-65. <p>As an AWS Administrator you are given the following requirement:</p> <p>a. MP4 files needing to be streamed publicly on the company’s new video website.<br>b. The streaming needs to be done on-demand<br>c. The video files are archived and are expected to be streamed globally, primarily on mobile devices.</p> <p>Given the above requirements which of the below options will fulfill the above requirements.</p> | <p> AWS管理者には、次の要件があります。</p> <p> a。MP4ファイルは、同社の新しいビデオサイトで公開される必要があります。<br> b。ストリーミングはオンデマンドで行う必要があります。<br> c。ビデオファイルはアーカイブされてお​​り、主にモバイルデバイス上で全世界にストリーミングされる予定です。</p> <p>上記の要件を前提とすると、上記の要件を満たすことができます。</p>	sa:	Upload the MP4 files to S3 and create an Elastic Transcoder job that transcodes the MP4 source into HLS chunks. Store the HLS output in S3 and confiure the Amazon CloudFront Web distribution for streaming the video contents. <br>  S3にHLS出力を保存し、ビデオコンテンツをストリーミングするためのAmazon CloudFront Webディストリビューションを確認します。|<p><br></p><p><span id="docs-internal-guid-8ca17aa6-87b4-f798-8d92-e0c419b57f35"></span></p><p dir="ltr" style=""><span id="docs-internal-guid-8ca17aa6-87b4-f798-8d92-e0c419b57f35" style="font-size: 1rem;"></span></p><p dir="ltr" style="display: inline !important;">Tip: In exam, if the question presents a scenario, where the media is to be streamed globally in MP4 format, on multiple platform devices, always think about using Elastic Transcoder.</p><p dir="ltr" style="">Option A is incorrect because (a) provisioning &nbsp;streaming EC2 instances is a costly solution, (b) the videos are to be delivered on-demand, not live streaming.</p><p dir="ltr" style="">Option B is incorrect because the videos are to be delivered on-demand, not live streaming. So, streaming server is not required.</p><p dir="ltr" style=""></p>For on-demand video streaming, your video content is stored on a server and viewers can watch it at any time<br>For on-demand video streaming we can deliver the video in two ways. The first method is allowing them to download the entire video and play it and the second option is streaming the video.<br>For streaming on demand videos, Use the Elastic Transcoder to convert your video files to HLS format (the most widely supported streaming protocol). This will split the video into short segments, and will also create a manifest file. The player uses the manifest file to fetch and play the segments as needed.<br><br>Option D is invalid as it is providing an option for RTMP distribution for live streaming the video contents which is not the requirement.<br><br><a href="https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/" rel="noreferrer">https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/</a><br><b><br>More information on Elastic Transcoder:</b><br><p></p><p>Amazon Elastic Transcoder manages all aspects of the media transcoding process for you transparently and automatically. There’s no need to administer software, scale hardware, tune performance, or otherwise manage transcoding infrastructure. You simply create a transcoding “job” specifying the location of your source media file and how you want it transcoded. Amazon Elastic Transcoder also provides transcoding presets for popular output formats, which means that you don’t need to guess about which settings work best on particular devices.&nbsp;</p><p>For more information on Elastic transcoder, please visit the link below:</p><p><a href="https://aws.amazon.com/elastictranscoder/" target="_blank">https://aws.amazon.com/elastictranscoder/</a></p><p>For more information on&nbsp;Using Amazon CloudFront for Video Streaming, please visit the link below:</p><p><a href="https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/" target="_blank">https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/</a><br></p><p><br></p>	Provision streaming EC2 instances which use S3 as the source for the HLS on-demand transcoding on the servers. Provision a new CloudFront download distribution with the WOWZA streaming server as the origin. <br>  S3をサーバー上のHLSオンデマンドトランスコードのソースとして使用するストリーミングEC 2インスタンスをプロビジョニングします。WOWZAストリーミングサーバーを起点として、新しいCloudFrontダウンロード配布を提供します。	Provision streaming EC2 instances which use S3 as the source for the HLS on-demand transcoding on the servers. Provision a new CloudFront streaming distribution with the streaming server as the origin. <br>  S3をサーバー上のHLSオンデマンドトランスコードのソースとして使用するストリーミングEC 2インスタンスをプロビジョニングします。ストリーミングサーバーを起点として新しいCloudFrontストリーミング配信を提供します。	Upload the MP4 files to S3 and create an Elastic Transcoder job that transcodes the MP4 source into HLS chunks. Store the HLS output in S3 and confiure the Amazon CloudFront RTMP distribution for live streaming the video contents. <br>  S3にHLS出力を保存し、ビデオコンテンツをライブストリーミングするためのAmazon CloudFront RTMPディストリビューションを確認します。
Test1-66. <p>As an AWS administrator, what is the best way to configure the NAT instance with fault tolerance? Choose the correct answer from the below options.<br></p> | <p> AWS管理者として、フォールトトレランスでNATインスタンスを設定する最も良い方法は何ですか？以下のオプションから正解を選択します。<br> </p>	sa:	Create two NAT instances in two separate public subnet; create a route from the private subnet to each NAT instance for fault tolerance <br>  2つの別個のパブリックサブネットに2つのNATインスタンスを作成します。フォールトトレランスのためにプライベートサブネットから各NATインスタンスへのルートを作成する|<p><br></p><p></p><p>Option A is incorrect because you would need at least two NAT instances for fault tolerance.</p><p>Option B is incorrect because if you put both NAT instances in a single public subnet and that subnet becomes unavailable or unreachable to the other instances, the architecture would not be fault tolerant.</p><p>Option C is CORRECT because you should place two NAT instances in two separate public subnets, and create route from instances via each NAT instance for achieving fault tolerance.</p><p>Option D is incorrect because you should not be putting the NAT instances in private subnet as they need to communicate with the internet. They should be in public subnet.</p><br><p></p><p><b>More information on NAT instances:</b></p><p>One approach to this situation is to leverage multiple NAT instances that can take over for each other if the other NAT instance should fail. This walkthrough and associated monitoring script (nat_monitor.sh) provide instructions for building a HA scenario where two NAT instances in separate Availability Zones (AZ) continuously monitor each other. If one NAT instance fails, this script enables the working NAT instance to take over outbound traffic and attempts to fix the failed instance by stopping and restarting it.</p><p>Below is a diagram for fault tolerant NAT instances.</p><p><img src="https://s3.amazonaws.com/awssap/1_66_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="475" height="347">&nbsp;</p><p>For more information on fault tolerant NAT gateways please see the below link:</p><p><a href="https://aws.amazon.com/articles/2781451301784570" target="_blank">https://aws.amazon.com/articles/2781451301784570</a></p>	Create two NAT instances in a public subnet; create a route from the private subnet to each NAT instance for fault tolerance <br>  パブリックサブネットに2つのNATインスタンスを作成します。フォールトトレランスのためにプライベートサブネットから各NATインスタンスへのルートを作成する	Create one NAT instance in a public subnet; create a route from the private subnet to that NAT instance. <br>  パブリックサブネットに1つのNATインスタンスを作成します。プライベートサブネットからそのNATインスタンスへのルートを作成します。	Create two NAT instances in two separate private subnets. <br>  2つの別個のプライベートサブネットに2つのNATインスタンスを作成します。
Test1-67. <p>Your company has an on-premises multi-tier PHP web application, which recently experienced downtime due to a large burst In web traffic due to a company announcement. Over the coming days, you are expecting similar announcements to drive similar unpredictable bursts, and are looking to find ways to quickly improve your infrastructures ability to handle unexpected increases in traffic.</p> <p>The application currently consists of a 2 tier web tier which consists of a load balancer and several Linux Apache web servers as well as a database tier which hosts a Linux server hosting a MySQL database.</p> <p>Which of the below scenario will provide full site functionality, while helping to improve the ability of your application in the short timeframe required?</p> | <p>貴社には社内マルチティアのPHP Webアプリケーションがあります。これは最近、会社の発表によるWebトラフィックの大規模なバーストのためにダウンタイムが発生しました。今後の予定では、同様の予期せぬバーストを起こすような同様のアナウンスを期待しており、予期せぬトラフィックの増加を処理するためのインフラストラクチャの機能をすばやく改善する方法を模索しています。</p> <pこのアプリケーションは現在、ロードバランサといくつかのLinux Apache Webサーバーから構成されている2層のWeb層と、MySQLデータベースをホストするLinuxサーバーをホストするデータベース層で構成されています。</ p> <p> フルサイトの機能を提供し、短期間でアプリケーションの能力を向上させるのに役立つでしょうか？</p>	sa:	Offload traffic from on-premises environment by setting up a CloudFront distribution and configure CloudFront to cache objects from a custom origin. Choose to customize your object cache behaviour, and select a TTL that objects should exist in cache. <br>  CloudFrontディストリビューションを設定して、オンプレミス環境からトラフィックをオフロードし、カスタム起点からオブジェクトをキャッシュするようにCloudFrontを設定します。 オブジェクトキャッシュの動作をカスタマイズし、オブジェクトがキャッシュに存在するはずのTTLを選択することを選択します。|<p><br></p><p></p><p>In this scenario, the major points of consideration are: (1) your application may get unpredictable bursts of traffic, (b) you need to improve the current infrastructure in shortest period possible, and (3) your web servers are on premise.</p><p>Since the time period in hand is short, instead of migrating the app to AWS, you need to consider different ways where the performance would improve without doing much modification to the existing infrastructure.</p><p><br></p><p>Option A is CORRECT because (a) CloudFront is AWS’s highly scalable, highly available content delivery service, where it can perform excellently even in case of sudden unpredictable burst of traffic, (b) the only change you need to make is make the on-premises load balancer as the custom origin of the CloudFront distribution.&nbsp;</p><p>Option B is incorrect because you are supposed to improve the current situation in shortest time possible. Migrating to AWS would be more time consuming than simply setting up the CloudFront distribution.</p><p>Option C is incorrect because you cannot host dynamic web sites on S3 bucket. Also, this option provides insufficient infrastructure set up options.</p><p>Option D is incorrect because ELB cannot do balancing between AWS EC2 instances and on-premise instances.</p><br><p></p><p><br><b>More information on CloudFront:</b></p><p>You can have CloudFront sit in front of your on-premise web environment, via a custom origin. This would protect against unexpected bursts in traffic by letting CloudFront handle the traffic from the cache, thus removing some of the load from the on-premise web servers.<br>Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. Like other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long-term. commitments or minimum fees. With CloudFront, your files are delivered to end-users using a global network of edge locations.<br>If you have dynamic content, then it is best to have the TTL set to 0.</p><p>For more information on CloudFront, please visit the below URL:<br><a href="https://aws.amazon.com/cloudfront/" target="_blank">https://aws.amazon.com/cloudfront/</a><br><br></p>	Migrate to AWS. Use VM import ‘Export to quickly convert an on-premises web server to an AMI create an Auto Scaling group, which uses the imported AMI to scale the web tier based on incoming traffic. Create an RDS read replica and setup replication between the RDS instance and on-premises MySQL server to migrate the database. <br>  AWSに移行します。 VM Import 'Exportを使用して、オンプレミスWebサーバーをAMIにすばやく変換し、インポートされたAMIを使用して着信トラフィックに基づいてWeb層を拡張するAuto Scalingグループを作成します。 RDS読み取りレプリカを作成し、RDSインスタンスとオンプレミスMySQLサーバー間でレプリケーションをセットアップして、データベースを移行します。	Create an S3 bucket and configure it tor website hosting. Migrate your DNS to Route53 using zone import and leverage Route53 DNS failover to failover to the S3 hosted website. <br>  S3バケットを作成し、ウェブサイトのホスティングを設定します。 ゾーンインポートを使用してDNSをRoute53に移行し、Route53 DNSフェイルオーバーを利用してS3ホストされたWebサイトにフェールオーバーします。Create an AMI which can be used of launch web servers in EC2. Create an Auto Scaling group which uses the AMI’s to scale the web tier based on incoming traffic. Leverage Elastic Load Balancing to balance traffic between on-premises web servers and those hosted in AWS. <br>  EC2でWebサーバーを起動するために使用できるAMIを作成します。 着信トラフィックに基づいてWeb層をスケールするためにAMIを使用するAuto Scalingグループを作成します。 Elastic Load Balancingを活用して、オンプレミスWebサーバーとAWSでホストされているWebサーバーとの間のトラフィックを分散します。
Test1-68. <p><span id="docs-internal-guid-846a0cce-31c5-e4e4-ec07-299f31267ed3">Your application provides data transformation services. Files containing data to be transformed are first uploaded to Amazon S3 and then transformed by a fleet of spot EC2 instances. Files submitted by your premium customers must be transformed with the highest priority. How should you implement such a system?</span><br></p> | <p> <span id = "docs-internal-guid-846a0cce-31c5-e4e4-ec07-299f31267ed3">アプリケーションはデータ変換サービスを提供します。変換されるデータを含むファイルは、最初にAmazon S3にアップロードされ、スポットEC2インスタンスのフリートによって変換されます。あなたのプレミアム顧客によって提出されたファイルは、最優先で変換されなければなりません。あなたはそのようなシステムをどのように実装すべきですか？</ span> <br> </p>	sa:	Use two SQS queues, one for high priority messages, the other for default priority. Transformation instances first poll the high priority queue; if there is no message, they poll the default priority queue. <br>  2つのSQSキューを使用します.1つは高優先度メッセージ用、もう1つはデフォルト優先度用です。変換インスタンスは、まず優先度の高いキューをポーリングします。メッセージがない場合は、デフォルトの優先度キューをポーリングします。|<br><p dir="ltr" style="">Option A is incorrect because using DynamoDB tables will be a very expensive solution compared to using SQS queue(s).</p><p dir="ltr" style="">Option B is incorrect because the transformation instances are spot instances which may not be up and running all the time; there are chances that they will be terminated. </p><p dir="ltr" style="">Option C is CORRECT because (a) it decouples the components of a distributed application, so the application is not impacted due to using spot instances, (b) it is a much cheaper option compared to using DynamoDB tables, and more importantly (b) it maintains a separate queue for the high priority messages which can be processed before the default priority queue. </p><p dir="ltr" style="">Option D is incorrect because the transformation instances cannot poll high-priority messages first; they just poll and can determine priority only after receiving the messages.</p><br><p dir="ltr" style=""><b>More information about implementing priority queue via SQS:</b></p></span><a href="http://awsmedia.s3.amazonaws.com/pdf/queues.pdf" target="_blank">http://awsmedia.s3.amazonaws.com/pdf/queues.pdf</a><br><br><p></p>	Use Route 53 latency based-routing to send high priority tasks to the closest transformation instances. <br>  ルート53のレイテンシに基づくルーティングを使用して、優先度の高いタスクを最も近い変換インスタンスに送信します。	Use a DynamoDB table with an attribute defining the priority level. Transformation instances will scan the table for tasks, sorting the results by priority level. <br>  優先レベルを定義する属性を持つDynamoDBテーブルを使用します。変換インスタンスは、タスクのためにテーブルをスキャンし、結果を優先順位でソートします。	Use a single SQS queue. Each message contains the priority level. Transformation instances poll high-priority messages first. <br>  単一のSQSキューを使用します。各メッセージには優先レベルが含まれています。変換インスタンスは、優先度の高いメッセージを最初にポーリングします。
Test1-69. <p>There is a requirement to split a VPC with CIDR 10.0.0.0/24 into two subnets, each supporting 128 IP addresses. Can this be done and if so, how will the allocation of IP addresses be configured? Choose the correct answer from the below options.<br></p> | <p> CIDR 10.0.0.0/24のVPCを、それぞれが128のIPアドレスをサポートする2つのサブネットに分割する必要があります。これを行うことができますか？もしそうなら、IPアドレスの割り当てはどのように設定されますか？以下のオプションから正解を選択します。<br> </p>	sa:	One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/25 (for addresses 10.0.0.128 - 10.0.0.255). <br>  1つのサブネットはCIDRブロック10.0.0.0 / 25（アドレス10.0.0.0  -  10.0.0.127用）を使用し、もう1つはCIDRブロック10.0.0.128 / 25（アドレス10.0.0.128  -  10.0.0.255用）を使用します。|<p><br></p><p>This is clearly given in the AWS documentation</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_69_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="1169" height="150"></p><p>&nbsp;</p><p>For more information on VPC and subnets please see the below link:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><p></p>	One subnet will use CIDR block 10.0.0.0/25 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.1.0/25 (for addresses 10.0.1.0 - 10.0.1.127). <br>  1つのサブネットはCIDRブロック10.0.0.0/25（アドレス10.0.0.0  -  10.0.0.127用）を使用し、もう1つはCIDRブロック10.0.1.0/25（アドレス10.0.1.0  -  10.0.1.127用）を使用します。	One subnet will use CIDR block 10.0.0.0/127 (for addresses 10.0.0.0 - 10.0.0.127) and the other will use CIDR block 10.0.0.128/255 (for addresses 10.0.0.128 - 10.0.0.255). <br>  1つのサブネットはCIDRブロック10.0.0.0 / 127（アドレス10.0.0.0  -  10.0.0.127用）を使用し、もう1つはCIDRブロック10.0.0.128 / 255（アドレス10.0.0.128  -  10.0.0.255用）を使用します。	This is not possible. <br>  これは不可能です。
Test1-70. <p>There is a requirement to change the DHCP options set with a VPC. Which of the following options do you need to take to achieve this?</p> | <p> VPCで設定されたDHCPオプションを変更する必要があります。これを達成するためには、次のうちどれを選択する必要がありますか？</p>	sa:	You must create a new set of DHCP options and associate them with your VPC. <br>  DHCPオプションの新しいセットを作成し、VPCに関連付ける必要があります。|<p>As per the AWS documentation, once you create a set of DHCP options, you cannot modify them.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_70_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="747" height="122"></p><p>&nbsp;</p><p>For more information on DHCP Options set please see the below link:</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html</a></p>	You can modify the options from the console or the CLI. <br>  オプションは、コンソールまたはCLIから変更できます。	You need to stop all the instances in the VPC. You can then change the options, and they will take effect when you start the instances. <br>  VPC内のすべてのインスタンスを停止する必要があります。その後、オプションを変更することができ、インスタンスの起動時に有効になります。	You can modify the options from the CLI only, not from the console. <br>  コンソールからではなく、CLIからのみオプションを変更できます。
Test1-71. <p>An internal auditor has been assigned to view your company’s internal AWS services. As an AWS administrator, what is the best solution to provide the auditor so that he can carry out the required auditing services? Choose the correct answer from the below options.<br></p> | <p>社内のAWSサービスを表示する内部監査人が割り当てられています。AWS管理者として、必要な監査サービスを実行できるように監査員に提供する最適なソリューションは何ですか？以下のオプションから正解を選択します。<br> </p>	sa:	Create an IAM Role with the read only permissions to access the AWS VPC infrastructure and assign that role to the auditor. <br>  AWS VPCインフラストラクチャにアクセスし、そのロールを監査人に割り当てるための読み取り専用権限を持つIAMロールを作成します。|<p>Generally, you should refrain from giving high-level permissions and give only the required permissions. In this case, option C fits well by just providing the relevant access which is required.</p><p><br></p><p>Option A is incorrect because you should create an IAM Role with the needed permissions.&nbsp;&nbsp;</p><p>Option B is incorrect because you should not give the root access as it will give the user full access to all AWS resources.</p><p>Option C is CORRECT because IAM Role gives just the minimum required permissions (read-only) to audit the VPC infrastructure to the auditor.</p><p>Option D is incorrect because you should not give the auditor full access to the VPC.</p><p><br></p><p>For more information on IAM please see the below link</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/iam/" target="_blank">https://aws.amazon.com/iam/</a></span></li></ul><p></p>	Give the auditor root access to your AWS Infrastructure. <br>  監査人にAWSインフラストラクチャへのアクセス権を与えます。	Create an IAM user tied to an administrator role. Also, provide an additional level of security with MFA <br>  管理者ロールに関連付けられたIAMユーザーを作成します。また、MFAで追加のセキュリティレベルを提供する	Create an IAM user with full VPC access but set a condition that will not allow him to modify anything if the request is from any IP other than his own. <br>  完全なVPCアクセスを持つIAMユーザーを作成しますが、要求が自分以外のIPからのものであれば、何も変更できない条件を設定します。
Test1-72. <p>There is a requirement to host a database server. This server should not be able to connect to the internet except in the case of downloading the required database patches. Which of the following solutions would be the best to satisfy all the above requirements? Choose the correct answer from the below options.<br></p> | <p>データベースサーバーをホストする必要があります。このサーバーは、必要なデータベースパッチをダウンロードする場合を除いて、インターネットに接続できません。上記のすべての要件を満たすには、次の解決策のどれがベストになるでしょうか？以下のオプションから正解を選択します。<br> </p>	sa:	Set up the database in a private subnet which connects to the Internet via a NAT instance. <br>  NATインスタンス経由でインターネットに接続するプライベートサブネットにデータベースを設定します。|<p><br></p><p>Option A is incorrect because (a) you need NAT instance or NAT gateway to be able to download the required patches, and (b) you cannot allow or deny only outbound traffic via security group as it is stateful.</p><p>Option B is incorrect because (a) you need NAT instance or NAT gateway to be able to download the required patches, and (b) you cannot allow or deny only inbound traffic via security group as it is stateful.</p><p>Option C is incorrect because you do not need to set up any local data center.</p><p>Option D is CORRECT because you should set up the data server in private subnet as it needs only the traffic from NAT instance or NAT Gateway, and not from the internet.</p><p><br></p><p>For more information on the VPC Scenario for public and private subnets please see the below link:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html</a><br><p></p>	Set up the database in a public subnet with a security group which only allows inbound traffic. <br>  受信トラフィックのみを許可するセキュリティグループを使用して、パブリックサブネットにデータベースを設定します。	Set up the database in a local data center and use a private gateway to connect the application to the database. <br>  ローカルのデータセンターにデータベースをセットアップし、プライベートゲートウェイを使用してアプリケーションをデータベースに接続します。	Set up the database in a private subnet with a security group which only allows outbound traffic. <br>  プライベートサブネットに、送信トラフィックのみを許可するセキュリティグループを使用してデータベースを設定します。
Test1-73. <p>There is a requirement for an application hosted on AWS to work with DynamoDB tables. Which of the following is the best option for the application hosted on an EC2 instance to work with the data in the DynamoDB table? Choose the correct answer from the below options.<br></p> | <p> AWSにホストされているアプリケーションでDynamoDBテーブルを操作する必要があります。DynamoDBテーブルのデータを操作するために、EC2インスタンスでホストされるアプリケーションに最適なオプションはどれですか？以下のオプションから正解を選択します。<br> </p>	sa:	Create an IAM role with the proper permission policy to communicate with the DynamoDB table. Use web identity federation, which assumes the IAM role using AssumeRoleWithWebIdentity. when the user signs in, granting temporary security credentials using STS. <br>  ユーザーがサインインすると、STSを使用して一時的なセキュリティ資格情報を付与します。私はDynamoDBテーブルとの通信で適切なアクセス許可ポリシーとの通信にIAMロールを使用しています。|<p><br></p><p>Option A is incorrect because IAM Roles are preferred over IAM Users, because IAM Users have to access the AWS resources using access and secret keys, which is a security concern.</p><p>Option B is this is not a feasible configuration.</p><p>Option C is CORRECT because it (a) creates an IAM Role with the needed permissions to connect to DynamoDB, (b) it authenticates the users with Web Identity Federation, and (c) the application accesses the DynamoDB with temporary credentials that are given by STS.</p><p>Option D is incorrect because the step to create the Active Directory (AD) server and using AD for authenticating is unnecessary and costly.</p><p><br></p><p>See the image below for more information on AssumeRoleWithWebIdentity API.</p><p><img src="https://s3.amazonaws.com/awssap/1_73_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p>For more information on web identity federation please refer to the below link</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></span></li></ul><p></p>	Create an IAM group that only gives access to your application and to the DynamoDB tables. Then, when writing to DynamoDB, simply include the unique device ID to associate the data with that specific user. <br>  アプリケーションとDynamo DBテーブルにのみアクセスできるIAMグループを作成します。次に、Dynamo DBに書き込むときに、固有のデバイスIDを指定するだけで、その特定のユーザーとデータを関連付けることができます。	Create an IAM user and assign the IAM user to a group with proper permissions to communicate with DynamoDB <br>  IAMユーザーを作成し、DynamoDBと通信するための適切な権限を持つグループにIAMユーザーを割り当てます	Create an Active Directory server and an AD user for each mobile application user. When the user signs in to the AD sign-on, allow the AD server to federate using SAML 2.0 to IAM and assign a role to the AD user which is the assumed with AssumeRoleWithSAML. <br>  ユーザーがADサインオンにサインオンすると、ADサーバーはSAML 2.0を使用してIAMにフェデレートし、AssumeRoleWithSAMLで想定されるADユーザーに役割を割り当てます。
Test1-74. <p>As an AWS Administrator, there is a requirement to monitor all changes in an AWS environment and all traffic sent to and from the environment.</p> <p>Which of the following 2 options can you take into consideration to ensure the requirements are met?</p> | <p> AWS管理者は、AWS環境内のすべての変更と環境との間で送受信されるすべてのトラフィックを監視する必要があります。</p> <p>次の2つのオプションのうち、要件は満たされていますか？</p>	ma:	x:Configure an IPS/IDS in promiscuous mode, which will listen to all packet traffic and API changes. <br>  プロミスキャスモードでIPS / IDSを設定します。このモードでは、すべてのパケットトラフィックとAPIの変更を監視します。|<p><br></p><p>Option A and B both are incorrect because promiscuous mode is not supported in AWS.</p><p>Option C is CORRECT because (a) it detects and blocks the malicious traffic coming into and out of VPC, and (b) it also leverages CloudTrail logs and CloudWatch to monitor all the changes in the environment.</p><p>option D is CORRECT because it monitors, filters, and alerts about the potentially hazardous traffic leaving from VPC.</p><p><br></p><p>Please find the below developer forums thread on the same.</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://forums.aws.amazon.com/thread.jspa?threadID=35683" target="_blank">https://forums.aws.amazon.com/thread.jspa?threadID=35683</a></span></li></ul><p></p><p>Please find the below url to a good slide deck from AWS for getting IDS in place.</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://awsmedia.s3.amazonaws.com/SEC402.pdf" target="_blank">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a></span></li></ul><p></p>	x:Configure an IPS/IDS system, such as Palo Alto Networks, using promiscous mode that monitors, filters, and alerts of all potential hazard traffic leaving the VPC. <br>  パロアルトネットワークなどのIPS / IDSシステムを、VPCから出るすべての潜在的なハザードトラフィックを監視、フィルタリング、アラートする無差別モードを使用して設定します。	o:Configure an IPS/IDS to listen and block all suspected bad traffic coming into and out of the VPC. Configure CloudTrail with CloudWatch Logs to monitor all changes within an environment. <br>  IPS / IDSを設定して、VPCとの間で送受信されるすべての疑いのあるトラフィックを監視してブロックします。CloudWatchログを使用してCloudTrailを設定して、環境内のすべての変更を監視します。	o:Configure an IPS/IDS system, such as Palo Alto Networks, that monitors, filters, and alerts of all potential hazard traffic leaving the VPC. <br>  VPCから出る潜在的な危険トラフィックをすべて監視し、フィルタし、アラートする、パロアルトネットワークなどのIPS / IDSシステムを構成します。
Test1-75. <p>A legacy application needs to be moved to AWS. But the legacy application has a dependency on multicast? Which of the below options need to be considered to ensure the legacy application works in the AWS environment?</p> | <p>レガシーアプリケーションをAWSに移動する必要があります。しかし、レガシーアプリケーションはマルチキャストに依存していますか？レガシーアプリケーションをAWS環境で確実に動作させるために、以下のオプションのどれを考慮する必要がありますか？</p>	sa:	Create a virtual overlay network that runs on the OS level of the instance. <br>  インスタンスのOSレベルで実行される仮想オーバーレイネットワークを作成します。|<p><br></p><p>Option A is incorrect because just providing ENIs between the subnets would not resolve the dependency on multicast.</p><p>Option B is CORRECT because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).</p><p>Option C is incorrect because the only option that will work in this scenario is creating a virtual overlay network.</p><p>Option D is incorrect because VPC peering and multicast are not the same.</p><p><br></p><p>For more information on Overlay Multicast in Amazon VPC, please visit the URL below:</p><p><a href="https://aws.amazon.com/articles/6234671078671125" target="_blank">https://aws.amazon.com/articles/6234671078671125</a></p>	Provide Elastic Network Interfaces between the subnets. <br>  サブネット間の弾性ネットワークインターフェイスを提供します。	All of the answers listed will help in deploying applications that require multicast on AWS. <br>  リストされているすべての回答は、AWSでのマルチキャストが必要なアプリケーションの導入に役立ちます。	Create all the subnets on a different VPC and use VPC peering between them. <br>  異なるVPC上のすべてのサブネットを作成し、それらの間のVPCピアリングを使用します。
Test1-76. <p>An auditor needs read-only access to the event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. What is the best way for giving them this sort of access?</p> | <p>監査人は、AWS管理コンソール、AWS SDK、コマンドラインツール、およびその他のAWSサービスを通じて行われたアクションを含む、AWSアカウントアクティビティのイベント履歴への読み取り専用アクセス権が必要です。このようなアクセスを与えるための最良の方法は何ですか？</p>	sa:	Enable CloudTrail logging and create an IAM user who has read-only permissions to the required AWS resources, including the bucket containing the CloudTrail logs. <br>  CloudTrailログを有効にし、IAMユーザーを作成するには、CloudTrailログを含むバケットを含む、必要なAWSリソースに対する読み取り専用のアクセス許可が必要です。|<p><br></p><p>Option A is incorrect because just creating a role in not sufficient. CloudTrail logging needs to be enabled as well.</p><p>Option B is incorrect because sending the logs via email is not a good architecture.</p><p>Option C is incorrect because granting the auditor access to AWS resources is not AWS's responsibility. It is the AWS user or account owner's responsibility.</p><p>Option D is CORRECT because you need to enable the CloudTrail logging in order to generate the logs with information about all the activities related to the AWS account and resources. It also creates an IAM user that has permissions to read the logs that are stored in the S3 bucket.</p><p><br></p><p><b style="font-size: 1rem;">More information on AWS CloudTrail</b></p><p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting.</p><p><br></p><p>For more information on CloudTrail, please visit the below URL:<br></p><p></p><a href="https://aws.amazon.com/cloudtrail/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/cloudtrail/</a><br><br><p></p>	Create an SNS notification that sends the CloudTrail log files to the auditor's email when CloudTrail delivers the logs to S3, but do not allow the auditor access to the AWS environment. <br>  CloudTrailがS3にログを配信するが、監査人がAWS環境にアクセスすることを許可していないときに、CloudTrailログファイルを監査人の電子メールに送信するSNS通知を作成する。	The company should contact AWS as part of the shared responsibility model, and AWS will grant required access to the third-party auditor. <br>  会社は共有責任モデルの一環としてAWSに連絡し、AWSは第三者監査人に必要なアクセス権を付与します。	Create a role that has the required permissions for the auditor. <br>  監査人に必要な権限を持つ役割を作成します。
Test1-77. <p>There is a requirement to have the read replica of a running MySQL RDS instance inside of AWS to an on-premise location. What is the securest way of performing this replication? Choose the correct answer from the below options.<br></p> | <p>実行中のMySQL RDSインスタンスの読み取りレプリカをAWS内で社内の場所に移動する必要があります。このレプリケーションを実行する安全な方法は何ですか？以下のオプションから正解を選択します。<br> </p>	sa:	Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. <br>  Virtual Private Cloudサービスを通じてOpenVPNまたはVPN / VGWのいずれかを使用してIPSec VPN接続を作成します。|<p><br></p><p>Option A is incorrect because SSL endpoint cannot be used here as it is used for securely accessing the database.</p><p>Option B is incorrect because replicating via EC2 instances is very time consuming and very expensive cost-wise.</p><p>Option C is incorrect because Data Pipeline is for batch jobs and not suitable for this scenario.</p><p>Option D is CORRECT because it is feasible to setup the secure IPSec VPN connection between the on premise server and AWS VPC using the VPN/Gateways.</p><p><span style="font-size: 1rem;">&nbsp;See the image below:</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_77_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="730" height="372"></p><p>For more information on VPN connections , please visit the below URL:</p><p></p><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a></span><br><p></p>	RDS cannot replicate to an on-premise database server. Instead, first configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection. <br>  RDSはオンプレミスデータベースサーバーに複製できません。代わりに、コアMySQLを使用してEC 2インスタンスにレプリケートするようにRDSインスタンスを構成し、セキュアなVPN / VPG接続を介してレプリケーションを構成します。	Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. <br>  毎晩MySQLデータをエクスポートし、HTTPSエンドポイントからS3を安全にダウンロードするデータパイプラインを作成します。	Configure the RDS instance as the master and enable replication over the open internet using a secure SSL endpoint to the on-premise server. <br>  RDSインスタンスをマスターとして構成し、セキュアSSLエンドポイントを使用してオンプレミスサーバーにインターネット経由でレプリケーションを有効にします。
Test1-78. <p>Your company produces customer commissioned one-of-a-kind skiing helmets combining high fashion with custom technical enhancements. The current manufacturing process is data rich and complex including assessments to ensure that the custom electronics and materials used to assemble the helmets are to the highest standards. Assessments are a mixture of human and automated assessments you need to add a new set of assessment to model the failure modes of the custom electronics using GPUs across a cluster of servers with low latency networking.&nbsp; What architecture would allow you to automate the existing process using a hybrid approach and ensure that the architecture can support the evolution of processes over time?</p> | <p>あなたの会社は、ハイファッションとカスタムテクニカルな機能強化を組み合わせたユニークなスキーヘルメットを顧客から製作しています。現在の製造プロセスは、ヘルメットを組み立てるために使用されるカスタム電子機器および材料が最高基準になるようにする評価を含む、データが豊富で複雑です。評価は、レイテンシの短いネットワーキングのサーバークラスタでGPUを使用してカスタムエレクトロニクスの障害モードをモデル化するための新しい一連の評価を追加するために必要な、人と自動の評価の組み合わせです。ハイブリッドアプローチを使用して既存のプロセスを自動化し、アーキテクチャがプロセスの進化を時間をかけてサポートできるようにするには、どのようなアーキテクチャが必要ですか？</p>	sa:	Use Amazon Simple Workflow (SWF) to manage assessments, movement of data & meta-data. Use an autoscaling group of G2 instances in a placement group. <br>  Amazon Simple Workflow（SWF）を使用して、アセスメント、データおよびメタデータの移動を管理します。プレースメントグループ内のG2インスタンスの自動拡張グループを使用します。|<p><br></p><p>Tip: Whenever the scenario in the question mentions about high graphical processing servers with low latency networking, always think about using G2 instances. And, when there are tasks involving human intervention, always think about using SWF.</p><p><br></p><p>Option A is incorrect because AWS Data Pipeline cannot work in hybrid approach where some of the tasks involve human actions.</p><p>Option B is CORRECT because (a) it uses G2 instances which are specialized for high graphical processing of data with low latency networking, and (b) SWF supports workflows involving human interactions along with AWS services.</p><p>Option C is incorrect because it uses C3 instances which are used for situations where compute optimization is required. In this scenario, you should be using G2 instances.</p><p>Option D is incorrect because (a) AWS Data Pipeline cannot work in hybrid approach where some of the tasks involve human actions, and (b) it uses C3 instances which are used for situations where compute optimization is required. In this scenario, you should be using G2 instances.</p><p><br></p><p><b>More information on G2 instances:</b></p><p>Using G2 instances is preferred. Hence option C and D are wrong.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/1_78_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" width="964" height="446"></p><p>For more information on Instances types, please visit the below URL:</p><p><a href="https://aws.amazon.com/ec2/instance-types/" target="_blank">https://aws.amazon.com/ec2/instance-types/</a></p><p>Since there is an element of human intervention, SWF can be used for this purpose.</p><p>For more information on SWF, please visit the below URL:</p><p><a href="https://aws.amazon.com/swf/" target="_blank">https://aws.amazon.com/swf/</a></p>	Use AWS Data Pipeline to manage movement of data & meta-data and assessments. Use an auto-scaling group of G2 instances in a placement group. <br>  AWS Data Pipelineを使用して、データとメタデータと評価の移動を管理します。プレースメントグループ内のG2インスタンスの自動スケーリンググループを使用します。	Use Amazon Simple Workflow (SWF) to manage assessments movement of data & meta-data. Use an autoscaling group of C3 instances with SR-IOV (Single Root I/O Virtualization). <br>  Amazon Simple Workflow（SWF）を使用して、データおよびメタデータのアセスメントの移動を管理します。SR-IOV（シングルルートI / O仮想化）を備えたC3インスタンスのオートスケーリンググループを使用します。	Use AWS data Pipeline to manage movement of data & meta-data and assessments. Use auto-scaling group of C3 with SR-IOV (Single Root I/O virtualization). <br>  AWSデータパイプラインを使用して、データとメタデータと評価の移動を管理します。SR-IOV（シングルルートI / O仮想化）でC3の自動スケーリンググループを使用します。
Test1-79. <p>There is a requirement for a company to transfer large amounts of data between AWS and an on-premise location. There is an additional requirement for low latency and high consistency traffic to AWS. Out of these given requirements, how would you design a hybrid architecture? Choose the correct answer from the below options.<br></p> | <p>企業がAWSとオンプレミスの場所の間で大量のデータを転送する必要があります。AWSへの低遅延と高一貫性トラフィックの追加要件があります。これらの要件から、ハイブリッドアーキテクチャをどのように設計しますか？以下のオプションから正解を選択します。<br> </p>	sa:	Provision a Direct Connect connection to an AWS region using a Direct Connect partner. <br>  Direct Connectパートナーを使用して、AWSリージョンへのダイレクトコネクト接続をプロビジョニングします。|<p><br></p><p>Tip: Whenever he scenario in the question requires the use of low latency transfer of data between AWS/VPC and on premise servers/database, always think about provisioning AWS Direct Connect.</p><p><br></p><p>Option A is CORRECT because Direct Connect creates a dedicated connection between AWS and on premises server for low latency secured transfer of data.</p><p>Option B is incorrect because setting up VPN connectivity has higher cost as well as setup and maintenance overhead compared to Direct Connect. Also, Direct Connect provides a dedicated network connection bypassing the internet. Hence it is more secure.</p><p>Option C is incorrect because setting up IPSec tunnel has setup and maintenance overhead. Also, IPSec tunnel does not guarantee the end-to-end security of the data as it uses internet.</p><p>Option D is incorrect as Direct Connect is the most suited option for this scenario.</p><p><br></p><p><b>More information on AWS Direct Connect:</b></p><p>AWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p><p>For more information on AWS direct connect, just browse to the below URL:</p><p><a href="https://aws.amazon.com/directconnect/" target="_blank">https://aws.amazon.com/directconnect/</a></p>	Create a VPN tunnel for private connectivity which increases network consistency and reduces latency. <br>  プライベート接続用のVPNトンネルを作成すると、ネットワークの一貫性が高まり、レイテンシが削減されます。	Create an IPSec tunnel for private connectivity which increases network consistency and reduces latency. <br>  プライベート接続用のIPSecトンネルを作成して、ネットワークの整合性を高め、レイテンシを削減します。	This is not possible. <br>  これは不可能です。
Test1-80. <p>You’re running an application on-premises due to its dependency on non-x86 hardware and want to use AWS for data backup. Your backup application is only able to write to POSIX-compatible block-based storage. You have 624TB of data and would like to mount it as a single folder on your file server. Users must be able to access portions of this data while the backups are taking place. What backup solution would be most appropriate for this use case?</p> | <p>非x86ハードウェアへの依存性のためにオンプレミスのアプリケーションを実行しており、データバックアップにAWSを使用したいとします。バックアップアプリケーションは、POSIX互換のブロックベースストレージにのみ書き込むことができます。あなたは624TBのデータを持っており、それをファイルサーバー上の単一のフォルダとしてマウントしたいと考えています。バックアップが行われている間、ユーザーはこのデータの一部にアクセスできる必要があります。このユースケースに最も適したバックアップソリューションは何ですか？</p>	sa:	Use Storage Gateway and configure it to use Gateway Cached volumes. <br>  Storage Gatewayを使用し、ゲートウェイキャッシュボリュームを使用するように構成します。|<p><br></p><p>Gateway-Cached volumes can support&nbsp; volumes of 1,024TB in size, where as Gateway-stored volume supports volumes of 512 TB size.</p><p>Option A is CORRECT because (a)&nbsp;it supports volumes of upto 1,024TB in size, and (b) the frequently accessed data is store on the on-premise server while the entire data is backed up over AWS.</p><p>Option B is incorrect because S3 is not ideal for POSIX compliant data.</p><p>Option C is incorrect because the data stored in Amazon Glacier is not available immediately. Retrieval jobs typically require 3–5 hours to complete; so, if you need immediate access to your data as mentioned in the question, this may not be the ideal choice.</p><p><span style="font-size: 1rem;">Option D is incorrect because gateway stored volumes can only store only 512TB worth of data.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on all of the options for storage please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits" target="_blank" style="background-color: rgb(255, 255, 255); font-size: 1rem;"><span style="font-size: 1rem;">http://docs.aws.amazon.com/st</span><wbr><span style="font-size: 1rem;">oragegateway/latest/userguide/</span><wbr><span style="font-size: 1rem;">resource-gateway-limits.html#</span><wbr><span style="font-size: 1rem;">resource-volume-limits</span></a><br><p></p>	Configure your backup software to use S3 as the target for your data backups. <br>  S3をデータバックアップのターゲットとして使用するようにバックアップソフトウェアを設定します。	Configure your backup software to use Glacier as the target for your data backups. <br>  データバックアップの対象としてGlacierを使用するようにバックアップソフトウェアを設定します。	Use Storage Gateway and configure it to use Gateway Stored volumes. <br>  Storage Gatewayを使用し、ゲートウェイ格納ボリュームを使用するように構成します。
