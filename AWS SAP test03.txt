#format:table
#title:AWS SAP
#question_count:5
#shuffle_questions:true
#shuffle_choices:true
Test3-01. <p>You have an application running on an EC2 instance which allows users to download files from a private S3 bucket using a pre-signed URL. Before generating the URL, the application should verify the existence of the file in S3. How should the application use AWS credentials to access the S3 bucket securely?</p> | <p> EC2インスタンス上でアプリケーションを実行すると、事前に署名されたURLを使用してプライベートS3バケットからファイルをダウンロードできます。URLを生成する前に、アプリケーションはS3でファイルの存在を確認する必要があります。アプリケーションがAWS認証を使用してS3バケットに安全にアクセスするにはどうすればよいですか？</p>	sa:	Create an IAM role for EC2 that allows list access to objects in the S3 bucket. Launch the instance with the role, and retrieve the role’s credentials from the EC2 Instance metadata. <br>  S3バケット内のオブジェクトへのリストアクセスを許可するEC2のIAMロールを作成します。ロールを使用してインスタンスを起動し、EC2インスタンスのメタデータからロールの資格情報を取得します。|<p dir="ltr" style=""><br></p><p dir="ltr" style="">An IAM&nbsp;role&nbsp;is similar to a user. In that, it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have any credentials (password or access keys) associated with it. Instead, if a user is assigned to a role, access keys are created dynamically and provided to the user.</p><p dir="ltr" style="">You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.</p><p dir="ltr" style="">Whenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User. The reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service. This has security concerns. Whereas, when an IAM Role is created, it has all the necessary policies attached to it. So, the use of access key and secret key is not needed. This is the preferred approach.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because you should not use the account access keys , instead you should use the IAM Role.</p><p dir="ltr" style="">Option B is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p dir="ltr" style="">Option C is CORRECT because, (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.</p><p dir="ltr" style="">Option D is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">For more information on IAM roles, please visit the below URL:</p><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p></span><br><br><p></p>	Create an IAM user for the application with permissions that allow list access to the S3 bucket launch the instance as the IAM user and retrieve the IAM user’s credentials from the EC2 instance user data. <br>  S3バケットへのリストアクセスがIAMユーザーとしてインスタンスを起動し、EC2インスタンスユーザーデータからIAMユーザーの資格情報を取得する権限を持つアプリケーション用のIAMユーザーを作成します。	Use the AWS account access Keys. The application retrieves the credentials from the source code of the application. <br>  AWSアカウントのアクセスキーを使用します。アプリケーションは、アプリケーションのソースコードから資格情報を取得します。	Create an IAM user for the application with permissions that allow list access to the S3 bucket. The application retrieves the IAM user credentials from a temporary directory with permissions that allow read access only to the application user. <br>  アプリケーションは、アプリケーションを取得し、そのアプリケーションのユーザーのみに読み取りアクセスを許可するアクセス許可を使用して、一時ディレクトリからIAMユーザーの資格情報を取得します。
Test3-02. <p></p><span id="docs-internal-guid-08d78d15-335c-4f17-f2e0-f81f3762aaf9"><p dir="ltr" style="">You’ve been brought in as solutions architect to assist an enterprise customer with their migration of an e-commerce platform to Amazon Virtual Private Cloud (VPC). The previous architect has already deployed a 3- tier VPC. The configuration is as follows:&nbsp;</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">VPC vpc-2f8bc447</p><p dir="ltr" style="">IGW ig-2d8bc445</p><p dir="ltr" style="">NACL acl-208bc448</p><p dir="ltr" style="">Subnets and Route Tables:</p><p dir="ltr" style="">Web server’s subnet-258bc44d</p><p dir="ltr" style="">Application server’s subnet-248bc44c</p><p dir="ltr" style="">Database server’s subnet-9189c6f9</p><p dir="ltr" style="">Route Tables:</p><p dir="ltr" style="">rtb-218bc449</p><p dir="ltr" style="">rtb-238bc44b</p><p dir="ltr" style="">Associations:</p><p dir="ltr" style="">Subnet-258bc44d: rtb-218bc449</p><p dir="ltr" style="">Subnet-248bc44c: rtb-238bc44b</p><p dir="ltr" style="">Subnet-9189c6f9: rtb-238bc44b</p><p dir="ltr" style=""><br></p><p dir="ltr" style="">You are now ready to begin deploying EC2 instances into the VPC. Web servers must have direct access to the internet Application and database servers cannot have direct access to the internet. Which configuration below will allow you the ability to remotely administer your application and database servers, as well as allow these servers to retrieve updates from the Internet?</p></span><br><p><br></p> | あなたはソリューションアーキテクトとして迎え入れられました。<p> </p> <span id = "docs-internal-guid-08d78d15-335c-4f17-f2e0-f81f3762aaf9"> <p dir = "ltr" style = "">エンタープライズ顧客がAmazon仮想プライベートクラウド（VPC）に電子商取引プラットフォームを移行するのを支援します。以前のアーキテクトは既に3層VPCを導入しています。構成は次のとおりです。＆nbsp; <p dir = "ltr" style = ""> </p> <p dir = "ltr" style = ""> VPC vpc-2f8bc447 </ p > <p dir = "ltr" style = ""> IGW ig-2d8bc445 </p> <p dir = "ltr" style = ""> NACL acl-208bc448 </p> <p dir = "ltr" style = "">サブネットとルートテーブル：</p> <p dir = "ltr" style = ""> style = ""> EC2インスタンスをVPCに配備する準備が整いました。Webサーバーはインターネットに直接アクセスする必要がありますアプリケーションとデータベースサーバーはインターネットに直接アクセスできません。下記のどの設定を使用すると、アプリケーションサーバーとデータベースサーバーをリモートで管理したり、インターネットから更新プログラムを取得できるようになりますか？</p> </ span> <br> <p> <br> </ p >	sa:	Create a Bastion and NAT instance in subnet-258bc44d and add a route from rtb-238bc44b to the NAT instance. <br>  bastionとNATインスタンスをサブネット-258bc44dに作成し、rtb-238bc44bからNATインスタンスにルートを追加します。|<br><p dir="ltr" style="">Option A is incorrect because the route should be pointing to NAT.</p><p dir="ltr" style="">Option B is incorrect because adding IGW to route rtb-238bc44b would expose the application and database server to internet. Bastion and NAT should be in public subnet.</p><p dir="ltr" style="">Option C is incorrect because the route should point to NAT and not Internet Gateway else it would be internet accessible.</p><p dir="ltr" style="">Option D is CORRECT because Bastion and NAT should be in the public subnet. As Web Server has direct access to Internet, the subnet subnet-258bc44d should be public and Route rtb-2i8bc449 pointing to IGW. Route rtb-238bc44b for private subnets should point to NAT for outgoing internet access.</p></span><br><p></p>	Add a route from rtb-238bc44b to igw-2d8bc445 and add a bastion and NAT instance within Subnet-248bc44c. <br>  rtb-238bc44bからigw-2d8bc445へのルートを追加し、Subnet-248bc44c内にバスションとNATインスタンスを追加します。	Create a Bastion and NAT Instance in subnet-258bc44d. Add a route from rtb-238bc44b to igw-2d8bc445, and a new NACL that allows access between subnet-258bc44d and subnet-248bc44c. <br>  サブネット内でBastionおよびNATインスタンスを作成する-258 bc 44 d。	Create a bastion and NAT Instance in subnet-258bc44d and add a route from rtb-238bc44b to subnet-258bc44d. <br>  bastionとNATインスタンスをサブネット258 bc 44 dに作成し、rtb  -  238 bc 44 bからサブネット258 bc 44 dにルートを追加します。
Test3-03. <p>Which of the following are the best techniques to avoid DDoS attacks for your infrastructure hosted on AWS?</p><p>Choose 3 options from the below:</p> | <p> AWSでホストされているインフラストラクチャのDDoS攻撃を避けるための最良の方法はどれですか？</p> <p>以下の3つのオプションを選択してください：</p>	ma:	x:Add multiple Elastic Network Interfaces (ENIs) to each EC2 instance to increase the network bandwidth. <br>  各EC2インスタンスに複数のElastic Network Interface（ENI）を追加して、ネットワーク帯域幅を拡大します。|<p><br></p><p></p><p>This question is asking you to select some of the most recommended and widely used DDoS mitigation techniques.</p><p><b>What is a DDoS Attack?</b></p><p>A Distributed Denial of Service (DDoS) attack is an attack orchestrated by distributed multiple sources that makes your web application unresponsive and unavailable for the end users.</p><p><b>DDoS Mitigation Techniques</b></p><p>Some of the recommended techniques for mitigating the DDoS attacks are&nbsp;</p><p>(i) build the architecture using the AWS services and offerings that have the capabilities to protect the application from such attacks. e.g. CloudFront, WAF, Autoscaling, Route53, VPC etc.</p><p>(ii) defend the infrastructure layer by over-provisioning capacity, and deploying DDoS mitigation systems.</p><p>(iii) defend the application layer by using WAF, and operating at scale by using autoscale so that the application can withstand the attack by scaling and absorbing the traffic.</p><p>(iv) minimizing the surface area of attack</p><p>(v) obfuscating the AWS resources</p><p>Option A is incorrect because ENIs do not help in increasing the network bandwidth.</p><p><br></p><p>Option B is incorrect because having dedicated instances performing at maximum capacity will not help mitigating the DDoS attack. What is needed is instances behind auto-scaling so that the traffic can be absorbed while actions are being taken on the attack and the application can continue responding to the clients.</p><p>Option C is CORRECT because (a) CloudFront is AWS managed service and it can scale automatically, (b) helps absorbing the traffic, and (c) it can help putting restriction based on geolocation. i.e. if the attack is coming from IPs from specific location, such requests can be blocked.</p><p>Option D is CORRECT because (a) ELB helps distributing the traffic to the instances that are part of auto-scaling (helps absorbing the traffic), and (b) Amazon RDS is an Amazon managed service which can withstand the DDoS attack.</p><p>Option E is CORRECT because CloudWatch can help monitoring the network traffic as well as CPU Utilization for suspicious activities.</p><p>Option F is incorrect because adding and removing rules of firewall is not going to mitigate the DDoS attack.</p><p><br></p><p>It is very important to read the AWS Whitepaper on Best Practices for DDoS Resiliency.</p>  <a href="https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a><br><p></p><ul></ul><p></p>	x:Use dedicated instances to ensure that each instance has the maximum performance possible. <br>  専用インスタンスを使用して、各インスタンスが最大のパフォーマンスを発揮できるようにします。	o:Use an Amazon CloudFront distribution for both static and dynamic content. <br>  静的コンテンツと動的コンテンツの両方にAmazon CloudFrontディストリビューションを使用します。	o:Use an Elastic Load Balancer with auto scaling groups for Web servers and Application servers. <br>  Webサーバーとアプリケーションサーバーの自動スケーリンググループを持つElastic Load Balancerを使用します。	o:Add alert Amazon CloudWatch to look for high Network in and CPU utilization. <br>  Amazon CloudWatchにアラートを追加して、ネットワークとCPU使用率の高いネットワークを探します。	x: Create processes and capabilities to quickly add and remove rules to the instance OS firewall. <br>  インスタンスOSファイアウォールにルールを迅速に追加および削除するためのプロセスと機能を作成します。
Test3-04. <p>If you want to deliver private content to users from an S3 bucket, which of the below options is the most feasible to fulfill this requirement?</p><p>Choose an option from the below:</p> | </p> <p> S3バケットからユーザーにプライベートコンテンツを配信する場合は、以下のうちどれがこの要件を満たすのが最も適していますか？</p>	sa:	Use pre-signed URL <br>  事前に署名されたURLを使用する|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is CORRECT because a pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object. That is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.</span></p><p>Option B, C, and D are all incorrect.</p><p>&nbsp;&nbsp;</p><p>For more information on pre-signed URLs, please refer to the below URL</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html</a><br><p></p>	Use EC2 to deliver content from the S3 bucket <br>  EC2を使用してS3バケットからコンテンツを配信する	?Use SQS to deliver content from the S3 bucket <br>  ？SQSを使用してS3バケットからコンテンツを配信する	None of the above <br>  上記のどれでもない
Test3-05. <p>Which is the best option to avoid SQL Injection attacks against your infrastructure in AWS?</p> | <p> AWSのインフラストラクチャに対するSQLインジェクション攻撃を避ける最良の方法はどれですか？</p>	sa:	Add a WAF tier by creating a new ELB and an AutoScaling group of EC2 Instances running a host-based WAF. They would redirect Route 53 to resolve to the new WAF tier ELB. The WAF tier would pass the traffic to the current web tier. The web tier Security Groups would be updated to only allow traffic from the WAF tier Security Group <br>  ホストベースのWAFを実行している新しいELBと自動スケーリンググループのEC2インスタンスを作成して、WAF層を追加します。彼らはRoute 53を新しいWAF層ELBに解決するようにリダイレクトします。WAF層は、トラフィックを現在のWeb層に渡します。Web層セキュリティグループは、WAF層セキュリティグループからのトラフィックのみを許可するように更新されます|<p><br></p><p></p><p>In such scenarios where you are designing a solution to prevent the DDoS attack, always think of using Web Access Firewall (WAF).</p><p>AWS WAF is a web application firewall that helps protect your web&nbsp;applications from common web exploits that could affect application&nbsp;availability, compromise security, or consume excessive resources. AWS&nbsp;WAF gives you control over which traffic to allow or block to your web&nbsp;applications by defining customizable web security rules. You can use AWS&nbsp;WAF to create custom rules that block common attack patterns, such as SQL&nbsp;injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond&nbsp;quickly to changing traffic patterns.&nbsp;</p><p><br></p><p>Option A is incorrect because, although this option could work, the setup is very complex and it not a cost effective solution.</p><p>Option B is incorrect because, (a) even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and (b) it does not prevent the attack from the new source of threat.</p><p>Option C is CORRECT because (a) WAF Tiers acts as the first line of defense, it filters out the known sources of attack and blocks common attack patterns, such as SQL&nbsp;injection or cross-site scripting, (b) the ELB of the application is not exposed to the attack, and most importantly (c) this pattern - known as "WAF Sandwich" pattern - has WAF layer with EC2 instances are placed between two ELBs - one that faces the web, receives all the traffic, and sends them to WAF layer to filter out the malicious requests, and sends the filtered non-malicious requests, another ELB - which receives the non-malicious requests and send them to the EC2 instances for processing. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/3_5_1.png" alt="" width="1261" height="702" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p>Option D is incorrect because there is no such thing as&nbsp;Advanced Protocol Filtering feature for ELB.</p><p><br></p><p>For more information on WAF, please visit the below URL:</p>  <a href="https://aws.amazon.com/waf/">https://aws.amazon.com/waf/</a><br><p></p><ul></ul><p></p>	Create NACL rules for the subnet hosting the application <br>  アプリケーションをホストするサブネット用のNACLルールを作成する	Create a DirectConnect connection so that your have a dedicated connection line. <br>  専用の接続回線を持つようにDirectConnect接続を作成します。	Remove all but TLS 1 & 2 from the web tier ELB and enable Advanced Protocol Filtering. This will enable the ELB itself to perform WAF functionality. <br>  Web層ELBからTLS 1および2を除くすべてを削除し、Advanced Protocol Filteringを有効にします。これにより、ELB自体がWAF機能を実行できるようになります。
Test3-06. <p>Which of the following is a reliable and durable logging solution to track changes made to your AWS resources?</p> | <p>あなたのAWSリソースに対する変更を追跡するための、信頼性と耐久性のあるロギングソリューションはどれですか？</p>	sa:	Create a new CloudTrail trail with one new S3 bucket to store the logs and with the global services option selected. Use IAM roles, S3 bucket policies and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs. <br>  1つの新しいS3バケットを使用して新しいCloudTrailトレイルを作成し、ログを保存し、グローバルサービスオプションを選択します。ログを保存するS3バケットでIAMロール、S3バケットポリシー、マルチファクタ認証（MFA）削除を使用します。|<p><br></p><p></p><p>For the scenarios where the application is tracking (or needs to track) the changes made by any AWS service, resource, or API, always think about AWS CloudTrail service.</p><p>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticated AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets.&nbsp;</p><p>The most important points in this question are (a)&nbsp;S3 bucket with global services option enabled, (b) Data integrity, and (c) Confidentiality.</p><p><br></p><p>Option A is CORRECT because (a) it uses AWS CloudTrail with Global Option enabled, (b) a single new S3 bucket and IAM Roles so that it has the confidentiality, (c)&nbsp; MFA on Delete on S3 bucket so that it maintains the data integrity. See the AWS CloudTrail setting below which sets the Global Option.</p><p><img src="https://s3.amazonaws.com/awssap/3_6_1.png" alt="" width="808" height="184" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><!--[endif]--></p><p>Options B is incorrect because (a) although&nbsp;it uses AWS CloudTrail,&nbsp;the Global Option is not enabled, and (b) SNS notifications can be a overhead in this situation.</p><p>Option C is incorrect because (a) as an existing S3 bucket is used, it may already be accessed to the user, hence not maintaining the confidentiality, and (b) it is not using IAM roles.</p><p>Option D is incorrect because (a)&nbsp;although&nbsp;it uses AWS CloudTrail,&nbsp;the Global Option is not enabled, and (b) three S3 buckets are not needed.</p><p><br></p><p>For more information on Cloudtrail, please visit the below URL:</p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events" target="_blank">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p>  <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a><br><p></p><ul></ul><p></p>	Create a new CloudTrail with one new S3 bucket to store the logs. Configure SNS to send log file delivery notifications to your management system. Use IAM roles and S3 bucket policies on the S3 bucket that stores your logs <br>  1つの新しいS3バケットで新しいCloudTrailを作成し、ログを保存します。ログファイル配信通知を管理システムに送信するようにSNSを設定します。ログを保存するS3バケットにIAMロールとS3バケットポリシーを使用する	Create a new CloudTrail trail with an existing S3 bucket to store the logs and with the global services option selected. Use S3 ACLs and Multi Factor Authentication (MFA) Delete on the S3 bucket that stores your logs. <br>  既存のS3バケットを使用して新しいCloudTrailトレイルを作成して、ログを保存し、グローバルサービスオプションを選択します。S3 ACLとMulti Factor Authentication（MFA）を使用して、ログを保存するS3バケットを削除します。	Create three new CloudTrail trails with three new S3 buckets to store the logs one for the AWS Management console, one for AWS SDKs and one for command line tools. Use IAM roles and S3 bucket policies on the S3 buckets that store your logs. <br>  AWS管理コンソール用、AWS SDK用、コマンドラインツール用のログを格納する3つの新しいS3バケットを備えた3つの新しいCloudTrailトレイルを作成します。ログを格納するS3バケットにIAMロールとS3バケットポリシーを使用します。
Test3-07. <p>A company has recently started using a custom SaaS-based solution that is hosted on AWS. There is a requirement for the SaaS solution to access AWS resources. Which of the following would meet the requirement for enabling the SaaS solution to work with AWS resources in the most secured manner?</p> | <p>ある会社が最近、AWSでホストされているカスタムSaaSベースのソリューションを使用し始めました。SaaSソリューションがAWSリソースにアクセスするための要件が​​あります。SaaSソリューションがAWSリソースと最も安全に連携できるようにするための要件を満たすのは次のうちどれですか？</p>	sa:	Create an IAM role for cross-account access allows the SaaS provider’s account to assume the role and assign it a policy that allows only the actions required by the SaaS application. <br>  クロスアカウントアクセス用のIAMロールを作成すると、SaaSプロバイダのアカウントはその役割を引き受け、SaaSアプリケーションが要求するアクションのみを許可するポリシーを割り当てることができます。|<p><br></p><p></p><p>When a user, a resource, an application, or any service needs to access any AWS service or resource, always prefer creating appropriate role that has least privileged access or only required access, rather than using any other credentials such as keys.</p><p><br></p><p>Option A is incorrect because you should never share your access and secret keys.</p><p>Option B is incorrect because (a) when a user is created, even though it may have the appropriate policy attached to it, its security credentials are stored in the EC2 which can be compromised, and (b) creation of the appropriate role is always the better solution rather than creating a user.</p><p>Option C is CORRECT because AWS role creation allows cross-account access to the application to access the necessary resources. See the image and explanation below:</p><p>Many SaaS platforms can access AWS resources via a Cross-account access created in AWS. If you go to Roles in your identity management, you will see the ability to add a cross-account role.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_7_1.png" alt="" width="1140" height="476" role="presentation" class="img-responsive atto_image_button_text-bottom"><!--[endif]--></p><p>Option D is incorrect because the role is to be assigned to the application and it's resources, not the EC2 instances.</p><p><br></p><p>For more information on the cross-account role, please visit the&nbsp;below URL:</p>  <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a><br><p></p><ul></ul><p></p>	Create an IAM user within the enterprise account assign a user policy to the IAM user that allows only the actions required by the SaaS application. Create a new access and secret key for the user and provide these credentials to the SaaS provider. <br>  エンタープライズアカウント内でIAMユーザーを作成すると、SaaSアプリケーションが要求するアクションのみを許可するユーザーポリシーをIAMユーザーに割り当てます。ユーザの新しいアクセスと秘密鍵を作成し、これらの資格情報をSaaSプロバイダに提供します。	From the AWS Management Console, navigate to the Security Credentials page and retrieve the access and secret key for your account. <br>  AWS Management Consoleから、Security Credentialsページに移動し、アカウントのアクセスと秘密鍵を取得します。	Create an IAM role for EC2 instances, assign it a policy that allows only the actions required tor the Saas application to work, provide the role ARM to the SaaS provider to use when launching their application instances. <br>  EC2インスタンス用のIAMロールを作成し、Saasアプリケーションに必要なアクションだけを実行できるポリシーを割り当て、アプリケーションインスタンスを起動するときに使用するARMの役割をSaaSプロバイダに提供します。
Test3-08. <p>Your company has recently extended its data center into a VPC on AWS to add burst computing capacity as needed. Members of your Network Operations Center need to be able to go to the AWS Management Console and administer Amazon EC2 instances as necessary. You don’t want to create new IAM users for each member and make those users sign in again to the AWS Management Console. Which option below will meet the needs of your NOC members?</p> | <p>貴社は最近、データセンターをAWS上のVPCに拡張し、必要に応じてバーストコンピューティング能力を追加しました。Network Operations Centerのメンバーは、必要に応じてAWS Management Consoleにアクセスし、Amazon EC2インスタンスを管理できる必要があります。メンバーごとに新しいIAMユーザーを作成し、そのユーザーがAWS Management Consoleに再度サインインすることは望ましくありません。下記のどのオプションがあなたのNOCメンバーのニーズを満たすのですか？</p>	sa:	Use your on-premises SAML 2.0-compliant identity provider (IDP) to grant the members federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint. <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバーにAWSシングルサインオン（SSO）エンドポイント経由でAWS管理コンソールへのフェデレーションアクセスを許可します。|<p><br></p><p></p><p>This scenario has two requirements: (a) temporary access to AWS resources be given to certain users or application (NOC members in this case), and (b) you are not supposed to create new IAM users for the NOC members to log into AWS console.&nbsp;</p><p>This scenario is handled by a concept named "Federated Access". Read this for more information on federated access:&nbsp;<a href="https://aws.amazon.com/identity/federation/" target="_blank">https://aws.amazon.com/identity/federation/</a>&nbsp;.</p><p>Read this article for more information on how to establish the federated access to the AWS resources:</p><p><a href="https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/" target="_blank">https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/</a></p><p><br></p><p>Option A is incorrect because OAuth 2.0 is not applicable in this scenario as we are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google etc.</p><p>Option B is incorrect because we are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google etc.</p><p>Option C is CORRECT because (a) it gives a federated access to the NOC members to AWS resources by using SAML 2.0 identity provider, and (b) it uses on-premise single sign on (SSO) endpoint to authenticate users and gives them access tokens prior to providing the federated access.</p><p>Option D is incorrect because, even though it uses SAML 2.0 identity provider, one of the requirements is not to let users sign in to AWS console using any security credentials.</p>  See this diagram that explains the Federated Access using SAML 2.0.&nbsp;<br><p></p><p><img src="https://s3.amazonaws.com/awssap/3_8_1.png" alt="" width="1246" height="717" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><ul></ul><p></p>	Use web Identity Federation to retrieve AWS temporary security credentials to enable your members to sign in to the AWS Management Console <br>  Web Identity Federationを使用してAWSの一時的なセキュリティ資格情報を取得し、メンバーがAWS管理コンソールにサインインできるようにする	Use OAuth 2.0 to retrieve temporary AWS security credentials to enable your members to sign in to the AWS Management Console. <br>  OAuth 2.0を使用して、メンバがAWS管理コンソールにサインインできるように、一時的なAWSセキュリティ資格情報を取得します。	Use your on-premises SAML 2.0-compliant identity provider (IDP) to retrieve temporary security credentials to enable members to sign in to the AWS Management Console. <br>  オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IDP）を使用して、メンバがAWS管理コンソールにサインインできるように一時的なセキュリティ資格情報を取得します。
Test3-09. <p>You have an application running on an EC2 Instance accesses an SQS queue. How should the application use AWS credentials to access the SQS queue securely?</p> | <p> EC2インスタンス上で実行されているアプリケーションがSQSキューにアクセスしています。アプリケーションがSQSキューに安全にアクセスするためにAWS認証情報をどのように使用するのですか？</p>	sa:	Create an IAM role for EC2 that allows access to the SQS queue. Launch the instance with the role, and retrieve the role’s credentials from the EC2 Instance metadata <br>  SQSキューへのアクセスを許可するEC2のIAMロールを作成します。ロールを使用してインスタンスを起動し、EC2インスタンスのメタデータからロールの資格情報を取得します|<p><br></p><p></p><p>An IAM&nbsp;<i>role</i>&nbsp;is similar to a user. In that, it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have any credentials (password or access keys) associated with it. Instead, if a user is assigned to a role, access keys are created dynamically and provided to the user.</p><p>You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.</p><p>Whenever the question presents you with a scenario where an application, user, or service wants to access another service, always prefer creating IAM Role over IAM User. The reason being that when an IAM User is created for the application, it has to use the security credentials such as access key and secret key to use the AWS resource/service. This has security concerns. Whereas, when an IAM Role is created, it has all the necessary policies attached to it. So, the use of access key and secret key is not needed. This is the preferred approach.</p><p><br></p><p>Option A is incorrect because you should not use the account access keys , instead you should use the IAM Role.</p><p>Option B is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p>Option C is CORRECT because, (a) it creates the IAM Role with appropriate permissions, and (b) the application accesses the AWS Resource using that role.</p><p>Option D is incorrect because instead of IAM User, you should use the IAM Role. See the explanation given above.</p><p><br></p><p>For more information on IAM roles, please visit the below URL:</p>  <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a><br><p></p><ul></ul><p></p>	Create an IAM user for the application with permissions that allow access to the SQS queue launch the instance as the IAM user and retrieve the IAM user’s credentials from the EC2 instance user data <br>  SQSキューへのアクセスを許可する権限を持つアプリケーション用のIAMユーザーを作成して、IAMユーザーとしてインスタンスを起動し、EC2インスタンスユーザーデータからIAMユーザーの資格情報を取得します	Use the AWS account access Keys the application retrieves the credentials from the source code of the application. <br>  AWSアカウントのアクセスキーを使用するアプリケーションは、アプリケーションのソースコードから資格情報を取得します。	Create an IAM user for the application with permissions that allows access to the SQS queue. The application retrieves the IAM user credentials from a temporary directory with permissions that allow access only to the application user. <br>  SQSキューへのアクセスを許可する権限を持つアプリケーションのIAMユーザーを作成します。アプリケーションは、アプリケーションユーザーのみにアクセスを許可する権限を持つ一時ディレクトリからIAMユーザーの資格情報を取得します。
Test3-10. <p>Which of the below-mentioned methods is the best to stop a series of attacks coming from a set of determined IP ranges?</p> | <p>特定のIP範囲から一連の攻撃を阻止するには、以下の方法のどれがベストですか？</p>	sa:	Create an inbound NACL (Network Access control list) associated with the web tier subnet with deny rules to block the attacking IP addresses <br>  攻撃者のIPアドレスをブロックする拒否ルールを持つWeb層サブネットに関連付けられた着信NACL（ネットワークアクセス制御リスト）を作成する|<p><br></p><p></p><p>In this scenario, the attack is coming from a set of certain IP addresses over specific port from a specific country. You are supposed to defend against this attack.&nbsp;</p><p>In such questions, always think about two options: Security groups and Network Access Control List (NACL). Security Groups operate at the individual instance level, whereas NACL operates at subnet level. You should always fortify the NACL first, as it is encounter first during the communication with the instances in the VPC.</p><p><br></p><p>Option A is incorrect because IP addresses cannot be blocked using route table or IGW.</p><p>Option B is incorrect because changing the EIP of NAT instance cannot block the incoming traffic from a particular IP address.</p><p>Option C is incorrect because (a) you cannot deny port access using security groups, and (b) by default all requests are denied; you open access for particular IP address or range. You cannot deny access for particular IP addresses using security groups.</p>  Option D is CORRECT because (a) you can add deny rules in NACL and block access to certain IP addresses. See an example below:<br><p></p><p><img src="https://s3.amazonaws.com/awssap/3_10_1.png" alt="" width="859" height="368" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><ul></ul><p></p>	Change the EIP (Elastic IP Address) of the NAT instance in the web tier subnet and update the Main Route ?Table with the new EIP <br>  Web層サブネット内のNATインスタンスのEIP（Elastic IP Address）を変更し、新しいEIPを使用してMain Route？Tableを更新します	Create 15 Security Group rules to block the attacking IP addresses over port 80 <br>  ポート80を介して攻撃するIPアドレスをブロックする15のセキュリティグループルールを作成する	Create a custom route table associated with the web tier and block the attacking IP addresses from the IGW (internet Gateway) <br>  Web層に関連付けられたカスタムルートテーブルを作成し、IGW（インターネットゲートウェイ）からの攻撃IPアドレスをブロックします。
Test3-11. <p>A company has the requirement to analyze the clickstreams from a web application in real time? Which of the&nbsp;<span style="font-size: 1rem;">below AWS services will fulfill this requirement?</span></p> | <p>会社はリアルタイムでウェブアプリケーションからのクリックストリームを分析する必要がありますか？AWSサービスの下の＆nbsp; <span style = "font-size：1rem;">のどちらがこの要件を満たしますか？</ span> </p>	sa:	Amazon Kinesis <br>  Amazon Kinesis|<p><br></p><p>Kinesis Data Streams are extremely useful for rapid and continuous data intake and aggregation. The type of data used includes IT infrastructure log data, application logs, social media, market data feeds, and web clickstream data. Because the response time for the data intake and processing is in real time, the processing is typically lightweight.<br></p><p><br></p><p>Option A is CORRECT because Amazon Kinesis Data Streams are very useful in processing website clickstreams in real time, and then analyzing using multiple different Kinesis Data Streams applications running in parallel.</p><p>Option B is incorrect because SQS is used for storing messages/work items for asynchronous processing in the application, not the real time processing of clickstream data.</p><p>Option C is incorrect because Redshift is a data warehouse solution that is used for Online Analytical Processing of data, and where&nbsp;complex analytic queries against petabytes of structured data. It is not used in real time processing of clickstream data.</p><p>Option D is incorrect because AWS IoT is a platform that enables you to connect devices to AWS Services and other devices, secure data and interactions, process and act upon device data. It does not do the real time processing of the clickstream data. However, it can leverage Amazon Kinesis Analytics to do it.</p><p><br></p><p>For more information on Kinesis , please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/streams/latest/dev/introduction.html</a><br><br><p></p>	Amazon SQS <br>  Amazon SQS	Amazon Redshift <br>  Amazon Redshift	AWS IoT <br>  AWS IoT
Test3-12. <p>A customer is deploying an SSL enabled Web application on AWS and would like to implement a separation of roles between the EC2 service administrators that are entitled to login to Instances as well making API calls and the security officers who will maintain and have exclusive access to the application's X.509 certificate that contains the private key. Which configuration option would satisfy the above requirements?</p> | <p>顧客はSSL対応のWebアプリケーションをAWSに導入しており、インスタンスにログインしてAPI呼び出しを行うことができるEC2サービス管理者と、これらのサービス管理者に排他的にプライベートキーを含むアプリケーションのX.509証明書にアクセスするセキュリティ担当者との間で役割の分離を実装したいと考えています。 上記の要件を満たす構成オプションはどれですか？</p>	sa:	Configure IAM policies authorizing access to the certificate store only to the security officer's and terminate SSL on the ELB. <br>  IAMポリシーを設定して、証明書ストアへのアクセスをセキュリティ担当者のみに許可し、ELBのSSLを終了します。|<p><br></p><p></p><div><div>Option A is CORRECT because (a) only the security officers have access to the certificate store, and (b) the certificate is not stored on an EC2 instances, hence avoiding giving access to it to the EC2 service administrators.</div><div>Option B is incorrect because it will still involve storing the certificate on the EC2 instances and additional configuration overhead to give access to the security officers which is unnecessary.</div><div>Option C and D both are incorrect because giving EC2 instances the access to the certificate should be avoided. It is better to let ELB manage the SSL certificate, instead of the EC2 web servers.</div><div><br></div></div><div><br></div><div>For more information please refer to the links given below:</div><div></div><a href="http://docs.aws.amazon.com/IAM/latest/APIReference/API_UploadServerCertificate.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/APIReference/API_UploadServerCertificate.html</a><br><a href="https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/" target="_blank">https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/</a><br><br><p></p>	Configure system permissions on the web servers to restrict access to the certificate only to the authorized security officers. <br>  認証されたセキュリティ担当者のみに証明書へのアクセスを制限するために、Webサーバーでシステム権限を設定します。	Upload the certificate on an S3 bucket owned by the security officers and accessible only by the EC2 role of the web servers <br>  セキュリティ担当者が所有するS3バケット上に証明書をアップロードし、WebサーバーのEC2ロールによってのみアクセス可能	Configure the web servers to retrieve the certificate upon boot from an CloudHSM that is managed by the security officers. <br>  セキュリティ担当者が管理するCloudHSMからブート時に証明書を取得するようにWebサーバーを設定します。
Test3-13. <p><span id="docs-internal-guid-846a0cce-30ff-9ad0-12cf-745db7505b8e"></span></p><p dir="ltr" style="">Your company runs a customer facing event registration site which is built with a 3-tier architecture with web and application tier servers, and a MySQL database. The application requires 6 web tier servers and 6 application tier servers for normal operation, but can run on a minimum of 65% server capacity and a single MySQL database. When deploying this application in a region with three availability zones (AZs) which architecture provides high availability?</p><br><p></p> | <p> <p id = "docs-internal-guid-846a0cce-30ff-9ad0-12cf-745db7505b8e"> </ span> <p dir = "ltr" style = ""> Webおよびアプリケーション層サーバーを備えた3層アーキテクチャとMySQLデータベースで構築されています。このアプリケーションには、通常の操作では6つのWeb層サーバーと6つのアプリケーション層サーバーが必要ですが、サーバー容量と1つのMySQLデータベースで65％以上稼動できます。高可用性を提供する3つの可用性ゾーン（AZ）を持つ地域にこのアプリケーションを配備する場合</p> <br> <p> </p>	sa:	A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database services) deployment. <br>  ELB（エラスティックロードバランサ）の背後にある自動スケーリンググループ内の各AZに2つのEC2（Elastic Compute Cloud）インスタンスを持つ3つのAZに展開されたWeb層。自動AZA内の各AZに2つのEC2インスタンスを持つ3つのAZに展開されたアプリケーション層ELBの背後にあるScaling Group、およびMulti-AZ RDS（リレーショナルデータベースサービス）の展開をサポートします。|<p dir="ltr" style=""><br></p><p dir="ltr" style="">In this scenario, the application can run on minimum 65% of the overall capacity of servers. I.e. it can run on minimum 4 web and 4 application servers. </p><p dir="ltr" style="">Since there are 3 AZs, there are many ways the instances can be put across them. The most important point to consider is that even of an entire AZ becomes unavailable, there should be minimum 4 servers running. So, placing 3 servers in 2 AZs is not a good architecture. Based on this, <b>option A and C are incorrect</b>. The best solution would be to have 2 servers in each AZ. So, in case of an entire AZ being unavailable, the application still has 4 servers available. </p><p dir="ltr" style="">Now, regarding RDS instance, the high availability is provided by the Multi-AZ deployment, not by read replicas (although they improve the performance in case of read-heavy workload). So, <b>option B is incorrect</b>.</p><p dir="ltr" style="">Hence, <b>option D is CORRECT</b> because (a) it places 2 EC2 instances in each of the 3 AZs, and (b) it uses the Multi-AZ deployment of RDS.</p></span><br><p></p>	A web tier deployed across 3 AZs with 2 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 3 AZs with 2 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the two other AZs. <br>  ELB（エラスティックロードバランサ）の背後にある自動スケーリンググループ内の各AZに2つのEC2（Elastic Compute Cloud）インスタンスを持つ3つのAZに展開されたWeb層。自動AZA内の各AZに2つのEC2インスタンスを持つ3つのAZに展開されたアプリケーション層ELBの背後にあるスケーリンググループ、および1つのRDS（Relational Database Service）インスタンスが、2つの他のAZに読込みレプリカとともに配備されています。	A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 2 AZs with 3 EC2 instances m each AZ inside an Auto Scaling Group behind an ELB, and a Multi-AZ RDS (Relational Database Service) deployment. <br>  ELB（エラスティックロードバランサ）の背後にある自動スケーリンググループ内の各AZに3つのEC2（Elastic Compute Cloud）インスタンスを持つ2つのAZに展開されたWeb層。2つのAZに配置されたアプリケーション層で、 ELBの背後にあるScaling Group、およびMulti-AZ RDS（Relational Database Service）デプロイメントが含まれます。	A web tier deployed across 2 AZs with 3 EC2 (Elastic Compute Cloud) instances in each AZ inside an Auto Scaling Group behind an ELB (elastic load balancer), an application tier deployed across 2 AZs with 3 EC2 instances in each AZ inside an Auto Scaling Group behind an ELB, and one RDS (Relational Database Service) instance deployed with read replicas in the other AZ. <br>  ELB（エラスティックロードバランサ）の背後にある自動スケーリンググループ内の各AZに3つのEC2（Elastic Compute Cloud）インスタンスを持つ2つのAZに展開されたWeb層、2つのAZに配置されたアプリケーション層、3つのEC2インスタンスを持つAZ ELBの背後にあるスケーリンググループ、および1つのRDS（Relational Database Service）インスタンスが、他のAZに読込みレプリカとともに配置されています。
Test3-14. <p>Your company’s on-premises content management system has the following architecture. It has an Application Tier hosted on IIS. The database Tier is MySQL database. This is regularly backed up to Amazon Simple Storage Service (S3) using the a custom backup utility. The static Content is stored on a 512GB gateway stored Storage Gateway volume attached to the application server via the iSCSI interface</p><p>Which AWS based disaster recovery strategy will give you the best RTO?</p> | <p>社内のコンテンツ管理システムには、次のアーキテクチャがあります。IISにホストされているアプリケーション層があります。データベース層はMySQLデータベースです。これは、カスタムバックアップユーティリティを使用してAmazon Simple Storage Service（S3）に定期的にバックアップされます。静的なコンテンツは、512GBのゲートウェイに格納され、iSCSIインタフェースを介してアプリケーションサーバーに接続されたStorage Gatewayボリュームに格納されます。</p> <p>どのAWSベースの災害復旧戦略により、	sa:	Deploy the MySQL database and the IIS app server on EC2. Restore the backups from Amazon S3. Generate an EBS volume of static? content from the Storage Gateway and attach it to the IIS EC2 server. <br>  MySQLデータベースとIISアプリケーションサーバーをEC2にデプロイします。Amazon S3からバックアップを復元します。静的なEBSボリュームを生成しますか？コンテンツをStorage Gatewayから取り出し、IIS EC2サーバーに接続します。|<p><br></p><p></p><p>Option A is CORRECT because (i) it deploys the MySQL database on EC2 instance by restoring the backups from S3 which is quick, and (ii) it generates the EBS volume of static content from Storage Gateway. Due to these points, option A meet the best RTO compared to all the remaining options.&nbsp;</p><p>Option B is incorrect because restoring the backups from the Amazon Glacier will be slow and will not meet the RTO.</p><p>Option C is incorrect because there is no need to attach the Storage Gateway as an iSCSI volume; you can just easily and quickly create an EBS volume from the Storage Gateway. Then you can generate snapshots from the EBS volumes for better recovery time.</p>  Option D is incorrect as restoring the content from Virtual Tape Library will not fit into the RTO.<br><p></p><p><br></p>	Deploy the MySQL database on RDS. Deploy the IIS app server on EC2. Restore the backups from Amazon Glacier. Generate an EBS volume of static content from the Storage Gateway and attach it to the IIS EC2 server. <br>  RDSにMySQLデータベースを展開します。EC2にIISアプリケーションサーバーを展開します。Amazon Glacierからバックアップを復元します。Storage Gatewayから静的コンテンツのEBSボリュームを生成し、IIS EC2サーバーに接続します。	Deploy the MySQL database and the IIS app server on EC2. Restore the MySQL backups from Amazon S3. Restore the static content by attaching an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the IIS EC2 server. <br>  MySQLデータベースとIISアプリケーションサーバーをEC2にデプロイします。Amazon S3からMySQLバックアップを復元します。Amazon EC2上で動作するAWS Storage GatewayをiSCSIボリュームとしてIIS EC2サーバーに接続することにより、静的コンテンツを復元します。	Deploy the MySQL database and the IIS app server on EC2. Restore the backups from Amazon S3. Restore the static content from an AWS Storage Gateway-VTL running on Amazon EC2. Deploy the MySQL database and the IIS app server on EC2. Restore the backups from Amazon S3. Restore the static content from an AWS Storage Gateway-VTL running on Amazon EC2. <br>  MySQLデータベースとIISアプリケーションサーバーをEC2にデプロイします。Amazon S3からバックアップを復元します。Amazon EC2上で動作するAWS Storage Gateway-VTLから静的コンテンツを復元します。MySQLデータベースとIISアプリケーションサーバーをEC2にデプロイします。Amazon S3からバックアップを復元します。Amazon EC2上で動作するAWS Storage Gateway-VTLから静的コンテンツを復元します。
Test3-15. <p>An application is deployed in multiple Availability Zones in a single region. In the event of failure, the RTO must be less than 3 hours, and the RPO is 15 minutes. Which DR strategy can be used to achieve this RTO and RPO in the event of this kind of failure?</p> | <p>アプリケーションは、単一のリージョン内の複数の可用性ゾーンに配備されています。障害が発生した場合、RTOは3時間未満、RPOは15分間でなければなりません。このような障害が発生した場合に、このRTOとRPOを達成するために使用できるDR戦略は何ですか？</p>	sa:	Take hourly DB backups to Amazon S3, with transaction logs stored in S3 every 5 minutes <br>  トランザクションログをS3に5分ごとに格納して、時間軸のDBバックアップをAmazon S3に転送する|<p><br></p><p></p><p>Option A is incorrect because restoring the backups from Amazon Glacier would be slow and will definitely not meet the RTO and RPO.&nbsp;</p><p>Option B is incorrect because with the synchronous replication you cannot go back to point in time recovery. You will always have the latest data.&nbsp;</p><p>Option C is CORRECT because it takes hourly backups to Amazon S3 - which makes restoring the backups quick, and since the transaction logs are stored in S3 every 5 minutes, it will help to restore the application to a state that is within the RPO of 15 minutes.&nbsp;</p><p>Option D is incorrect because instant store volume is ephemeral. i.e. the data can get lost when the instance is terminated.</p><p>&nbsp;</p><p>NOTE:</p><p>Although Glacier supports expedited retrieval (On-Demand and Provisioned), it is an expensive option and is recommended only for occasional urgent request for a small number of archives. Having said this (and even if we go with glacier as solution), the option also mentions taking database snapshots every 15 minutes. Now if you keep taking backups every 15 mins, the database users are going to face lot of outages during the backup (due to I/O suspension especially in non-AZ deployment). Also, within 15 minutes the backup process may not even finish!</p><p>&nbsp;</p>  As an architect you need to use the database change (transaction) logs along with the backups to restore your database to a point in time. Since option (c) stores the transaction details up to last 5 minutes, you can easily restore your database and meet the RPO of 15 minutes. Hence, C is the best choice.<br><p></p><p><br></p>	Use synchronous database master-slave replication between two Availability Zones <br>  2つの可用性ゾーン間の同期データベースのマスター/スレーブレプリケーションを使用する	Take 15-minute DB backups stored in Amazon Glacier, with transaction logs stored in Amazon S3 every 5 minutes <br>  5分ごとにAmazon S3にトランザクションログを保存し、Amazon Glacierに格納された15分のDBバックアップを取得	Take hourly DB backups to an Amazon EC2 instance store volume, with transaction logs stored in Amazon S3 every 5 minutes. <br>  時間軸のDBバックアップをAmazon EC2インスタンスストアボリュームに持ち込みます。トランザクションログは5分ごとにAmazon S3に保存されます。
Test3-16. <p>The Marketing Director in your company asked you to create a mobile app that lets users post sightings of good deeds known as random acts of kindness in 80-character summaries. You decided to write the application in JavaScript so that it would run on the broadest range of phones, browsers, and tablets. Your application should provide access to Amazon DynamoDB to store the good deed summaries. Initial testing of a prototype shows that there aren’t large spikes in usage. Which option provides the most cost-effective and scalable architecture for this application?</p> | <p>あなたの会社のマーケティングディレクターは、80文字の要約でランダムな行為を知らせるモバイルアプリを作成するようユーザーに要求しました。JavaScriptを使ってアプリケーションを作成し、最も幅広い携帯電話、ブラウザ、タブレット上で動作させることにしました。あなたのアプリケーションは、Amazon DynamoDBにアクセスして、適切な要約を保存する必要があります。プロトタイプの最初のテストでは、使用量に大きなスパイクがないことが示されています。このアプリケーションで最も費用対効果の高いスケーラブルなアーキテクチャを提供するオプションはどれですか？</p>	sa:	Register the application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow S3 gets and DynamoDB puts. You serve your mobile application out of an S3 bucket enabled as a web site. Your client updates DynamoDB. <br>  Amazon、Google、FacebookなどのWebアイデンティティプロバイダにアプリケーションを登録し、そのプロバイダのIAMロールを作成し、IAMロールのアクセス許可を設定してS3取得とDynamoDBプットを許可します。あなたは、Webサイトとして有効になっているS3バケットからモバイルアプリケーションを提供します。クライアントがDynamoDBを更新します。|<p></p><div><br></div><div><p>This scenario asks to design a cost-effective and scalable solution where a multi-platform application needs to communicate with DynamoDB. For such scenarios, federated access to the application is the most likely solution.</p><p>&nbsp;</p><p>Option A is incorrect because the Token Vending Machine (STS Service) is implemented on a single EC2 instance which is a single point of failure. This is not a scalable solution either as the instance can become the performance bottleneck.</p><p>Option B is CORRECT because, (i) it authenticates the application via federated identity provider such as Amazon, Google, Facebook etc, (ii) it sets up the proper permisssion for DynamoDB access, and (iii) S3 website which supports Javascript - is a highly scalable and cost effective solution.</p><p>Option C is incorrect because deploying EC2 instances in auto-scaled environment is not as cost-effective solution as the S3 website, even though it is scalable.</p>  Option D is incorrect because (i) it does not mention any security token service that generates temporary credentials, and (ii) deploying EC2 instances in auto-scaled environment is not as cost-effective solution as the S3 website, even though it is scalable.<br></div><div><br></div><ul></ul><p></p>	Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance to provide signed credentials mapped to an Amazon Identity and Access Management (IAM) user allowing DynamoDB puts and S3 gets. You serve your mobile application out of an S3 bucket enabled as a web site. Your client updates DynamoDB <br>  EC2インスタンス上のトークン自動販売機（TVM）を使用して、セキュリティトークンサービスからの一時的な資格情報をJavaScriptクライアントに提供し、DynamoDBのputとS3取得を可能にするAmazon Identity and Access Management（IAM）あなたは、Webサイトとして有効になっているS3バケットからモバイルアプリケーションを提供します。クライアントがDynamoDBを更新する	Provide the JavaScript client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials mapped to an IAM user allowing DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoDB. <br>  トークン自動販売機（TVM）を使用してセキュリティトークンサービスからの一時的な資格情報をJavaScriptクライアントに提供し、IAMユーザーにマップされた署名済みの資格情報を提供して、DynamoDBのputを許可します。ロードバランスされ、自動拡張されたApache EC2インスタンスからモバイルアプリケーションを提供します。EC2インスタンスは、DynamoDBのputを許可するIAMロールで構成されています。サーバーがDynamoDBを更新します。	Register the JavaScript application with a Web Identity Provider like Amazon, Google, or Facebook, create an IAM role for that provider, and set up permissions for the IAM role to allow DynamoDB puts. You serve your mobile application out of Apache EC2 instances that are load-balanced and autoscaled. Your EC2 instances are configured with an IAM role that allows DynamoDB puts. Your server updates DynamoDB. <br>  Amazon、Google、FacebookなどのWebアイデンティティプロバイダにJavaScriptアプリケーションを登録し、そのプロバイダのIAMロールを作成し、DynamoDBの配置を許可するIAMロールのアクセス許可を設定します。ロードバランスされ、自動拡張されたApache EC2インスタンスからモバイルアプリケーションを提供します。EC2インスタンスは、DynamoDBのputを許可するIAMロールで構成されています。サーバーがDynamoDBを更新します。
Test3-17. <p>You have an ELB on AWS which has a set of web servers behind them. There is a requirement that the SSL key used to encrypt data is always kept secure. Secondly, the logs of ELB should only be decrypted by a subset of users. Which of these architectures meets all of the requirements?</p> | <p>あなたの背後に一連のWebサーバがあるAWS上にELBがあります。データを暗号化するために使用されるSSLキーは常に安全に保たれるという要件があります。第二に、ELBのログは、ユーザーのサブセットによってのみ解読されるべきです。これらのアーキテクチャのどれがすべての要件を満たしていますか？</p>	sa:	Use Elastic Load Balancing to distribute traffic to a set of web servers, configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption. <br>  Elastic Load Balancingを使用して、AWS CloudHSMを使用してSSLトランザクションを実行し、Amazon S3サーバを使用してプライベートAmazon S3バケットにWebサーバーログを書き込むために、一連のWebサーバーにトラフィックを分散し、ロードバランサを構成しますサイドの暗号化。|<p><br></p><p></p><p>Option A and D both are incorrect because the logs - which contain the sensitive information - are written to ephemeral volume. So there are chances that the data can get lost upon termination of the EC2 instance.</p><p>Option B is incorrect because it does not use a secure way of managing the SSL private key for SSL transaction.</p><p>Option C is CORRECT because it uses CloudHSM for performing the SSL transaction without requiring any additional way of storing or managing the SSL private key. This is the most secure way of ensuring that the key will not be moved outside of the AWS environment. Also, it uses the highly available and durable S3 service for storing the logs.</p><p>&nbsp;</p><p><b>More information on AWS CloudHSM:</b></p>  The AWS CloudHSM service helps you meet corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) appliances within the AWS cloud. With CloudHSM, you control the encryption keys and cryptographic operations performed by the HSM.<br><p></p><p><br></p><ul></ul><p></p>	Use Elastic Load Balancing to distribute traffic to a set of web servers. Use TCP load balancing on the load balancer and configure your web servers to retrieve the private key from a private Amazon S3 bucket on boot. Write your web server logs to a private Amazon S3 bucket using Amazon S3 server-side encryption. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。ロードバランサでTCPロードバランシングを使用し、起動時に非公開のAmazon S3バケットから秘密鍵を取得するようにWebサーバーを構成します。Amazon S3サーバー側の暗号化を使用して、Webサーバーログを非公開のAmazon S3バケットに書き込みます。	Use Elastic Load Balancing to distribute traffic to a set of web servers. To protect the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。SSL秘密鍵を保護するには、鍵をロードバランサにアップロードし、ロードバランサを設定してSSLトラフィックをオフロードします。ランダムに生成されたAESキーを使用して暗号化されたエフェメラルボリュームにWebサーバーログを書き込みます。	Use Elastic Load Balancing to distribute traffic to a set of web servers. Configure the load balancer to perform TCP load balancing, use an AWS CloudHSM to perform the SSL transactions, and write your web server logs to an ephemeral volume that has been encrypted using a randomly generated AES key. <br>  Elastic Load Balancingを使用して、一連のWebサーバーにトラフィックを分散します。TCPロードバランシングを実行するようにロードバランサを設定し、AWS CloudHSMを使用してSSLトランザクションを実行し、ランダムに生成されたAESキーを使用して暗号化されたエフェメラルボリュームにWebサーバーログを書き込みます。
Test3-18. <p>A company host a web application. The application is designed for business travelers who must be able to connect to it from their hotel rooms, cafes, public Wi-Fi hotspots, and elsewhere on the Internet, but the application server itself should not be exposed to the internet. Which of the below options can help to fulfill these requirements?</p> | <p>会社がウェブアプリケーションをホストしています。このアプリケーションは、ホテルの客室、カフェ、公共Wi-Fiホットスポット、およびインターネットの他の場所から接続できる必要があるビジネストラベラー向けに設計されていますが、アプリケーションサーバー自体はインターネットに公開しないでください。これらの要件を満たすのに役立つ以下のオプションはどれですか？</p>	sa:	Configure an SSL VPN solution in a public subnet of your VPC, then install and configure SSL VPN client software on all user computers. Create a private subnet in your VPC and place your application servers in it. <br>  VPCのパブリックサブネットにSSL VPNソリューションを設定し、すべてのユーザコンピュータにSSL VPNクライアントソフトウェアをインストールして設定します。VPCにプライベートサブネットを作成し、アプリケーションサーバーをそのサブネットに配置します。|<p><br></p><p></p><p>Option A is incorrect because AWS Direct Connect is not a cost effective solution compared to using VPN solution.</p><p>Option B is incorrect because it does not mention how the application would be accessible only to the business travelers and not to the public.</p><p>Option C is incorrect because if the application servers are put in the public subnet, they would be publicly accessible via the internet.</p>  Option D is CORRECT because configuring the SSL VPN solution is cost-effective and allows access only to the business travelers and since the application servers are in private subnet, the application is not accessible via the internet.<br><p></p><p><br></p>	Implement Elastic Load Balancing with an SSL listener that terminates the back-end connection to the application. <br>  アプリケーションへのバックエンド接続を終了するSSLリスナーを使用してElastic Load Balancingを実装します。	Configure an IPsec VPN connection, and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it. <br>  IPsec VPN接続を構成し、ユーザーに構成の詳細を提供します。VPCにパブリックサブネットを作成し、アプリケーションサーバーを配置します。	Implement AWS Direct Connect, and create a private interface to your VPC. Create a public subnet and place your application servers in it. <br>  AWS Direct Connectを実装し、VPCへのプライベートインターフェイスを作成します。パブリックサブネットを作成し、アプリケーションサーバーを配置します。
Test3-19. <p>An application is composed of multiple components. Currently, all the components are hosted on a single EC2 instance. Due to security reasons, the organization wants to implement 2 separate SSL for the separate modules. How can the organization achieve this with a single instance? <br></p><p>Choose an answer from the below options:</p> | <p>アプリケーションは複数のコンポーネントで構成されています。現在、すべてのコンポーネントは1つのEC2インスタンスでホストされています。セキュリティ上の理由から、組織は別々のモジュールに2つのSSLを実装したいと考えています。組織はどのようにして単一のインスタンスでこれを達成できますか？<br> </p> <p>以下のオプションから回答を選択します：</p>	sa:	Create an EC2 instance which has multiple network interfaces with multiple elastic IP addresses. <br>  複数の弾性IPアドレスを持つ複数のネットワークインターフェイスを持つEC2インスタンスを作成します。|<p><br></p><p></p><p>It can be useful to assign multiple IP addresses to an instance in your VPC to do the following:</p><p>&nbsp;</p><p>(1) Host multiple websites on a single server by using multiple SSL certificates on a single server and associating each certificate with a specific IP address.</p><p>(2) Operate network appliances, such as firewalls or load balancers, that have multiple IP addresses for each network interface.</p><p>(3) Redirect internal traffic to a standby instance in case your instance fails, by reassigning the secondary IP address to the standby instance.</p><p>&nbsp;</p><p>Option A is CORRECT because, as mentioned above, if you have multiple elastic network interfaces (ENIs) attached to the EC2 instance, each network IP can have a component running with a separate SSL certificate.</p><p>Option B is incorrect because having separate rules in security group as well as NACL does not mean that the instance supports multiple SSLs.</p><p>Option C is incorrect because an EC2 instance cannot belong to multiple subnets.</p>  Option D is incorrect because NAT address is not related to supporting multiple SSLs.<br><p></p><p><br></p><p></p><p>For more information on Multiple IP Addresses, please refer to the link below:</p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html</a></p><p><br></p><br><p></p><ul></ul><p></p>	Create an EC2 instance which has both an ACL and the security group attached to it and have separate rules for each IP address. <br>  ACLとそれに接続されたセキュリティグループの両方を持つEC2インスタンスを作成し、IPアドレスごとに個別のルールを設定します。	Create an EC2 instance which has multiple subnets attached to it and each will have a separate IP address. <br>  複数のサブネットが接続されたEC2インスタンスを作成し、それぞれに別々のIPアドレスを割り当てます。	Create an EC2 instance with a NAT address. <br>  NATアドレスを持つEC2インスタンスを作成します。
Test3-20. <p>Your company hosts an on-premises legacy engineering application with 900GB of data shared via a central file server. The engineering data consists of thousands of individual files ranging in size from megabytes to multiple gigabytes. Engineers typically modify 5-10 percent of the files a day. Your CTO would like to migrate this application to AWS, but only if the application can be migrated over the weekend to minimize user downtime. You calculate that it will take a minimum of 48 hours to transfer 900GB of data using your company’s existing 45-Mbps Internet connection.</p><p>After replicating the application’s environment in AWS, which option will allow you to move the application’s data to AWS without losing any data and within the given timeframe?</p> | <p>貴社では、900GBのデータを中央ファイルサーバー経由で共有するオンプレミスのレガシーエンジニアリングアプリケーションをホストしています。エンジニアリングデータは、メガバイトから数ギガバイトまでのサイズのファイルが数千もあります。エンジニアは通常、1日に5〜10％のファイルを修正します。あなたのCTOはこのアプリケーションをAWSに移行したいが、週末にアプリケーションを移行してユーザのダウンタイムを最小限に抑えることができる場合に限る。</p> <p> AWSでアプリケーションの環境を複製すると、アプリケーションの環境を移行することができます。特定の期間内にデータを失うことなくデータをAWSに送信できますか？</p>	sa:	Sync the application data to Amazon S3 starting a week before the migration, on Friday morning perform a final sync, and copy the entire data set to your AWS file server after the sync completes. <br>  金曜日の午前中に移行の1週間前から開始して、アプリケーションデータをAmazon S3と同期させ、最終同期を実行し、同期完了後にデータセット全体をAWSファイルサーバにコピーします。|<p><br></p><p></p><p>In this scenario, following important points need to be considered - (i) only fraction of the data (5-10%) is modified every day, (ii) there are only 48 hrs for the migration, (iii) downtime should be minimized, and (iv) there should be no data loss.</p><p><br></p><p>Option A is incorrect because even though it is theoretically possible to transfer 972GB of data in 48 hours with 45Mbps speed, this option will only work if you consistently utilize the bandwidth to the max. This option will have less time in hand if there are any problems with the multipart upload. Hence, not a practical solution.</p><p>Option B is a proactive approach, which is CORRECT, because the data changes are limited and can be propagated over the week. Also, the bandwidth would be used efficiently, and you would have sufficient time and bandwidth in hand, should there be any unexpected issues while migrating.</p><p>Option C is incorrect because physically shipping the disk to Amazon would involve many external factors such as shipping delays, loss of shipping, damage to the disk, and also the time would not be sufficient to import the data in a day (Sunday). This is a very risky option and should not be exercised.</p><p>Option D is incorrect because AWS Storage Gateway involves creating S3 snapshots and synchronizing. This option is slow and may not meet the limitation of 48 hrs downtime.</p><br><p></p><p>Please view the below video for best practices for cloud migration to AWS:</p><p><br></p><p><br></p><p><br></p><p></p><div class="mediaplugin mediaplugin_videojs"><div style="max-width:400px;"><div tabindex="-1" title="watch" preload="auto" class="video-js vjs-paused vjs-fluid id_videojs_5b6d1083305d0-dimensions vjs-controls-enabled vjs-workinghover vjs-youtube vjs-youtube-mobile vjs-user-inactive" id="id_videojs_5b6d1083305d0" data-setup-lazy="{&quot;techOrder&quot;: [&quot;youtube&quot;], &quot;sources&quot;: [{&quot;type&quot;: &quot;video/youtube&quot;, &quot;src&quot;:&quot;https://www.youtube.com/watch?v=UpeV4OqB6Us&amp;amp;list=PL_RVC-cMNyYTz8zlxq117O1bfji-knooI&amp;amp;index=23&quot;}], &quot;language&quot;: &quot;en&quot;, &quot;fluid&quot;: true}" role="region" aria-label="video player"><div><iframe id="id_videojs_5b6d1083305d0_Youtube_api" style="width:100%;height:100%;top:0;left:0;position:absolute" class="vjs-tech" frameborder="0" allowfullscreen="1" allow="autoplay; encrypted-media" title="YouTube video player" width="640" height="360" src="https://www.youtube.com/embed/UpeV4OqB6Us?controls=0&amp;modestbranding=1&amp;rel=0&amp;showinfo=0&amp;loop=0&amp;fs=0&amp;hl=en&amp;enablejsapi=1&amp;origin=https%3A%2F%2Flearning.whizlabs.com&amp;widgetid=1"></iframe></div><div></div><div class="vjs-poster vjs-hidden" tabindex="-1" aria-disabled="false"></div><div class="vjs-text-track-display" aria-live="off" aria-atomic="true"></div><div class="vjs-loading-spinner" dir="ltr"></div><button class="vjs-big-play-button" type="button" aria-live="polite" title="Play Video" aria-disabled="false"><span class="vjs-control-text">Play Video</span></button><div class="vjs-control-bar" dir="ltr" role="group"><button class="vjs-play-control vjs-control vjs-button" type="button" aria-live="polite" title="Play" aria-disabled="false"><span class="vjs-control-text">Play</span></button><div class="vjs-volume-menu-button vjs-menu-button vjs-menu-button-inline vjs-control vjs-button vjs-volume-menu-button-horizontal vjs-vol-3" tabindex="0" role="button" aria-live="polite" title="Mute" aria-disabled="false"><div class="vjs-menu"><div class="vjs-menu-content"><div tabindex="0" class="vjs-volume-bar vjs-slider-bar vjs-slider vjs-slider-horizontal" role="slider" aria-valuenow="100.00" aria-valuemin="0" aria-valuemax="100" aria-label="volume level" aria-valuetext="100.00%"><div class="vjs-volume-level"><span class="vjs-control-text"></span></div></div></div></div><span class="vjs-control-text">Mute</span></div><div class="vjs-current-time vjs-time-control vjs-control"><div class="vjs-current-time-display" aria-live="off"><span class="vjs-control-text">Current Time </span>0:00</div></div><div class="vjs-time-control vjs-time-divider"><div><span>/</span></div></div><div class="vjs-duration vjs-time-control vjs-control"><div class="vjs-duration-display" aria-live="off"><span class="vjs-control-text">Duration Time</span> 0:00</div></div><div class="vjs-progress-control vjs-control"><div tabindex="0" class="vjs-progress-holder vjs-slider vjs-slider-horizontal" role="slider" aria-valuenow="NaN" aria-valuemin="0" aria-valuemax="100" aria-label="progress bar" aria-valuetext="0:00"><div class="vjs-load-progress" style="width: 0%;"><span class="vjs-control-text"><span>Loaded</span>: 0%</span><div style="left: 0%; width: 0%;"></div></div><div class="vjs-mouse-display" data-current-time="0:00" style="left: 0px;"></div><div class="vjs-play-progress vjs-slider-bar" data-current-time="0:00"><span class="vjs-control-text"><span>Progress</span>: 0%</span></div></div></div><div class="vjs-live-control vjs-control vjs-hidden"><div class="vjs-live-display" aria-live="off"><span class="vjs-control-text">Stream Type</span>LIVE</div></div><div class="vjs-remaining-time vjs-time-control vjs-control"><div class="vjs-remaining-time-display" aria-live="off"><span class="vjs-control-text">Remaining Time</span> -0:00</div></div><div class="vjs-custom-control-spacer vjs-spacer ">&nbsp;</div><div class="vjs-playback-rate vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Playback Rate" aria-disabled="false" aria-expanded="false" aria-haspopup="true"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"></ul></div><span class="vjs-control-text">Playback Rate</span><div class="vjs-playback-rate-value">1</div></div><div class="vjs-chapters-button vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Chapters" aria-disabled="false" aria-expanded="false" aria-haspopup="true" aria-label="Chapters Menu"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"><li class="vjs-menu-title" tabindex="-1">Chapters</li></ul></div><span class="vjs-control-text">Chapters</span></div><div class="vjs-descriptions-button vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Descriptions" aria-disabled="false" aria-expanded="false" aria-haspopup="true" aria-label="Descriptions Menu"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"><li class="vjs-menu-item vjs-selected" tabindex="-1" role="menuitemcheckbox" aria-live="polite" aria-disabled="false" aria-checked="true">descriptions off<span class="vjs-control-text">, selected</span></li></ul></div><span class="vjs-control-text">Descriptions</span></div><div class="vjs-subtitles-button vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Subtitles" aria-disabled="false" aria-expanded="false" aria-haspopup="true" aria-label="Subtitles Menu"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"><li class="vjs-menu-item vjs-selected" tabindex="-1" role="menuitemcheckbox" aria-live="polite" aria-disabled="false" aria-checked="true">subtitles off<span class="vjs-control-text">, selected</span></li></ul></div><span class="vjs-control-text">Subtitles</span></div><div class="vjs-captions-button vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Captions" aria-disabled="false" aria-expanded="false" aria-haspopup="true" aria-label="Captions Menu"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"><li class="vjs-menu-item vjs-texttrack-settings" tabindex="-1" role="menuitem" aria-live="polite" aria-disabled="false">captions settings<span class="vjs-control-text">, opens captions settings dialog</span></li><li class="vjs-menu-item vjs-selected" tabindex="-1" role="menuitemcheckbox" aria-live="polite" aria-disabled="false" aria-checked="true">captions off<span class="vjs-control-text">, selected</span></li></ul></div><span class="vjs-control-text">Captions</span></div><div class="vjs-audio-button vjs-menu-button vjs-menu-button-popup vjs-control vjs-button vjs-hidden" tabindex="0" role="menuitem" aria-live="polite" title="Audio Track" aria-disabled="false" aria-expanded="false" aria-haspopup="true" aria-label="Audio Menu"><div class="vjs-menu" role="presentation"><ul class="vjs-menu-content" role="menu"></ul></div><span class="vjs-control-text">Audio Track</span></div><button class="vjs-fullscreen-control vjs-control vjs-button" type="button" aria-live="polite" title="Fullscreen" aria-disabled="false"><span class="vjs-control-text">Fullscreen</span></button></div><div class="vjs-error-display vjs-modal-dialog vjs-hidden " tabindex="-1" aria-describedby="id_videojs_5b6d1083305d0_component_349_description" aria-hidden="true" aria-label="Modal Window" role="dialog"><p class="vjs-modal-dialog-description vjs-offscreen" id="id_videojs_5b6d1083305d0_component_349_description">This is a modal window.</p><div class="vjs-modal-dialog-content" role="document"></div></div>alogLabel-id_videojs_5b6d1083305d0_component_354" aria-level="1" role="heading">Caption Settings Dialog</div><div class="vjs-control-text" id="TTsettingsDialogDescription-id_videojs_5b6d1083305d0_component_354">Beginning of dialog window. Escape will cancel and close the window.</div>	Copy the data to Amazon S3 using multiple threads and multi-part upload for large files over the weekend, and work in parallel with your developers to reconfigure the replicated application environment to leverage Amazon S3 to serve the engineering files. <br>  週末に複数のスレッドと大容量ファイルのマルチパートアップロードを使用してAmazon S3にデータをコピーし、開発者と連携して複製されたアプリケーション環境を再構成し、Amazon S3を利用してエンジニアリングファイルを提供します。	Copy the application data to a 1-TB USB drive on Friday and immediately send overnight, with Saturday delivery, the USB drive to AWS Import/Export to be imported as an EBS volume, mount the resulting EBS volume to your AWS file server on Sunday <br>  金曜日に1 TBのUSBドライブにアプリケーションデータをコピーし、すぐに土曜日の配信で、USBドライブをAWSインポート/エクスポートにEBSボリュームとしてインポートし、結果として得られるEBSボリュームを日曜日にAWSファイルサーバーにマウントする	Leverage the AWS Storage Gateway to create a Gateway-Stored volume. On Friday copy the application data to the Storage Gateway volume. After the data has been copied, perform a snapshot of the volume and restore the volume as an EBS volume to be attached to your AWS file server on Sunday. <br>  AWS Storage Gatewayを活用してGateway-Storedボリュームを作成します。金曜日に、アプリケーション・データをStorage Gatewayボリュームにコピーします。データがコピーされた後、日曜日にボリュームのスナップショットを実行し、AWSファイルサーバーに接続するEBSボリュームとしてボリュームを復元します。
Test3-21. <p>Which of the following are some of the best examples where Amazon Kinesis can be used?</p> | <p> Amazon Kinesisを使用できる最良の例はどれですか？</p>	sa:	All of the above <br>  上記のすべて|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;"></span></p><p>The following are typical scenarios for using Kinesis Data Streams:</p><div><dl><dt>Accelerated log and data feed intake and processing</dt><dd><p>You can have producers push data directly into a stream. For example, push system and application logs and they are available for processing in seconds. This prevents the log data from being lost if the front end or application server fails. Kinesis Data Streams provides accelerated data feed intake because you don't batch the data on the servers before you submit it for intake.</p></dd><dt>Real-time metrics and reporting</dt><dd><p>You can use data collected into Kinesis Data Streams for simple data analysis and reporting in real time. For example, your data-processing application can work on metrics and reporting for system and application logs as the data is streaming in, rather than wait to receive batches of data.</p></dd><dt>Real-time data analytics</dt><dd><p>This combines the power of parallel processing with the value of real-time data. For example, process website clickstreams in real time, and then analyze site usability engagement using multiple different Kinesis Data Streams applications running in parallel.</p></dd><dt>Complex stream processing</dt><dd><p>You can create Directed Acyclic Graphs (DAGs) of Amazon Kinesis Data Streams applications and data streams. This typically involves putting data from multiple Amazon Kinesis Data Streams applications into another stream for downstream processing by a different Amazon Kinesis Data Streams application.</p></dd></dl></div>&nbsp;<p></p><p>For more information on Kinesis, please refer to the below URL:</p><p></p><a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a><br><p></p>	Real-time metrics and reporting <br>  リアルタイムのメトリックとレポート	Real-time data analytics <br>  リアルタイムデータ分析	Accelerated log and data feed intake <br>  加速されたログとデータフィードの摂取
Test3-22. <p>Which of the following are Lifecycle events available in OpsWorks?</p><p>Choose 3 options from the below:</p> | <p> OpsWorksで利用可能なLifecycleイベントはどれですか？</p> <p>以下の3つのオプションを選択してください：</p>	ma:	o:Setup <br>  セットアップ|<p><br></p><p>Below is a snapshot of the Lifecycle events in OpsWorks.</p><p><img src="https://s3.amazonaws.com/awssap/3_22_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="399" width="525">&nbsp;</p><p><br></p><p>For more information on Lifecycle events, please refer to the below URL:</p><p></p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html</a><br><br><p></p>	x:Decommision <br>  デコミッション	o:Deploy <br>  展開	o:Shutdown <br>  o:Shutdown
Test3-23. <p>You tried to integrate 2 systems (front end and back end) with an HTTP interface to one large system. These subsystems don’t store any state inside. All of the state information is stored in a DynamoDB table. You have launched each of the subsystems with separate AMIs.</p><p>After testing, these servers stopped running and are issuing malformed requests that do not meet the HTTP specifications of the client. Your developers fix the issue and deploy the fix to the subsystems as soon as possible without service disruption.</p><p>What are the 3 most effective options from the below to deploy these fixes and ensure that healthy instances are redeployed?</p><p><br></p> | <p> 2つのシステム（フロントエンドとバックエンド）をHTTPインターフェイスと1つの大きなシステムに統合しようとしました。 これらのサブシステムは、内部に状態を格納しません。 すべての状態情報は、DynamoDBテーブルに格納されます。 別々のAMIを使用して各サブシステムを起動しました。</p> <p>テスト後、これらのサーバーは実行を停止し、クライアントのHTTP仕様に適合しない不正な形式の要求を発行しています。 開発者は問題を修正し、サービスを中断することなくできるだけ早くサブシステムに修正プログラムを導入します。</p> <p>以下の3つの最も効果的なオプションを使用して、これらの修正プログラムを導入し、正常なインスタンスを確実に再デプロイすることができますか？</p> <p> / p> <p> <br> </p>	ma:	x:Use VPC. <br>  VPCを使用します。|<p><br></p><p>Option A is incorrect because the instances should already be there in a VPC, and even if not, this option is not going to help fix the issue.</p><p>Option B is CORRECT because Autohealing would try to bring the instances back up with the healthy configuration with which it was launched. Please see the "More information.." section.</p><p>Option C and D are CORRECT because you can pause instances in AutoScaling, apply the patches and then add the instances back to AutoScaling and it will be registered with ELB.</p><p>Option E is incorrect because deploying CloudFront is not needed in this situation.</p><p>Option F is incorrect because if you deploy SQS, even the malformed requests will also get queued and later processed. You should be avoiding that.</p><p><br></p><p><b>More information on Auto Healing in OpsWork:</b></p><p>Auto healing is an excellent feature of OpsWorks and is something that provides disaster recovery within a stack. All OpsWorks instances have an agent installed which not only works to install and configure each instance using Chef, but to also update OpsWorks with resource utilization information. If auto healing is enabled at the layer, and one or more instances experiences a health-related issue where the polling stops, OpsWorks will heal the instance. When OpsWorks heals an instance, it first terminates the problem instance, and then starts a new one as per the layer configuration. Being that the configuration is pulled from the layer; the new instance will be set up exactly as the old instance which has just been terminated.</p><p><br></p><p>For more information on Auto-healing, please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html</a><br><p></p><p><span style="font-size: 1rem;">For more information on the suspension process in AutoScaling, please refer to the below link</span></p><p></p><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-suspend-resume-processes.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-suspend-resume-processes.html</a><br><p></p><p><br></p>	o:Use AWS Opsworks autohealing for both the front end and back end instance pair. <br>  フロントエンドとバックエンドのインスタンスペアの両方にAWS Opsworks自動ヒーリングを使用します。	o:Use Elastic Load balancing in front of the front-end system and Auto scaling to keep the specified number of instances. <br>  フロントエンドシステムの前でElastic Load Balancingを使用し、指定された数のインスタンスを維持するために自動スケーリングを使用します。	o:Use Elastic Load balancing in front of the back-end system and Auto scaling to keep the specified number of instances. <br>  バックエンドシステムの前でElastic Load Balancingを使用し、指定された数のインスタンスを維持するために自動スケーリングを使用します。	x:Use Amazon Cloudfront with access the front end server with origin fetch. <br>  Amazon Cloudfrontを使用して、元のフェッチを使用してフロントエンドサーバーにアクセスします。	x: Use Amazon SQS between the front end and back end subsystems. <br>  フロントエンドとバックエンドのサブシステム間でAmazon SQSを使用します。
Test3-24. <p><span id="docs-internal-guid-846a0cce-3104-2e76-327e-a84dcaef8b07"></span></p><p dir="ltr" style="">You are the new IT architect in a company that operates a mobile sleep tracking application. When &nbsp;activated at night, the mobile app is sending collected data points of 1 kilobyte every 5 minutes to your backend. The backend takes care of authenticating the user and writing the data points into an Amazon DynamoDB table. Every morning, you scan the table to extract and aggregate last night’s data on a per user basis, and store the results in Amazon S3. Users are notified via Amazon SNS mobile push notifications that new data is available, which is parsed and visualized by the mobile app. Currently you have around 100k users who are mostly based out of North America. You have been tasked to optimize the architecture of the backend system to lower cost.</p><p dir="ltr" style="">What would you recommend? Choose 2 answers:</p><br><p></p> | <p> <p id = "docs-internal-guid-846a0cce-3104-2e76-327e-a84dcaef8b07"> </ span> </p> <p dir = "ltr" style = "">あなたは新しいITモバイルスリープトラッキングアプリケーションを運営する会社の設計者。夜に起動すると、モバイルアプリは5分ごとに1キロバイトのデータポイントをバックエンドに送信します。バックエンドは、ユーザーを認証し、データポイントをAmazon DynamoDBテーブルに書き込んで処理します。毎朝、テーブルをスキャンして、昨夜のデータをユーザー単位で抽出して集計し、その結果をAmazon S3に格納します。ユーザーには、Amazon SNSモバイルプッシュ通知を介して、新しいデータが利用可能であることが通知されます。これは、モバイルアプリによって解析され視覚化されます。現在、主に北米をベースにしている約100,000人のユーザーがいます。バックエンドシステムのアーキテクチャを最適化してコストを削減することが任されています。<p dir = "ltr" style = "">何をお勧めしますか？2つの回答を選択してください：</p> <br> <p> </p>	ma:	x:Have the mobile app access Amazon DynamoDB directly instead of JSON files stored on Amazon S3. <br>  Amazon S3に保存されているJSONファイルの代わりに、モバイルアプリがAmazon DynamoDBに直接アクセスできるようにします。|<br><p dir="ltr" style="">Option A is incorrect because, accessing the DynamoDB table for read and write by 100k users will exhaust the read and write capacity, which will increase the cost drastically.</p><p dir="ltr" style="">Option B is incorrect because, creating clusters of EC2 instances will be a very expensive solution in this scenario.</p><p dir="ltr" style="">Option C is CORRECT because, (a) with SQS, the huge number of writes overnight will be buffered/queued which will avoid exhausting the write capacity (hence, cutting down on cost), and (b) SQS can handle a sudden high load, if any.</p><p dir="ltr" style="">Option D is incorrect because, the data is not directly accessed from the DynamoDB table by the users, it is accessed from S3. So, there is no need for caching. Since the results are stored in S3, introducing ElastiCache is unnecessary.</p><p dir="ltr" style="">Option E is CORRECT because once the aggregated data is stored on S3, there is no point in keeping the DynamoDB tables pertaining to the previous days. Keeping the tables for the latest data only will certainly cut the unnecessary costs, keeping the overall cost of the solution down.</p></span><br><p></p>	x:Write data directly into an Amazon Redshift cluster replacing both Amazon DynamoDB and Amazon S3. <br>  Amazon DynamoDBとAmazon S3の両方を置き換えるAmazon Redshiftクラスタにデータを直接書き込みます。	o:Introduce an Amazon SQS queue to buffer writes to the Amazon DynamoDB table and reduce provisioned write throughput. <br>  Amazon SQSキューを導入し、Amazon DynamoDBテーブルへの書き込みをバッファし、プロビジョニングされた書き込みスループットを削減します。	x:Introduce Amazon Elasticache to cache reads from the Amazon DynamoDB table and reduce provisioned read throughput. <br>  Amazon Elasticacheを導入して、Amazon DynamoDBテーブルからの読み取りをキャッシュし、プロビジョニングされた読み取りスループットを削減します。	o:Create a new Amazon DynamoDB table each day and drop the one for the previous day after its data is on Amazon S3. <br>  毎日新しいAmazon Dynamo DBテーブルを作成し、Amazon Amazon S3の次の日を削除します。
Test3-25. <p>A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 in this VPC. The user is trying to create another subnet with the same VPC for CIDR 20.0.0.1/24. What will happen in this scenario?</p> | <p>ユーザーがCIDR 20.0.0.0/16のVPCを作成しました。ユーザは、このVPCでCIDR 20.0.0.0/16のサブネットを1つ作成しました。ユーザーは、CIDR 20.0.0.1/24と同じVPCを持つ別のサブネットを作成しようとしています。このシナリオではどうなるでしょうか？</p>	sa:	It will throw a CIDR overlap error <br>  CIDRオーバーラップエラーが発生する|<p><br></p><p></p><p>Since the CIDR of the new subnet overlaps with that of the first subnet, an overlap error will be displayed. See the snapshot below:</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_25_1.png" alt="" width="1015" height="580" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>&nbsp;</p><p>For more information on VPC subnets, please refer to the below link</p>  <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><p></p><ul></ul><p></p>	It is not possible to create a subnet with the same CIDR as the VPC <br>  VPCと同じCIDRを持つサブネットを作成することはできません	The second subnet will be created <br>  2番目のサブネットが作成されます	The VPC will modify the first subnet to allow this IP range <br>  VPCは、このIP範囲を許可するように最初のサブネットを変更します
Test3-26. <p>Currently, a company uses Redshift to store its analyzed data. They have started with the base configuration. What would they get when they initially start using Redshift?</p> | <p>現在、Redshiftを使用して分析されたデータを保存しています。彼らは基本構成から始めました。Redshiftの使用を最初に始めたときに得られるものは？</p>	sa:	One node of 160GB <br>  1つのノード160GB|<p>As per the AWS documentation,</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_26_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="85" width="967"></p><p><br></p><p>For more information on Redshift&nbsp; please refer to the below URL:<br></p><a href="https://aws.amazon.com/redshift/faqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/redshift/faqs/</a><br> <p></p>	One node of 320GB? <br>  320GBの1つのノード？	?Two nodes with 160GB each <br>  160GBの2つのノード	Two nodes with 320GB each <br>  2つのノード、それぞれ320GB
Test3-27. <p>If an on-premise application is dependent on multicast and is required to be moved on to AWS, which of the below steps need to be carried out on the Operating system hosting that app so that it can be moved to AWS?</p> | <p>オンプレミスアプリケーションがマルチキャストに依存しており、AWSに移行する必要がある場合、AWSに移行できるように、そのアプリケーションをホスティングするオペレーティングシステムで以下の手順のどれを実行する必要がありますか？< / p>	sa:	Create a virtual overlay network that runs on the OS level of the instance. <br>  インスタンスのOSレベルで実行される仮想オーバーレイネットワークを作成します。|<p><br></p><p></p><p>Option A is incorrect because just providing ENIs between the subnets would not resolve the dependency on multicast.</p><p>Option B is CORRECT because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).</p><p>Option C is incorrect because the only option that will work in this scenario is creating a virtual overlay network.</p><p>Option D is incorrect because VPC peering and multicast are not the same.</p><p>&nbsp;</p><p>For more information on Overlay Multicast in Amazon VPC, please visit the URL below:</p><p><a href="https://aws.amazon.com/articles/6234671078671125" target="_blank">https://aws.amazon.com/articles/6234671078671125</a></p><br><p></p><ul></ul><p></p>	Provide Elastic Network Interfaces between the subnets. <br>  サブネット間の弾性ネットワークインターフェイスを提供します。	All of the answers listed will help in deploying applications that require multicast on AWS. <br>  リストされているすべての回答は、AWSでのマルチキャストが必要なアプリケーションの導入に役立ちます。	Create all the subnets on a different VPC and use VPC peering between them. <br>  異なるVPC上のすべてのサブネットを作成し、それらの間のVPCピアリングを使用します。
Test3-28. <p>An auditor needs read-only access to the event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. What is the best way for giving them this sort of access?</p> | <p>監査人は、AWS管理コンソール、AWS SDK、コマンドラインツール、およびその他のAWSサービスを通じて行われたアクションを含む、AWSアカウントアクティビティのイベント履歴への読み取り専用アクセス権が必要です。このようなアクセスを与えるための最良の方法は何ですか？</p>	sa:	Enable CloudTrail logging and create an IAM user who has read-only permissions to the required AWS resources, including the bucket containing the CloudTrail logs. <br>  CloudTrailログを有効にし、CloudTrailログを含むバケットを含む、必要なAWSリソースへの読み取り専用アクセス権を持つIAMユーザーを作成します。|<p><br></p><p></p><p><span style="font-size: 1rem;">Option A is incorrect because (i) full permission to all the resources is not required, read only permissions should be given, and (ii) just creating access role in not sufficient, CloudTrail logging needs to be enabled as well.</span></p><p>Option B is incorrect because sending the logs via email is not a good architecture.</p><p>Option C is incorrect because granting the auditor access to AWS resources is not AWS's responsibility. It is the AWS user or account owner's responsibility.</p><p>Option D is CORRECT because you need to enable the CloudTrail logging in order to generate the logs with information about all the activities related to the AWS account and resources. It also creates an IAM user that has permissions to read the logs that are stored in the S3 bucket.</p><p>&nbsp;</p><p><b>More information on AWS CloudTrail</b></p><p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting.</p><p>For more information on CloudTrail, please visit the below URL:</p>  <a href="https://aws.amazon.com/cloudtrail/">https://aws.amazon.com/cloudtrail/</a><br><p></p><p><br></p><ul></ul><p></p>	Create an SNS notification that sends the CloudTrail log files to the auditor's email when CloudTrail delivers the logs to S3, but do not allow the auditor access to the AWS environment. <br>  CloudTrailがS3にログを配信するが、監査人がAWS環境にアクセスすることを許可していないときに、CloudTrailログファイルを監査人の電子メールに送信するSNS通知を作成する。	The company should contact AWS as part of the shared responsibility model, and AWS will grant required access to the third-party auditor. <br>  会社は共有責任モデルの一環としてAWSに連絡し、AWSは第三者監査人に必要なアクセス権を付与します。	Create a role that has the full permissions to access the resources for the auditor. <br>  監査人のためのリソースにアクセスするための完全な権限を持つ役割を作成します。
Test3-29. <p>Which of the following is the most recommended approach to replicate an RDS instance to an on-premise location to AWS in the most secure manner?</p> | <p>最も安全な方法でRDSインスタンスをオンプレミスの場所にAWSに複製する最も推奨される方法はどれですか？</p>	sa:	Create an IPSec VPN connection using either VPN/VGW through the Virtual Private Cloud service. <br>  仮想プライベートクラウドサービスを通じてVPN / VGWを使用してIPSec VPN接続を作成します。|<p><br></p><p></p><p>Option A is incorrect because SSL endpoint cannot be used here as it is used for securely accessing the database.</p><p>Option B is incorrect because replicating via EC2 instances is very time consuming and very expensive cost-wise.</p><p>Option C is incorrect because Data Pipeline is for batch jobs and not suitable for this scenario.</p><p>Option D is CORRECT because it is feasible to setup the secure IPSec VPN connection between the on premise server and AWS VPC using the VPN/Gateways.</p><p>See the image below:</p><p></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_29_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="372" width="730"></p><p><br></p><p>For more information on VPN connections, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a><br><p></p><p><br></p>	RDS cannot replicate to an on-premise database server. Instead, first configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection. <br>  RDSはオンプレミスデータベースサーバーに複製できません。代わりに、コアMySQLを使用してEC2インスタンスにレプリケートするようにRDSインスタンスを構成し、セキュアなVPN / VPG接続を介してレプリケーションを構成します。	Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. <br>  毎晩MySQLデータをエクスポートし、S3 HTTPSエンドポイントから安全にデータをダウンロードするデータパイプラインを作成します。	Configure the RDS instance as the master and enable replication over the open internet using a secure SSL endpoint to the on-premise server. <br>  RDSインスタンスをマスターとして構成し、セキュアなSSLエンドポイントを使用してオンデマンドサーバーに公開インターネット経由でレプリケーションを有効にします。
Test3-30. <p>A mobile application has been developed which stores data in DynamoDB. The application needs to scale to handle millions of views. The customer also needs access to the data in the DynamoDB table as part of the application. Which of the below methods would help to fulfill this requirement?</p> | <p> DynamoDBにデータを格納するモバイルアプリケーションが開発されました。数百万のビューを処理するためにアプリケーションを拡張する必要があります。顧客はまた、アプリケーションの一部としてDynamoDBテーブルのデータにアクセスする必要があります。次のどの方法でこの要件を満たすのに役立ちますか？</p>	sa:	Let the users sign in to the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the JavaScript interface in an S3 bucket. <br>  ユーザーがAmazon、Google、Facebookなどの第三者IDプロバイダを使用してアプリにログインできるようにします。AssumeRoleWithWebIdentity API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。JavaScriptでアプリケーションを記述し、S3バケットにJavaScriptインターフェイスをホストします。|<p><br></p><p>The AssumeRolewithWebIdentity returns a set of temporary security credentials for users who have been authenticated in a mobile or web application with a web identity provider, such as Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider.</p><p>Out of option C and D, Option C is invalid because S3 is used to host static websites and not server side language websites.</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/3_30_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>For more information on AssumeRolewithWebIdentity, please visit the below URL:<br></p><p></p><a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html</a><br><p></p>	Let the users sign into the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWith API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the JavaScript interface in an S3 bucket. <br>  Amazon、Google、Facebookなどの第三者のアイデンティティプロバイダを使用して、ユーザーがアプリケーションにログインできるようにします。AssumeRoleWith API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。JavaScriptでアプリケーションを記述し、S3バケットにJavaScriptインターフェイスをホストします。	Let the users sign into the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in a server-side language using the AWS SDK and host the application in an S3 bucket for scalability. <br>  Amazon、Google、Facebookなどの第三者のアイデンティティプロバイダを使用して、ユーザーがアプリケーションにログインできるようにします。AssumeRoleWithWebIdentity API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。AWS SDKを使用してアプリケーションをサーバー側言語で記述し、S3バケットにアプリケーションを配置してスケーラビリティを実現します。	Configure an on-premise AD server utilizing SAML 2.0 to manage the application users inside of the on-premise AD server and write code that authenticates against the LD serves. Grant a role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table. <br>  SAML 2.0を利用して社内のADサーバーを構成して、社内のADサーバー内のアプリケーションユーザーを管理し、LDのサービスに対して認証するコードを作成します。エンドユーザーがDynamoDBテーブル内の必要なデータにアクセスできるように、STSトークンに割り当てられたロールを付与します。
Test3-31. <p>You've created a temporary application that accepts image uploads, stores them in S3, and records information about the image in RDS. After building this architecture and accepting images for the duration required, it's time to delete the CloudFormation template. However, your manager has informed you that for archival reasons the RDS data needs to be stored and the S3 bucket with the images needs to remain. Your manager has also instructed you to ensure that the application can be restored by a CloudFormation template and run next year during the same period.</p><p><span style="font-size: 1rem;">Knowing that when a CloudFormation template is deleted, it will remove the resources it created. What is the best method for achieving the desired goals? <br></span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Choose the correct option from the belo</span><span style="font-size: 1rem;"></span><span style="font-size: 1rem;">w:</span></p> | <p>イメージのアップロードを受け付け、S3に保存し、RDSのイメージに関する情報を記録する一時的なアプリケーションを作成しました。このアーキテクチャを構築し、必要な期間画像を受け入れたら、CloudFormationテンプレートを削除します。しかし、あなたのマネージャーは、アーカイブの理由から、RDSデータを保存する必要があり、イメージ付きのS3バケットを残す必要があることを通知しました。あなたのマネージャーは、CloudFormationテンプレートでアプリケーションを復元して、同じ期間に翌年に実行するように指示しました。</p> <p> <span style = "font-size：1rem;"> CloudFormationテンプレートが削除され、作成されたリソースが削除されます。望ましい目標を達成するための最良の方法は何ですか？<br> </ span> </p> <p> <	sa:	Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot. <br>  CloudFormationテンプレートのS3リソース宣言のDeletionPolicyを保持するように設定し、RDSリソース宣言のDeletionPolicyをスナップショットに設定します。|<p><br></p><p>The main points in this questions are: (i) need for an ability by which the RDS data that is stored and can be restored of needed and (ii) the S3 bucket with the images needs to retain.</p><p><br></p><p>Option A is incorrect because this option replicates the images into another bucket, but does not ensure that the bucket itself would retain.</p><p>Option B is incorrect because RDS data does not need to be retained, you just need an ability to be able to restore the RDS data - for which you need to use snapshot policy.</p><p>Option C is incorrect because S3 bucket itself needs to be retained, hence you need to use retain policy for S3 bucket.</p><p>Option D is CORRECT because it uses retain policy for S3 bucket and snapshot policy for RDS such that the data can be restored when needed.</p><p><br></p><p><b>More information on DeletionPolicy Options:</b></p><p><span style="font-size: 1rem;"><i>Delete</i></span></p><p>AWS CloudFormation deletes the resource and all its content if applicable during stack deletion.</p><p><br></p><p><i>Retain</i></p><p>AWS CloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted.&nbsp;</p><p><br></p><p><i>Snapshot</i></p><p>For resources that support snapshots (AWS::EC2::Volume, AWS::ElastiCache::CacheCluster, AWS::ElastiCache::ReplicationGroup, AWS::RDS::DBInstance, AWS::RDS::DBCluster, and AWS::Redshift::Cluster), AWS CloudFormation creates a snapshot for the resource before deleting it.&nbsp;</p><p><br></p><p>For more information on CloudFormation deletion policy, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a><br><p></p>	For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to retain. <br>  CloudFormationテンプレートのRDSリソースタイプとS3リソースタイプの両方で、DeletionPolicyをretainに設定します。	Set the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot. <br>  S3リソースのDeletionPolicyをスナップショットに設定し、RDSリソースのDeletionPolicyをスナップショットに設定します。	Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to snapshot. <br>  ソースバケット上のS3バケットレプリケーションを宛先バケットに有効にして、すべてのS3オブジェクトのコピーを維持し、RDSインスタンスの削除ポリシーをスナップショットに設定します。
Test3-32. <p>What can be done if a company wants to establish a low latency dedicated connection to an S3 public endpoint over the Direct Connect?</p> | <p>企業がダイレクトコネクト経由でS3パブリックエンドポイントへの低レイテンシ専用接続を確立したい場合はどうすればよいですか？</p>	sa:	Configure a public virtual interface to connect to a public S3 endpoint resource. <br>  パブリックS3エンドポイントリソースに接続するようにパブリック仮想インターフェイスを構成します。|<p><br></p><p></p><p>You can create a public virtual interface to connect to public resources or a private virtual interface to connect to your VPC. You can configure multiple virtual interfaces on a single AWS Direct Connect connection, and you'll need one private virtual interface for each VPC to connect to. Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. See the image below:</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/3_32_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br> <!--[if !supportLineBreakNewLine]--><br> <!--[endif]--></p><p><span style="font-size: 1rem;">Option A is CORRECT because, as mentioned above, it creates a public virtual interface to connect to S3 endpoint.</span></p><p>Option B is incorrect because to connect to S3 endpoint, a public virtual interface needs to be created, not VPN.</p><p>Option C is incorrect because to connect to S3 endpoint, a&nbsp;public&nbsp;virtual interface needs to be created,&nbsp;not&nbsp;private.</p><p>Option D is incorrect because this setup will not help connecting to the S3 endpoint.</p><p>&nbsp;</p><p>For more information on virtual interfaces, please visit the below URL</p><p><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank">http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html</a></p><br><p></p><ul></ul><p></p>	Establish a VPN connection from the VPC to the public S3 endpoint. <br>  VPCからパブリックS3エンドポイントへのVPN接続を確立します。	Configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection. <br>  直接接続接続を介してパブリックS3エンドポイントに接続するようにプライベート仮想インターフェイスを設定します。	Add a BGP route as part of the on-premise router; this will route S3 related traffic to the public S3 endpoint to dedicated AWS region. <br>  BGPルートをオンプレミスルータの一部として追加します。S3関連のトラフィックをパブリックS3エンドポイントに専用のAWS領域にルーティングします。
Test3-33. <p>How can you configure the backups of an Oracle RAC configuration which is hosted on the AWS public cloud?</p> | <p> AWSパブリッククラウドでホストされているOracle RAC構成のバックアップをどのように構成できますか？</p>	sa:	Create a script that runs snapshots against the EBS volumes to create backups and durability. <br>  EBSボリュームに対してスナップショットを実行するスクリプトを作成し、バックアップと耐久性を作成します。|<p><br></p><p></p><p>Currently, Oracle Real Application Cluster (RAC) is not supported as per the AWS documentation. However,&nbsp;you can deploy scalable RAC on Amazon EC2 using the recently-published&nbsp;<a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/">tutorial&nbsp;</a>and Amazon Machine Images (AMI). So, in order to take the backups, you need to take the backup in the form of EBS volume snapshots of the EC2 that is deployed for RAC.</p><p>&nbsp;</p><p>Option A, B, and D are all incorrect because RDS does not support Oracle RAC.</p><p>Option C is CORRECT because Oracle RAC is supported via the deployment using Amazon EC2. Hence, for the data backup, you can create a script that takes the snapshots of the EBS volumes.</p><p>&nbsp;</p><p>For more information on Oracle RAC on AWS, please visit the below URL:</p><p><a href="https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/</a></p><p><a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/" target="_blank">https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/</a></p>  <a href="https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/" target="_blank">https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/</a><br><p></p><ul></ul><p></p>	Enable Multi-AZ failover on the RDS RAC cluster to reduce the RPO and RTO in the event of disaster or failure. <br>  災害や障害が発生した場合のRPOとRTOを削減するには、RDS RACクラスタで複数AZのフェールオーバーを有効にします。	Create manual snapshots of the RDS backup and write a script that runs the manual snapshot. <br>  RDSバックアップの手動スナップショットを作成し、手動スナップショットを実行するスクリプトを作成します。	Enable automated backups on the RDS RAC cluster; enable auto snapshot copy to a backup region to reduce RPO and RTO. <br>  RDS RACクラスタで自動バックアップを有効にする。RPOとRTOを減らすために、バックアップ領域への自動スナップショットコピーを有効にします。
Test3-34. <p>You are moving an existing traditional system to AWS. During migration, you discover that the master server is the single point of failure. Having examined the implementation of the master server you realize that there is not enough time during migration to re-engineer it to be highly available. You also discover that it stores its state in local MySQL database.</p><p>In order to minimize downtime, you select RDS to replace the local database and configure the master to use it. What steps would best allow you to create a self-healing architecture?</p> | <p>既存の従来のシステムをAWSに移行しています。移行中に、マスターサーバーが単一障害点であることがわかります。マスターサーバーのインプリメンテーションを検討したところ、高可用性になるようにリエンジニアリングするために、移行中に十分な時間がないことを認識しています。</p> <p>停止時間を最小限に抑えるには、RDSを選択してローカルデータベースを置き換え、マスターがそれを使用するように構成する必要があります。自己修復アーキテクチャを作成するにはどのような手順が最適でしょうか？</p>	sa:	Migrate the local database into Multi-AZ database. Place the master node into a multi-AZ auto-scaling group with a minimum of one and maximum of one with health checks. <br>  ローカルデータベースをマルチAZデータベースに移行します。マスターノードを複数のAZ自動スケーリンググループに配置し、最小1つ、最大1つのヘルスチェックを行います。|<p><br></p><p>Option A is CORRECT because (i) for database, Multi-AZ architecture provides high availability and can meet shortest of RTO and RPO requirements in case of failures, since it uses synchronous replication and maintains standby instance which gets promoted to primary, and (ii) for master server, it uses auto scaling which ensures that at least one server is always running.</p><p>Option B is incorrect because ELB cannot ensure the minimum or maximum number of instances running.</p><p>Option C is incorrect because (i) read replicas do not provide high availability, and (ii) ELB cannot ensure the minimum or maximum number of instances running.</p><p>Option D is incorrect because read replicas do not provide high availability.</p><p><br></p><p><b>More information on Multi-AZ RDS architecture:</b></p><p>Multi-AZ is used for highly available architecture. If a failover happens, the secondary DB which is a synchronous replica will have the data, and it’s just the CNAME which changes. For Read replica, it’s primarily used for distributing workloads.</p><p><br></p><p>For&nbsp; more information on Multi-AZ RDS, please refer to the below link</p><p></p><a href="https://aws.amazon.com/rds/details/multi-az/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/rds/details/multi-az/</a><br><p></p><p><br></p>	Migrate the local database into Multi-AZ database. Place the master node into a Cross Zone ELB with a minimum of one and maximum of one with health checks <br>  ローカルデータベースをマルチAZデータベースに移行します。マスターノードを最小1個、最大1個のクロスゾーンELBに配置し、ヘルスチェックを行います	Replicate the local database into a RDS Read Replica. Place the master node into a Cross Zone ELB with a minimum of one and maximum of one with health checks <br>  ローカルデータベースをRDS読み取りレプリカにレプリケートします。マスターノードを最小1個、最大1個のクロスゾーンELBに配置し、ヘルスチェックを行います	Replicate the local database into a RDS Read Replica.Place the master node into a multi-AZ auto-scaling group with a minimum of one and maximum of one with health checks. <br>  ローカルデータベースをRDS読み取りレプリカにレプリケートします。ヘルスチェックを使用して、マスターノードを1つ以上、最大1つのマルチAZ自動スケールグループに配置します。
Test3-35. <p>You're migrating an existing application to the AWS cloud. The application will be primarily using EC2 instances. This application needs to be built with the highest availability architecture available. The application currently relies on hardcoded hostnames for intercommunication between the three tiers. You've migrated the application and configured the multi-tiers using the internal Elastic Load Balancer for serving the traffic. The load balancer hostname is demo-app.us-east-1.elb.amazonaws.com. The current hard-coded hostname in your application used to communicate between your multi-tier application is demolayer.example.com. What is the best method for architecting this setup to have as much high availability as possible? <br></p><p>Choose the correct answer from the below options:<br></p> | <p>既存のアプリケーションをAWSクラウドに移行しようとしています。アプリケーションは主にEC2インスタンスを使用します。このアプリケーションは、利用可能な最高の可用性アーキテクチャで構築する必要があります。アプリケーションは現在、3つの層の相互通信のためにハードコードされたホスト名に依存しています。アプリケーションを移行し、トラフィックを処理するために内部Elastic Load Balancerを使用してマルチティアを設定しました。ロードバランサのホスト名はdemo-app.us-east-1.elb.amazonaws.comです。マルチレイヤアプリケーション間の通信に使用される、アプリケーション内の現在のハードコードされたホスト名は、demolayer.example.comです。できるだけ多くの高可用性を持たせるためにこのセットアップを設計する最良の方法は何ですか？<br> </p> <p>以下のオプションから正解を選んでください：<br> <	sa:	Create a private resource record set using Route 53 with a hostname of demolayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com. <br>  ホスト名がdemolayer.example.com、エイリアスレコードがdemo-app.us-east-1.elb.amazonaws.comのルート53を使用して、プライベートリソースレコードセットを作成します。|<p><br></p><p></p><p>Since demolayer.example.com is an internal DNS record, the best way is Route 53 to create an internal resource record. One can then point the resource record to the create ELB.</p><p>While ordinary Amazon Route&nbsp;53 resource record sets are standard DNS resource record sets,&nbsp;<i>alias resource record sets</i>&nbsp;provide an Amazon Route&nbsp;53–specific extension to DNS functionality. Instead of an IP address or a domain name, an alias resource record set contains a pointer to a CloudFront distribution, an Elastic Beanstalk environment, an ELB Classic or Application Load Balancer, an Amazon S3 bucket that is configured as a static website, or another Amazon Route&nbsp;53 resource record set in the same hosted zone.&nbsp;</p><p>&nbsp;</p><p>Option A is incorrect because it does not mention how the mapping between the existing hard-coded host name and the ELB host name.</p><p>Option B is CORRECT because it creates an internal ALIAS record set where it defines the mapping between the hard-coded host name and the ELB host name that is to be used.</p><p>Option C and D are incorrect because it should create a private record set, not public, since the mapping between the hard-coded host name and ELB host name should be done internally.</p><p>&nbsp;</p><p>For more information on alias and non-alias records please refer to the below link</p>  <a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html" target="_blank">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a><br><p></p>	Create an environment variable passed to the EC2 instances using user-data with the ELB hostname, demo-app.us-east-1.elb.amazonaws.com. <br>  ELBホスト名demo-app.us-east-1.elb.amazonaws.comを持つユーザーデータを使用して、EC2インスタンスに渡される環境変数を作成します。	Create a public resource record set using Route 53 with a hostname of demolayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com. <br>  ホスト名がdemolayer.example.com、エイリアスレコードがdemo-app.us-east-1.elb.amazonaws.comのルート53を使用して、パブリックリソースレコードセットを作成します。	Add a cname record to the existing on-premise DNS server with a value of demo-app.us-east-1.elb.amazonaws.com. Create a public resource record set using Route 53 with a hostname of applayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com. <br>  demo-app.us-east-1.elb.amazonaws.comの値を使用して、既存のオンプレミスDNSサーバーにcnameレコードを追加します。applayer.example.comのホスト名とdemo-app.us-east-1.elb.amazonaws.comのエイリアスレコードを持つRoute 53を使用してパブリックリソースレコードセットを作成します。
Test3-36. <p>A company has an application that is hosted on an EC2 instance. The code is written in .NET and connects to a MySQL RDS database. If you're executing .NET code against AWS on an EC2 instance that is assigned an IAM role, which of the following is a true statement? <br></p><p>Choose the correct option from the below:</p> | <p>企業には、EC2インスタンスでホストされているアプリケーションがあります。このコードは.NETで記述され、MySQL RDSデータベースに接続します。IAMロールが割り当てられているEC2インスタンスでAWSに対して.NETコードを実行している場合、真のステートメントはどれですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	The code will assume the same permissions as the EC2 role <br>  コードはEC2の役割と同じ権限を引き受けます|<p><br></p><p>The best practice for IAM is to create roles which have specific access to an AWS service and then give the user permission to the AWS service via the role.</p><p>To get the role in place, follow the below steps</p><p>Step 1) Create a role which has the required ELB access</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_36_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="199" width="632"></p><p>Step 2) You need to provide permissions to the underlying EC2 instances in the Elastic Load Balancer</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_36_2.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="170" width="1039"></p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/3_36_3.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="74" width="1029"><br></p><p>&nbsp;</p><p>For the best practices on IAM policies, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a><br><p></p>	The code must have AWS access keys in order to execute <br>  実行するにはコードにAWSアクセスキーが必要です	Only .NET code can assume IAM roles <br>  .NETコードのみがIAMの役割を引き受ける	None of the above <br>  上記のどれでもない
Test3-37. <p>A company is making extensive use of S3. They have a strict security policy and require that all artifacts are stored securely in S3. Which of the following request headers, when specified in an API call, will cause an object to be SSE? <br></p><p>Choose the correct option from the below:</p> | <p>ある企業がS3を大量に利用しています。彼らは厳しいセキュリティ方針を持っており、すべての成果物がS3に安全に保管されることを要求しています。次の要求ヘッダーのどれがAPI呼び出しで指定された場合、オブジェクトがSSEになるのでしょうか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	x-amz-server-side-encryption <br>  x-amz-server-side-encryption|<p><br></p><p>Server-side encryption is about protecting data at rest. Server-side encryption with Amazon S3-managed encryption keys (SSE-S3) employs strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p>The object creation REST APIs (see&nbsp;Specifying Server-Side Encryption Using the REST API) provides a request header,&nbsp;x-amz-server-side-encryption&nbsp;that you can use to request server-side encryption.</p><p><span style="font-size: 1rem;">To encrypt an object at the time of upload, you need to add a header called&nbsp;x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS. The following code example shows a&nbsp;Put&nbsp;request using SSE-S3.</span></p><p></p><div><pre>PUT /example-object HTTP/1.1 Host: myBucket.s3.amazonaws.com Date: Wed, 8 Jun 2016 17:50:00 GMT Authorization: authorization string&nbsp;&nbsp; Content-Type: text/plain Content-Length: 11434 x-amz-meta-author: Janet Expect: 100-continue <strong>x-amz-server-side-encryption: AES256</strong> [11434 bytes of object data] </pre></div><p>In order to enforce object encryption, create an S3 bucket policy that denies any S3&nbsp;Put&nbsp;request that does not include the&nbsp;x-amz-server-side-encryption&nbsp;header. There are two possible values for the&nbsp;x-amz-server-side-encryption&nbsp;header:&nbsp;AES256, which tells S3 to use S3-managed keys, and&nbsp;aws:kms, which tells S3 to use AWS KMS–managed keys.</p><br><p></p><p>For more information on S3 encryption, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a><br><a href="https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/" target="_blank">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a><br><p></p>	amz-server-side-encryption <br>  amz-server-side-encryption	AES256 <br>  AES256	server-side-encryption <br>  サーバー側暗号化
Test3-38. <p>You decide to create a bucket on AWS S3 called 'mybucket' and then perform the following actions in the order that they are listed here.</p><p>- You upload a file to the bucket called 'file1'</p><p>- You enable versioning on the bucket</p><p>- You upload a file called 'file2'</p><p>- You upload a file called 'file3'</p><p>- You upload another file called 'file2'</p><p>Which of the following is true for 'mybucket'? Choose the correct option from the below:</p> | <p> AWS S3で 'mybucket'という名前のバケットを作成し、ここに記載されている順序で次の操作を実行することにしました。</p> <p>  -  'file1'バケットのバージョン管理を有効にする</p> <p>  -  'file2'というファイルをアップロードする</p> <p>  -  'file3'というファイルをアップロードする</p> <p>  -  'file2'という別のファイルをアップロードする</p> <p> 'mybucket'には次のうちどれですか？下記から正しいオプションを選択してください：</p>	sa:	The version ID for file1 will be null, there will be 2 version IDs for file2 and 1 version ID for file3 <br>  file1のバージョンIDはnullになります。file2には2つのバージョンIDがあり、file3には1のバージョンIDがあります|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Objects stored in your bucket before you set the versioning state have a version ID of null.&nbsp;</span><span style="font-size: 1rem;">When you enable versioning, existing objects in your bucket do not change. What changes is how Amazon S3 handles the objects in future requests.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because the version ID for file1 would be null.</span></p><p><span style="font-size: 1rem;">Option B is CORRECT because the file1 was put in the bucket before the versioning was enabled; hence, it will have null version ID. The file2 will have two version IDs, and file3 will have a single version ID.</span></p><p><span style="font-size: 1rem;">Option C is incorrect because file2 cannot have a null version ID as the versioning was enabled before putting it in the bucket.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because once the versioning is enabled, all the files put <i>after</i>&nbsp;that will not have null version ID. But file1 was put <i>before </i>versioning was enabled, so it will have null as its version ID.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on S3 versioning, please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a><br><p></p>	There will be 1 version ID for file1, there will be 2 version IDs for file2 and 1 version ID for file3 <br>  file1には1つのバージョンIDがあり、file2には2つのバージョンIDがあり、file3には1つのバージョンIDがあります	There will be 1 version ID for file1, the version ID for file2 will be null and there will be 1 version ID for file3 <br>  file1のバージョンIDは1つで、file2のバージョンIDはnullになり、file3のバージョンIDは1つになります	All file version ID's will be null because versioning must be enabled before uploading objects to 'mybucket' <br>  オブジェクトを 'mybucket'にアップロードする前にバージョン管理を有効にする必要があるため、すべてのファイルバージョンIDはnullになります
Test3-39. <p>One of your requirements is to setup an S3 bucket to store your files like documents and images. However, those objects should not be directly accessible via the S3 URL, they should only be accessible from pages on your website so that only your paying customers can see them. How could you implement this? <br></p><p>Choose the correct option from the below:</p> | <p>あなたの要件の1つは、文書や画像のようなファイルを保存するためのS3バケットをセットアップすることです。ただし、これらのオブジェクトはS3 URL経由で直接アクセスできないようにする必要があります。有料の顧客のみが閲覧できるように、Webサイトのページからアクセス可能である必要があります。どのようにこれを実装することができますか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	You can use a bucket policy and check for the AWS: Referer key in a condition, where that key matches your domain <br>  バケットポリシーを使用して、そのキーがあなたのドメインと一致する条件でAWS：Refererキーをチェックすることができます|<p><br></p><p>Suppose you have a website with the domain name (www.example.com&nbsp;or&nbsp;example.com) with links to photos and videos stored in your S3 bucket,&nbsp;examplebucket. By default, all the S3 resources are private, so only the AWS account that created the resources can access them. To allow read access to these objects from your website, you can add a bucket policy that allows&nbsp;s3:GetObject&nbsp;permission with a condition, using theaws:referer&nbsp;key, that the get request must originate from specific web pages.</p><p><br></p><p>Option A is incorrect because HTTPS endpoint will not ensure that only authenticated users can get access to the content.&nbsp;</p><p>Option B is CORRECT because it defines appropriate bucket policy to give the access to the S3 content to the authenticated users.</p><p>Option C is incorrect because you can control the access to the S3 content via bucket policy.</p><p>Option D is incorrect because the question is not about encrypting/decrypting the data. To give access to the S3 content to certain users, proper bucket policy needs to be defined.</p><p><br></p><p>For more information on S3 bucket policy examples, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a><br><p></p>	Use HTTPS endpoints to encrypt your data. <br>  HTTPSエンドポイントを使用してデータを暗号化します。	You can't. The S3 URL must be public in order to use it on your website. <br>  できません。S3 URLは、ウェブサイトで使用するために公開されている必要があります。	You can use server-side and client-side encryption, where only your application can decrypt the objects <br>  サーバー側およびクライアント側の暗号化を使用できます。アプリケーションでは、オブジェクトのみを復号化できます
Test3-40. <p>While hosting a static website with Amazon S3, your static JavaScript code attempts to include resources from another S3 bucket but permission is denied. How might you solve the problem?</p><p> Choose the correct option from the below:</p> | <p> Amazon S3で静的なWebサイトをホストしている間、静的なJavaScriptコードは別のS3バケットからのリソースを含めようとしますが、許可は拒否されます。</p> <p>以下から正しいオプションを選択してください：</p>	sa:	Enable CORS Configuration <br>  CORS設定を有効にする|<p><br></p><p>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support in Amazon S3, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p><p><br></p><p>For more information on S3 CORS configuration, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a><br><p></p>	Disable Public Object Permissions <br>  パブリックオブジェクトのアクセス許可を無効にする	Move the object to the main bucket <br>  オブジェクトをメインバケットに移動する	None of the above <br>  上記のどれでもない
Test3-41. <p>You are having trouble maintaining session states on some of your applications that are using an Elastic Load Balancer(ELB). There does not seem to be an even distribution of sessions across your ELB. Which of the following is the recommended method by AWS to try and rectify the issues to overcome this problem that you are having? <br></p><p>Choose the correct option from the below:</p><p>&nbsp;</p> | <p> Elastic Load Balancer（ELB）を使用しているアプリケーションのセッション状態を維持するのに問題があります。あなたのELBにはセッションが均等に配られていないようです。AWSによるこの問題を解決するための問題を解決するために推奨される方法はどれですか？<br> </p> <p>下記から正しいオプションを選択してください：</p> <p>＆nbsp; </p>	sa:	Use ElastiCache, which is a web service that makes it easy to set up, manage, and scale a distributed in-memory cache environment in the cloud. <br>  クラウド内の分散メモリ内キャッシュ環境の設定、管理、および拡張を容易にするWebサービスであるElastiCacheを使用します。|<p><br></p><p>Option A is CORRECT because ElastiCache can be utilized to store the session state in cache rather than in any database. It also improves the performance by allowing you to quickly retrieve the session state information.</p><p>Option B and D are incorrect because the cookies will only help identifying the instance which would be tied to the request. It will not store any session state.</p><p>Option C is incorrect because sticky session allows the ELB to bind the user session to a particular instance, but it will not store any session state.</p><p><br></p><p><b>More information on Amazon ElastiCache</b></p><p>Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in&nbsp;the cloud. Amazon ElastiCache improves the performance of web applications by allowing you to retrieve information from a fast, managed, in-memory system, instead of relying entirely on slower disk-based databases. The service simplifies and offloads the management, monitoring, and operation of in-memory environments, enabling your engineering resources to focus on developing applications. Using Amazon ElastiCache, you can not only improve load and response times to user actions and queries but also reduce the cost associated with scaling web applications.</p><p>As an example for application session stickiness using Elastic cache, please refer to the below link</p><p></p><a href="https://aws.amazon.com/blogs/developer/elasticache-as-an-asp-net-session-store/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/blogs/developer/elasticache-as-an-asp-net-session-store/</a><br><p></p><p>&nbsp;</p>	Use a special cookie to track the instance for each request to each listener. When the load balancer receives a request, it will then check to see if this cookie is present in the request. <br>  特定のCookieを使用して、各リスナーへの各要求のインスタンスを追跡します。ロードバランサは要求を受け取ると、要求にこのクッキーが存在するかどうかを確認します。	Use the sticky session feature (also known as session affinity), which enables the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. <br>  セッション・アフィニティとも呼ばれるスティッキ・セッション機能を使用します。これにより、ロード・バランサはユーザーのセッションを特定のインスタンスにバインドできます。これにより、セッション中にユーザーからのすべての要求が同じインスタンスに確実に送信されます。	If your application does not have its own session cookie, then you can configure Elastic Load Balancing to create a session cookie by specifying your own stickiness duration. <br>  アプリケーションに独自のセッションCookieがない場合は、独自の粘着性の持続時間を指定してセッションCookieを作成するようにElastic Load Balancingを設定できます。
Test3-42. <p>You are deploying your first EC2 instance in AWS and are using the AWS console to do this. You have chosen your AMI and your instance type and have now come to the screen where you configure your instance details. One of the things that you need to decide is whether you want to auto-assign a public IP address or not. You assume that if you do not choose this option you will be able to assign an Elastic IP address later, which happens to be a correct assumption. Which of the below options best describes why an Elastic IP address would be preferable to a public IP address? <br></p><p>Choose the correct option from the below:</p> | <p>最初のEC2インスタンスをAWSにデプロイし、AWSコンソールを使用してこの作業を行っています。あなたはあなたのAMIとあなたのインスタンスタイプを選択して、あなたのインスタンスの詳細を設定するスクリーンに来ました。あなたが決定する必要があることの1つは、パブリックIPアドレスを自動割り当てするかどうかです。このオプションを選択しないと、あとでElastic IPアドレスを割り当てることができると想定していますが、これは正しい仮定になります。以下のオプションのうち、Elastic IPアドレスがパブリックIPアドレスよりも好ましい理由を最もよく説明しているのはどれですか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. <br>  エラスティックIPアドレスを使用すると、アカウント内の別のインスタンスにアドレスを迅速に再マッピングすることで、インスタンスまたはソフトウェアの障害を隠すことができます。|<p><br></p><p>Option A is incorrect because public IP addresses are free.</p><p>Option B is CORRECT because in case of an instance failure, you can reassign the EIP to a new instance, thus you do not need to change any reference to the IP address in your application.</p><p>Option C is incorrect because the number of EIPs per account per region is limited (5).</p><p>Option D is incorrect because EIPs are accessible from the internet.</p><p><br></p><p><b>More information on EIPs</b></p><p><span></span></p><p style="">An&nbsp;<em style="">Elastic IP address</em>&nbsp;is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p><p style="">An Elastic IP address is a public IPv4 address, which is reachable from the internet. If your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the internet; for example, to connect to your instance from your local computer.</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html</a><br><p></p>	An Elastic IP address is free, whilst you must pay for a public IP address. <br>  弾性IPアドレスは無料ですが、公衆IPアドレスを支払う必要があります。	You can have an unlimited amount of Elastic IP addresses, however public IP addresses are limited in number. <br>  無制限のElastic IPアドレスを使用できますが、パブリックIPアドレスの数には限りがあります。	An Elastic IP address cannot be accessed from the internet like a public IP address and hence is safer from a security standpoint. <br>  弾性IPアドレスは、パブリックIPアドレスのようにインターネットからアクセスすることはできないため、セキュリティの観点からは安全です。
Test3-43. <p>You have an EBS root device on /dev/sda1 on one of your EC2 instances. You are having trouble with this particular instance and you want to either Stop/Start, Reboot or Terminate the instance but you do not want to lose any data that you have stored on /dev/sda1. Which of the below statements best describes the effect each change of instance state would have on the data you have stored on /dev/sda1? <br></p><p>Choose the correct option from the below:</p> | <p> EC2インスタンスの1つで、/ dev / sda1にEBSルートデバイスがあります。この特定のインスタンスに問題があり、インスタンスを停止/再起動、または終了する必要がありますが、/ dev / sda1に保存したデータは失われたくありません。以下のステートメントのどれが、/ dev / sda1に保存したデータにインスタンス状態の各変更が与える影響を最もよく表していますか？<br> </p> <p>以下から正しいオプションを選択してください：</p>	sa:	The data in an instance store is not permanent - it persists only during the lifetime of the instance. The data will be lost if you terminate the instance. However, the data will remain on /dev/sda1 if you reboot or stop/start the instance because data on an EBS volume is not ephemeral. <br>  インスタンスストア内のデータは永続的ではなく、インスタンスの存続期間中のみ持続します。インスタンスを終了すると、データは失われます。ただし、EBSボリューム上のデータが一時的なものではないため、インスタンスを再起動または停止/開始すると、データは/ dev / sda1に残ります。|<p><br></p><p>Since this is an EBS backed instance, it can be stopped and later restarted without affecting data stored in the attached volumes. By default, the root device volume for this instance will be deleted when the instance terminates.&nbsp;</p><p><br></p><p>Option A is incorrect because upon termination, the volume would get deleted and the data would get lost (DeleteOnTermination setting is not mentioned, so this is a default case).</p><p>Option B is incorrect because the data on EBS volume would not get lost upon stop/start or reboot.</p><p>Option C is incorrect because the data on EBS volume would not get lost upon reboot.</p><p>Option D is CORRECT because the data on EBS volume would not get lost upon stop/start or reboot as it is not ephemeral. Instance store, on the other hand, is an ephemeral storage and the data would get lost upon starting/stopping of the instance (This is an extra information given in the option, which may not be related to the question, but is correct nonetheless).&nbsp;</p><p><br></p><p><b>More information on this topic:</b></p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ComponentsAMIs.html</a><br><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html</a><br><br><p></p>	Whether you stop/start, reboot or terminate the instance it does not matter because data on an EBS volume is ephemeral and it will be lost no matter what method is used. <br>  EBSボリューム上のデータは一時的なものであり、使用される方法にかかわらず失われるため、インスタンスを停止/起動、再起動、終了するかどうかは関係ありません。	If you stop/start the instance the data will not be lost. However, if you either terminate or reboot the instance the data will be lost. <br>  インスタンスを停止/開始すると、データは失われません。ただし、インスタンスを終了または再起動すると、データが失われます。	Whether you stop/start, reboot or terminate the instance it does not matter because data on an EBS volume is not ephemeral and the data will not be lost regardless of what method is used <br>  EBSボリューム上のデータが一時的なものではなく、使用される方法にかかわらずデータが失われないので、インスタンスを停止/起動、再起動、終了するかどうかは関係ありません
Test3-44. <p><span style="font-size: 1rem;">Someone on your team configured a Virtual Private Cloud with two public subnets in two separate AZs and two private subnets in two separate AZs. Each public subnet AZ has a matching private subnet AZ. The VPC and its subnets are properly configured. You also notice that there are multiple webserver instances in the private subnet, and you've been charged with setting up a public-facing Elastic Load Balancer which will accept requests from clients and distribute those requests to the webserver instances. How can you set this up without making any significant architectural changes? Choose the correct option from the below:</p> | あなたのチームの誰かが、2つの別個のAZに2つのパブリックサブネットを、2つの別個のAZに2つのプライベートサブネットを持つ仮想プライベートクラウドを構成しました。 各パブリックサブネットAZには、一致するプライベートサブネットAZがあります。 VPCとそのサブネットは正しく設定されています。 また、プライベートサブネットに複数のWebサーバーインスタンスがあり、クライアントからの要求を受け入れ、それらの要求をWebサーバーインスタンスに配布する、公開用のElastic Load Balancerを設定することになりました。 下記から正しいオプションを選択してください：</p>	sa:	Select both of the public subnets when configuring the ELB. <br>  ELBを構成するときは、両方のパブリックサブネットを選択します。|<p><br></p><p>Option A is incorrect because you need to setup the internet facing load balancer, to which the public subnets need to be associated.</p><p>Option B is incorrect because webservers need to remain in the private subnets. Shifting them to the public subnet would be a significant architectural change.</p><p>Option C is CORRECT because you need to associate the public subnets with the internet facing load balancer. You would also need to ensure that the security group that is assigned to the load balancer has the listener ports open and the security groups of the private instances allow traffic on the listener ports and the health check ports.</p><p>Option D is incorrect because you can configure the internet facing load balancer with the public subnet.&nbsp;</p><p><br></p><p>For more information on the AWS ELB, please refer to the below link:</p><p><a href="https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/" target="_blank">https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a><br></p><p><a href="https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/" target="_blank">https://aws.amazon.com/elasticloadbalancing/classicloadbalancer/</a><br></p><p></p><p></p><p></p>	Put the webserver instances in the public subnets and then configure the ELB with those subnets. <br>  Webサーバインスタンスをパブリックサブネットに配置し、それらのサブネットでELBを設定します。	Select both of the private subnets which contain the webserver instances when configuring the ELB. <br>  ELBを構成するときに、Webサーバーインスタンスを含むプライベートサブネットの両方を選択します。	You can't. Webserver instances must be in public subnets in order for this to work. <br>  できません。これを行うには、Webサーバーインスタンスがパブリックサブネットに存在する必要があります。
Test3-45. <p>A company has hired you to assist with the migration of an interactive website that allows registered users to rate local restaurants. Updates to the ratings are displayed on the home page and ratings are updated in real time. Although the website is not very popular today, the company anticipates that it will grow over the next few weeks. They also want to ensure that the website to remain highly available. The current architecture consists of a single Windows server 2008R2 web server and a MySQL database on Linux. Both reside inside on an on-premise hypervisor. What would be the most efficient way to transfer the application to AWS, ensuring high performance and availability?</p> | <p>登録ユーザーが地元のレストランを評価できるようにするインタラクティブなウェブサイトの移行を支援するために、あなたを雇った会社があります。レーティングの更新はホームページに表示され、レーティングはリアルタイムで更新されます。今日のウェブサイトはあまり人気がありませんが、今後数週間で成長すると予測しています。彼らはまた、ウェブサイトが高可用性を保つことを確実にしたい。現在のアーキテクチャは、単一のWindowsサーバー2008R2 WebサーバーとLinux上のMySQLデータベースで構成されています。どちらも社内のハイパーバイザー上にあります。高性能と可用性を確保するために、アプリケーションをAWSに転送する最も効率的な方法は何でしょうか？</p>	sa:	Launch one Windows Server 2008 R2 instance in us-west-1b and one in us-west-1a and configure auto-scaling. Copy the web files from on premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. Launch a multi -AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Create an elastic load balancer (ELB) to front your web servers. Use Route 53 and create an alias record pointing to the ELB. <br>  us-west-1bでは1つのWindows Server 2008 R2インスタンスを起動し、us-west-1aでは1つを起動して自動スケーリングを設定します。 Amazon S3をリポジトリとして使用して、オンサイトWebサーバーから各Amazon EC2 WebサーバーにWebファイルをコピーします。 us-west-1aで複数-AZ MySQL Amazon RDSインスタンスを起動します。 最新のMySQLバックアップからAmazon RDSにデータをインポートします。 あなたのWebサーバーの前に弾性ローダーバランサー（ELB）を作成します。 ルート53を使用し、ELBを指すエイリアスレコードを作成します。|<p>The main consideration in the question is that the architecture should be highly available with high performance.</p><p><br></p><p>Option A is CORRECT because (a) EC2 servers can communicate with S3 for the web files, and (b) auto-scaling of web servers and the setup of Multi-AZ RDS instance as well as the Route 53 alias record with ELB provides high availability.</p><p>Option B is incorrect because this is an interactive website and S3 is suitable for static website.</p><p>Option C is incorrect because Route 53 should create an Alias Record, not A record.</p><p>Option D is incorrect because, even though it tries to set up the ELB with Route 53 record set, it actually does not create an ELB.</p><p></p><br><span style="font-size: 1rem;">For more information, please refer to the below URL</span><p></p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a><br><p></p>	Use AWS VM Import/Export to create an Amazon EC2 AMI of the web server. Configure auto-scaling to launch one web server in us-west-1a and one in us-west-1b. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data Into Amazon RDS from the latest MySQL backup. Create an elastic load balancer (ELB) in front of your web servers. Use Amazon Route 53 and create an A record pointing to the ELB. <br> AWS VMのインポート/エクスポートを使用して、WebサーバーのAmazon EC2 AMIを作成します。 us-west-1aとus-west-1bに1つのWebサーバーを起動するように自動スケーリングを設定します。 us-west-1aで複数のAZのMySQL Amazon RDSインスタンスを起動します。 最新のMySQLバックアップからAmazon RDSにデータをインポートします。 Webサーバーの前に弾性ロードバランサ（ELB）を作成します。 Amazon Route 53を使用し、ELBを指すAレコードを作成します。	Use AWS VM Import/Export to create an Amazon EC2 AMI of the web server. Configure auto-scaling to launch one web server in us-west-1a and one in us-west-1b. Launch a Multi-AZ MySQL Amazon RDS instance in us-west-1b. Import the data into Amazon RDS from the latest MySQL backup. Use Amazon Route 53 to create a hosted zone and point an A record to the elastic load balancer. <br>  AWS VMのインポート/エクスポートを使用して、WebサーバーのAmazon EC2 AMIを作成します。 us-west-1aとus-west-1bに1つのWebサーバーを起動するように自動スケーリングを設定します。 us-west-1bで複数のAZのMySQL Amazon RDSインスタンスを起動します。 最新のMySQLバックアップからAmazon RDSにデータをインポートします。 Amazonルート53を使用してホストゾーンを作成し、Aレコードをエラスティックロードバランサに向けます。	 Export web files to an Amazon 53 bucket in us-west-1. Run the website directly out of Amazon 53. Launch a multi-AZ MySQL Amazon RDS instance in us-west-1a. Import the data into Amazon RDS from the latest MySQL backup. Use Route 53 and create an alias record pointing to the elastic load balancer.<br>us-west-1のAmazon 53バケットにWebファイルをエクスポートします。 Amazon Webサイトから直接実行する53. us-west-1aで複数のAZ MySQL Amazon RDSインスタンスを起動する。 最新のMySQLバックアップからAmazon RDSにデータをインポートします。 ルート53を使用し、エラスティックロードバランサを指すエイリアスレコードを作成します。
Test3-46. <p></p><span id="docs-internal-guid-846a0cce-310a-43e7-037a-b1198a2b9e81">You have been asked to design the storage layer for an application. The application requires disk performance of at least 100,000 IOPS. In addition, the storage layer must be able to survive the loss of an individual disk, EC2 instance, or Availability Zone without any data loss. The volume you provide must have a capacity of at least 3 TB. Which of the following designs will meet these objectives?</span><p></p> | <p> </p> <span id = "docs-internal-guid-846a0cce-310a-43e7-037a-b1198a2b9e81">アプリケーション用のストレージレイヤを設計するように求められました。このアプリケーションには、少なくとも100,000 IOPSのディスク性能が必要です。さらに、ストレージレイヤーは、個々のディスク、EC2インスタンス、または可用性ゾーンが失われてもデータを失うことなく生き残ることができなければなりません。提供するボリュームの容量は、少なくとも3 TB以上でなければなりません。これらの目標を達成するには、次のデザインのどれですか？</ span> <p> </p>	sa:	Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD ephemeral disks provided with the Instance Configure synchronous block-level replication to an identically configured instance in us-east-1b. <br>  us-east-1aにi2.8xlargeインスタンスをインスタンス化します。インスタンスと共に提供される4つの800GB SSDエフェメラルディスクを使用してRAID 0ボリュームを作成します。us-east-1bの同じ構成のインスタンスに対して同期ブロックレベルの複製を設定します。|<br><p dir="ltr" style="">Option A is incorrect because this configuration is done entirely in a single AZ. There will be a data loss if the entire AZ goes down.</p><p dir="ltr" style="">Option B is CORRECT because (a) it uses RAID 0 configuration that utilizes all the volumes and gives the aggregated IOPS performance, and (b) the replication across another AZ gives higher availability and fault tolerance even in case of an entire AZ becomes unavailable.</p><p dir="ltr" style="">Option C is incorrect because it uses asynchronous backup of the data. The problem scenario demands a synchronous replication to prevent any data loss.</p><p dir="ltr" style="">Option D is incorrect because, RAID 5 is not recommended for Amazon EBS since the parity write operations consume some of the IOPS available to the volumes. See the link below for more details.</p><p dir="ltr" style=""></p><ul><li><a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/raid-config.html" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/raid-config.html</a></li><li><a href="https://en.wikipedia.org/wiki/Standard_RAID_levels" style="font-size: 1rem;">https://en.wikipedia.org/wiki/Standard_RAID_levels</a></li></ul><p></p><p dir="ltr" style="">Option E is incorrect because, even if the snapshots are taken every 15 minutes, there are chances that there will be data loss during this time. The requirement is that there should be absolutely no data loss.</p></span><br><p></p>	Instantiate an i2.8xlarge instance in us-east-1a. Create a RAID 0 volume using the four 800GB SSD ephemeral disks provided with the instance. Provision 3×1 TB EBS volumes attach them to the instance and configure them as a second RAID 0 volume. Configure synchronous, block-level replication from the ephemeral backed volume to the EBS-backed volume. <br>  us-east-1aにi2.8xlargeインスタンスをインスタンス化します。インスタンスに付属の4つの800GB SSDエフェメラルディスクを使用してRAID 0ボリュームを作成します。3×1 TBのEBSボリュームをインスタンスに接続し、2番目のRAID 0ボリュームとして構成します。一時バックアップボリュームからEBSバックアップボリュームへの同期ブロックレベル複製を構成します。	Instantiate a c3.8xlarge instance in us-east-1. Provision an AWS Storage Gateway and configure it for 3 TB of storage and 100,000 IOPS. Attach the volume to the instance. <br>  us-east-1でc3.8xlargeインスタンスをインスタンス化します。AWS Storage Gatewayをプロビジョニングし、3 TBのストレージと100,000 IOPSで構成します。ボリュームをインスタンスに接続します。	Instantiate a c3.8xlarge instance in us-east-1 provision 4x1TB EBS volumes, attach them to the instance, and configure them as a single RAID 5 volume Ensure that EBS snapshots are performed every 15 minutes. <br>  us-east-1の4x1TB EBSボリュームをc3.8xlargeインスタンスでインスタンシエートし、インスタンスに接続し、単一のRAID 5ボリュームとして構成します。EBSスナップショットが15分ごとに実行されるようにします。	Instantiate a c3 8xlarge Instance in us-east-1 Provision 3x1TB EBS volumes attach them to the instance, and configure them as a single RAID 0 volume. Ensure that EBS snapshots are performed every 15 minutes. <br>  us-east-1でc3 8 xlargeインスタンスをインスタンス化する3 x 1 TBのEBSボリュームをプロビジョニングしてインスタンスに接続し、単一のRAID 0ボリュームとして構成します。EBSスナップショットが15分ごとに実行されることを確認します。
Test3-47. <p>There are currently multiple applications hosted in a VPC. During monitoring, it has been noticed that multiple port scans are coming in from a specific IP Address block. The internal security team has requested that all offending IP Addresses be denied for the next 24 hours. Which of the following is the best method to quickly and temporarily deny access from the specified IP Addresses?</p> | <p>現在、VPCでホストされている複数のアプリケーションがあります。監視中に、特定のIPアドレスブロックから複数のポートスキャンが到着していることに気付きました。社内のセキュリティチームは、問題となっているすべてのIPアドレスが今後24時間は拒否されるように要求しています。指定したIPアドレスからのアクセスを迅速かつ一時的に拒否する最も良い方法はどれですか？</p>	sa:	Modify the Network ACLs associated with all public subnets in the VPC to deny access from the IP Address block <br>  VPC内のすべてのパブリックサブネットに関連付けられたネットワークACLを変更して、IPアドレスブロックからのアクセスを拒否します|<p><br></p><p></p><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p><p>&nbsp;</p><p>Option A and D are incorrect because (a) it will only work for windows-based instances, and (b) better approach is to block the traffic at the subnet layer via NACL rather than instance layer (windows firewall).</p><p>Option B is CORRECT because the best way to allow or deny IP address-based access to the resources in the VPC is to configure rules in the Network access control list (NACL) which are applied at the subnet level.</p><p>Option C is incorrect because (a) you cannot explicitly deny access to particular IP addresses via security group, and (b) better approach is to block the traffic at the subnet layer via NACL rather than instance layer (security group).</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/3_47_1.png" alt="" width="859" height="368" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>For more information on network ACL’s please refer to the below link</p>  <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a><br><p></p><ul></ul><p></p>	Create an AD policy to modify the Windows Firewall settings on all hosts in the VPC to deny access from the IP Address block. <br>  ADポリシーを作成して、VPC内のすべてのホストのWindowsファイアウォール設定を変更し、IPアドレスブロックからのアクセスを拒否します。	Add a rule to all of the VPC Security Groups to deny access from the IP Address block <br>  すべてのVPCセキュリティグループにルールを追加して、IPアドレスブロックからのアクセスを拒否する	Modify the Windows Firewall settings on all AMI's that your organization uses in that VPC to deny access from the IP address block <br>  組織がそのVPC内でIPアドレスブロックからのアクセスを拒否するために使用するすべてのAMIのWindowsファイアウォール設定を変更する
Test3-48. <p>You have been asked to leverage Amazon VPC EC2 and SQS to implement an application that submits and receives millions of messages per second to a message queue. You want to ensure that your application has sufficient bandwidth between your EC2 instances and SQS. Which option will provide the most scalable solution for communicating between the application and SQS?</p> | <p> Amazon VPC EC2とSQSを活用して、毎秒何百万ものメッセージをメッセージキューに送信して受信するアプリケーションを実装するよう求められました。アプリケーションでEC2インスタンスとSQSの間に十分な帯域幅が確保されていることを確認します。アプリケーションとSQS間の通信に最も拡張性の高いソリューションを提供するオプションはどれですか？</p>	sa:	Ensure the application instances are launched in public subnets with an Auto Scaling group and Auto Scaling triggers are configured to watch the SQS queue size. <br>  アプリケーションインスタンスが自動スケーリンググループを持つパブリックサブネットで起動され、自動スケーリングトリガーがSQSキューサイズを監視するように設定されていることを確認します。|<p>For the exam, remember that Amazon SQS is an Internet-based service. To connect to the Amazon SQS Endpoint (sqs.us-east-1.amazonaws.com), the Amazon EC2 instance requires access to the Internet. Hence, either it should be in a public subnet or be in a private subnet with a NAT instance/gateway in the public subnet.</p><p><br></p><p>Option A is incorrect because ELB does not ensure scalability.</p><p>Option B is incorrect because (a) EBS-optimized option will not contribute to scalability, and (b) there should be a NAT instance/gateway in the public subnet of the VPC for accessing SQS.</p><p>Option C is incorrect because if you remove the NAT instance, the EC2 instance cannot access SQS service.</p><p>Option D is CORRECT because (a) it uses Auto Scaling for ensuring scalability of the application, and (b) it has instances in the public subnet so they can access the SQS service over the internet.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_48_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="297" width="602"></p><p><br></p><p>For more information on SQS, please visit the below URL</p><p></p><a href="https://aws.amazon.com/sqs/faqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/sqs/faqs/</a><br><p></p><p>&nbsp;</p><p>&nbsp;</p>	Ensure the application instances are launched in private subnets with the EBS-optimized option enabled. <br>  EBS最適化オプションを有効にして、アプリケーションインスタンスがプライベートサブネットで起動されていることを確認します。	Ensure the application instances are launched in private subnets with the associate-public-IP-address=true option enabled. Remove any NAT instance from the public subnet, if any. <br>  アプリケーションインスタンスが、private-public-IP-address = trueオプションを有効にしてプライベートサブネットで起動されていることを確認します。パブリックサブネットからNATインスタンスがあればすべて削除します。	Ensure the application instances are properly configured with an Elastic Load Balancer. <br>  アプリケーションインスタンスがElastic Load Balancerで正しく構成されていることを確認します。
Test3-49. <p>You have two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets. One instance is running a database and the other instance an application that will interface with the database. You want to confirm that they can talk to each other for your application to work properly. Which two things do we need to confirm in the VPC settings so that these EC2 instances can communicate inside the VPC?</p><p>Choose 2 options from the below:</p> | <p>同一の可用性ゾーン（AZ）内の異なるサブネットにある仮想プライベートクラウド（VPC）内に2つのEC2インスタンスがあります。1つのインスタンスはデータベースを実行し、もう1つのインスタンスはデータベースとインターフェースするアプリケーションです。アプリケーションが正しく動作するためには、お互いに話すことができるかどうかを確認する必要があります。これらのEC2インスタンスがVPC内で通信できるように、VPC設定で確認する必要があるのはどちらですか？</p> <p>以下の2つのオプションを選択してください：</p>	ma:	o:A network ACL that allows communication between the two subnets. <br>  2つのサブネット間の通信を可能にするネットワークACL。|<p></p><p>In order to have the instances communicate with each other, you need to properly configure both Security Group and Network access control lists (NACLs). For the exam, remember that Security Group operates at the instance level; where as, the NACL operates at subnet level.</p>  Option A is CORRECT because the security groups must be defined in order to allow web server to communicate with the database server. An example image from the AWS documentation is given below:<br><br><p><img src="https://s3.amazonaws.com/awssap/3_49_1.png" alt="" role="presentation" class="atto_image_button_text-bottom" height="573" width="462"></p><p></p><p><br></p><p>Option B is incorrect because it is not necessary to have the two instances of the same type or be using same key-pair.</p><p>Option C incorrect is because configuring NAT instance or NAT gateway will not enable the two servers to communicate with each other. NAT instance/NAT gateway are used to enable the communication between instances in the private subnets and internet.</p>  Option D is CORRECT because the two servers are in two separate subnets. In order for them to communicate with each other, you need to have the NACL’s configured as shown below:<p></p><p><br></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_49_2.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="445" width="398"></p><p><br></p><p>For more information on VPC and Subnets, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><p></p>	x:Both instances are the same instance class and using the same Key-pair. <br>  両方のインスタンスが同じインスタンスクラスで、同じキーペアを使用しています。	x:That the default route is set to a NAT instance or internet Gateway (IGW) for them to communicate. <br>  既定のルートがNATインスタンスまたはインターネットゲートウェイ（IGW）に設定されて通信すること。	o:Security groups are set to allow the application host to talk to the database on the right port/protocol. <br>  セキュリティグループは、アプリケーションホストが正しいポート/プロトコルでデータベースと通信できるように設定されています。
Test3-50. <p>Your team is excited about the use of AWS because now they have access to "programmable Infrastructure”. You have been asked to manage your AWS infrastructure In a manner similar to the way you might manage application code. You want to be able to deploy exact copies of different versions of your infrastructure, stage changes into different environments, revert back to previous versions, and identify what versions are running at any particular time (development test QA . production). Which approach addresses this requirement?</p> | <p>あなたのチームは、 "プログラム可能なインフラストラクチャ"にアクセスできるようになったので、AWSの使用に興奮しています。あなたはアプリケーションコードを管理する方法と同様の方法でAWSインフラストラクチャを管理するように求められています。さまざまなバージョンのインフラストラクチャの正確なコピーを展開し、さまざまな環境に変更を加え、以前のバージョンに戻し、特定の時点で実行されているバージョンを特定します（開発テストQA生産）。 >	sa:	Use AWS CloudFormation and a version control system like GIT to deploy and manage your infrastructure. <br>  AWS CloudFormationとGITなどのバージョン管理システムを使用して、インフラストラクチャを導入および管理します。|<p><br></p><p></p><p>You can use AWS Cloud Formation’s sample templates or create your own templates to describe the AWS resources, and any associated dependencies or runtime parameters, required to run your application. You don’t need to figure out the order for provisioning AWS services or the subtleties of making those dependencies work. CloudFormation takes care of this for you. After the AWS resources are deployed, you can modify and update them in a controlled and predictable way, in effect applying version control to your AWS infrastructure the same way you do with your software. You can also visualize your templates as diagrams and edit them using a drag-and-drop interface with the AWS CloudFormation Designer.</p><p>&nbsp;</p><p>Option A is incorrect because Cost Allocation Reports is not helpful for the purpose of the question.</p><p>Option B is incorrect because CloudWatch is used for monitoring the metrics pertaining to different AWS resources.</p><p>Option C is incorrect because it does not have the concept of programmable Infrastructure.</p><p>Option D is CORRECT because&nbsp;AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.</p><p>&nbsp;</p><p>For more information on CloudFormation, please visit the link:</p>  <a href="https://aws.amazon.com/cloudformation/" target="_blank">https://aws.amazon.com/cloudformation/</a><br><p></p><ul></ul><p></p>	Use AWS CloudWatch metrics and alerts along with resource tagging to deploy and manage your infrastructure. <br>  AWS CloudWatchのメトリックとアラートをリソースタギングとともに使用して、インフラストラクチャを導入および管理します。	Use AWS Beanstalk and a version control system like GIT to deploy and manage your infrastructure. <br>  AWS BeanstalkとGITのようなバージョン管理システムを使用して、インフラストラクチャを展開および管理します。	Use cost allocation reports and AWS Opsworks to deploy and manage your infrastructure. <br>  コスト割り当てレポートとAWS Opsworkを使用して、インフラストラクチャを展開および管理します。
Test3-51. <p>An organization is planning to use AWS for their production roll out. The organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3 and setup the ELB. Which of the below mentioned AWS services meets the requirement for making an orderly deployment of the software?</p> | <p>組織は、プロダクションロールアウトにAWSを使用する予定です。組織は、自動的にLAMPスタックを作成し、最新のPHPインストール可能ファイルをS3からダウンロードし、ELBをセットアップするように、導入のための自動化を実装したいと考えています。以下のAWSサービスのうち、ソフトウェアの整然とした展開を行うための要件を満たすものはどれですか？</p>	sa:	AWS Elastic Beanstalk <br>  AWS Elastic Beanstalk|<p></p><div></div><br><p>The Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services.</p><p>&nbsp;</p><p>We can simply upload code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. Meanwhile we can retain full control over the AWS resources used in the application and can access the underlying resources at any time.</p><p>&nbsp;</p><p>Hence, A is the CORRECT answer.</p><p><br></p><p>F<span style="font-size: 1rem;">or more information on launching a LAMP stack with Elastic Beanstalk:</span></p>  <a href="https://aws.amazon.com/getting-started/projects/launch-lamp-web-app/" target="_blank">https://aws.amazon.com/getting-started/projects/launch-lamp-web-app/</a><br><p></p>	AWS Cloudfront <br>  AWS Cloudfront	AWS Cloudformation <br>  AWSクラウドフォーメーション	AWS DevOps <br>  AWS DevOps
Test3-52. <p>A user is accessing RDS from an application. The user has enabled the Multi-AZ feature with the MS SQL RDS DB. During a planned outage how will AWS ensure that a switch from DB to a standby replica will not affect access to the application?</p> | <p>ユーザーがアプリケーションからRDSにアクセスしています。ユーザーは、MS SQL RDS DBを使用してマルチAZ機能を有効にしました。予定されている停止中に、AWSはDBからスタンバイレプリカへの切り替えがアプリケーションへのアクセスに影響しないことをどのように保証しますか？</p>	sa:	RDS uses DNS to switch over to stand by replica for seamless transition <br>  RDSはDNSを使用して、シームレスな移行のためにレプリカで待機するように切り替えます|<p><br></p><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure (for example, instance hardware failure, storage failure, or network disruption), Amazon RDS performs an automatic failover to the standby by changing the CNAME for the DB instance to point to the standby, so that you can resume database operations as soon as the failover is complete.&nbsp;</p><p><br></p><p>Option A is incorrect because there is no internal IP that is maintained by RDS.</p><p>Option B is CORRECT because, as mentioned above, RDS performs automatic failover by flipping the CNAME for the DB instance from primary to standby instance.</p><p>Option C is incorrect because there is no changes done by RDS in the hardware.</p><p>Option D is incorrect because with Multi-AZ there is no manual intervention needed for the failover.</p><p><br></p><p>For more information on RDS Multi-AZ please visit the link –</p><p></p><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a><br><p></p>	RDS will have an internal IP which will redirect all requests to the new DB <br>  RDSは内部IPを持ち、すべての要求を新しいDBにリダイレクトします	The switch over changes hardware so RDS does not need to worry about access <br>  スイッチオーバーによりハードウェアが変更されるため、RDSはアクセスを心配する必要はありません	RDS will have both the DBs running independently and the user has to manually switch over <br>  RDSは両方のDBを独立して実行し、ユーザーは手動で切り替える必要があります
Test3-53. <p>A user has launched a large EBS backed EC2 instance in the US-East-1a region. The user wants to achieve Disaster Recovery (DR) for that instance by creating another small instance in Europe. How can the user achieve DR?</p> | <p>ユーザーは、US-East-1a地域で大きなEBSバックアップEC2インスタンスを開始しました。ユーザーは、ヨーロッパで別の小さなインスタンスを作成して、そのインスタンスのDR（災害復旧）を達成したいと考えています。ユーザーはどのようにDRを達成できますか？</p>	sa:	Create an AMI of the instance and copy the AMI to the EU region. Then launch the instance from the EU AMI. <br>  インスタンスのAMIを作成し、AMIをEU地域にコピーします。その後、EU AMIからインスタンスを起動します。|<p><br></p><p>Option A and C are incorrect because you cannot directly copy the instance. You need to create AMI of each instance.</p><p>Option B is CORRECT because if you need an AMI across multiple regions, then you have to copy the AMI across regions. Note that by default AMI’s that you have created will not be available across all regions.</p><p><span style="font-size: 1rem;">Option D is incorrect because using "Launch More Like This..." enables you to use a current instance as a base for launching other instances in the same availability zone. It does not clone your selected instance; it only replicates some configuration details. To create a copy of your instance, first create an AMI from it, then launch more instances from the AMI.</span></p><p><br></p><p><span style="font-size: 1rem;">For the entire details to copy AMI’s, please visit the link -&nbsp;</span></p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a><br><br><p></p>	Copy the running instance using the “Instance Copy” command to the EU region. <br>  "Instance Copy"コマンドを使用して実行中のインスタンスをEU地域にコピーします。	Copy the instance from the US East region to the EU region. <br>  インスタンスを米国東部地域からEU地域にコピーします。	Use the “Launch more like this” option to copy the instance from one region to another. <br>  ある地域から別の地域にインスタンスをコピーするには、「これ以上起動する」オプションを使用します。
Test3-54. <p>You are managing the AWS account of a big organization. The organization has more than 1000+ employees and they want to provide access to the various services to most of the employees. Which of the below-mentioned options is the best possible solution in this case?</p> | <p>あなたは大きな組織のAWSアカウントを管理しています。 組織には1000人以上の従業員がいて、ほとんどの従業員にさまざまなサービスへのアクセスを提供したいと考えています。 この場合、以下のオプションのうち、どれが可能な限り最良の解決策ですか？</p>	sa:	Attach an IAM role with the organization’s authentication service to authorize each user for various AWS services. <br>  組織の認証サービスにIAMロールを添付して、各ユーザにさまざまなAWSサービスを認可します。|<p><br></p><p>The best practice for IAM is to create roles which have specific access to an AWS service and then give the user permission to the AWS service via the role.</p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because creating a separate IAM user is not a feasible solution here. Instead, creating an IAM role would be more appropriate solution.</span></p><p>Option B is incorrect because this is an invalid workflow of using IAM roles for authenticating the users.</p><p>Option C is incorrect because you should be creating IAM Role rather than IAM Users which will be added to the IAM group.</p><p>Option D is CORRECT because it authenticates the users with the organization’s authentication service and creates an appropriate IAM Role for accessing the AWS services.</p><p><br></p><p>For the best practices on IAM policies, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html</a><br><p></p>	The user should create an IAM role and attach STS with the role. The user should attach that role to the EC2? instance and setup AWS authentication on that server. <br>  ユーザーはIAMロールを作成し、ロールにSTSを添付する必要があります。ユーザーはその役割をEC2に添付する必要がありますか？インスタンスを作成し、そのサーバーでAWS認証を設定します。	The user should create IAM groups as per the organization’s departments and add each user to the group for better access control. <br>  ユーザーは組織の部門ごとにIAMグループを作成し、各ユーザーをグループに追加してアクセス制御を強化する必要があります。	The user should create a separate IAM user for each employee and provide access to them as per the policy. <br>  ユーザーは、各従業員に対して別々のIAMユーザーを作成し、ポリシーごとにそのユーザーにアクセスできるようにする必要があります。
Test3-55. <p>A user is using CloudFormation to launch an EC2 instance and then planning to configure an application after the instance is launched. The user wants the stack creation of ELB and AutoScaling to wait until the EC2 instance is launched and configured properly. How can the user configure this?</p> | <p>ユーザーはCloudFormationを使用してEC2インスタンスを起動し、インスタンスの起動後にアプリケーションを設定することを計画しています。ユーザーは、ELBと自動スケーリングのスタック作成が、EC2インスタンスが起動され、正しく構成されるまで待機する必要があります。ユーザーはどのようにこれを設定できますか？</p>	sa:	The user can use the WaitCondition resource to hold the creation of the other dependent resources. <br>  ユーザーはWaitConditionリソースを使用して、他の従属リソースの作成を保持できます。|<p><br></p><p>You can use a wait condition for situations like the following:</p> <ul> <li>To coordinate stack resource creation with configuration actions that are external to the stack creation</li> <li>To track the status of a configuration process</li> </ul> <p>&nbsp;</p><p>For more information on Cloudformation Wait condition please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-waitcondition.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-waitcondition.html</a><br><p></p>	The user can use the HoldCondition resource to wait for the creation of the other dependentresources. <br>  ユーザーはHoldConditionリソースを使用して、他の従属リソースの作成を待機することができます。	The user can use the DependentCondition resource to hold the creation of the other dependent resources. <br>  ユーザーはDependentConditionリソースを使用して、他の従属リソースの作成を保持できます。	It is not possible that the stack creation will wait until one service is created and launched. <br>  スタックの作成は、1つのサービスが作成されて起動されるまで待つことはできません。
Test3-56. <p>A marketing research company has developed a tracking system that collects user behavior during web marketing campaigns on behalf of the customers all over the world. The tracking system consists of an auto-scaled group of EC2 instances behind an ELB. And the collected data is stored in DynamoDB. After the campaign is terminated the tracking system is torn down and the data is moved to Amazon Redshift, where it is aggregated and used to generate detailed reports.</p><p>The company wants to be able to instantiate new tracking systems in any region without any manual intervention and therefore adopted CloudFormation.</p><p>What needs to be done to make sure that the AWS Cloudformation template works for every AWS region?</p><p>Choose 2 options from the below:<br></p> | <p>マーケティングリサーチ会社は、世界中の顧客に代わってウェブマーケティングキャンペーン中のユーザー行動を収集するトラッキングシステムを開発しました。追跡システムは、ELBの背後にあるEC2インスタンスの自動スケーリングされたグループで構成されています。収集したデータはDynamoDBに格納されます。キャンペーンが終了すると、追跡システムは切断され、データはAmazon Redshiftに移動され、集計され、詳細なレポートを生成するために使用されます。</p> <p>新しい追跡システムのインスタンスを作成できるようにする</p> <p> AWS CloudformテンプレートがすべてのAWS地域で機能することを確認するには、以下の2つのオプションを選択してください。 ：<br> </p>	ma:	x:Avoid using Deletion Policies for the EBS snapshots. <br>  EBSスナップショットの削除ポリシーを使用しないでください。|<p><br></p><p>Option A is incorrect because you need to retain or keep the snapshots of the EBS volumes in order to launch similar instances in the new region.</p><p>Option B is incorrect because DynamoDB table with the same name can be created in different regions. They have to be unique in a single region.</p><p>Option C is CORRECT because you need to get the name of the Availability Zone based on the region in which the template would be used.</p><p>Option D is incorrect because you do not need to define IAM users per region as they are global.</p><p>Option E is CORRECT because the AMI ID would be needed to launch the similar instances in the new region where the template would be used.&nbsp;</p><p><br></p><p><b>More information on CloudFormation intrinsic functions:</b></p><p>You can use the Fn::GetAZs function of CloudFormation to get the AZ of the region and assign it to the ELB.</p><p>An example of the Fn::GetAZs function is given below</p><p>{ "Fn::GetAZs" : "" }</p><p>{ "Fn::GetAZs" : { "Ref" : "AWS::Region" } }</p><p>{ "Fn::GetAZs" : "us-east-1" }</p><p>An example of the FindInMap is shown below. This is useful when you want to get particular values region wise which can be used as parameters. Since the Launch configuration contains the AMI ID information and since the AMI ID is different in different regions, you need to recreate the Launch Configurations based on the AMI ID.</p><p>&nbsp;</p><p>{</p><p>&nbsp; ...</p><p>&nbsp; "Mappings" : {</p><p>&nbsp; &nbsp; &nbsp; "RegionMap" : {</p><p>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; "us-east-1" : { "32" : "ami-6411e20d", "64" : "ami-7a11e213" },</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "us-west-1" : { "32" : "ami-c9c7978c", "64" : "ami-cfc7978a" },</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "eu-west-1" : { "32" : "ami-37c2f643", "64" : "ami-31c2f645" },</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ap-southeast-1" : { "32" : "ami-66f28c34", "64" : "ami-60f28c32" },</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ap-northeast-1" : { "32" : "ami-9c03a89d", "64" : "ami-a003a8a1" }</p><p>&nbsp;&nbsp;&nbsp; }</p><p>&nbsp; },</p><p>&nbsp;</p><p>&nbsp; "Resources" : {</p><p>&nbsp;&nbsp;&nbsp;&nbsp; "myEC2Instance" : {</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Type" : "AWS::EC2::Instance",</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "Properties" : {</p><p>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "ImageId" : { "Fn::FindInMap" : [ "RegionMap", { "Ref" : "AWS::Region" }, "32"]},</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; "InstanceType" : "m1.small"</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</p><p>&nbsp; &nbsp;&nbsp; &nbsp; }</p><p>&nbsp;&nbsp; }</p><p>}</p><p>For more information on the Fn::FindInMap function, please refer to below link</p><p><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html" target="_blank">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a><br></p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html</a><br><p></p>	x:The names of the DynamoDB tables must be different in every target region. <br>  DynamoDBテーブルの名前は、すべてのターゲット領域で異なる必要があります。	o:Use the built-in function of Cloudformation to set the AZ attribute of the ELB resource. <br>  Cloudformの組み込み関数を使用して、ELBリソースのAZ属性を設定します。	x:IAM users with the right to start Cloudformation stacks must be defined for every target region. <br>  Cloudformスタックを開始する権限を持つIAMユーザーは、すべてのターゲットリージョンに対して定義する必要があります。	o:Use the built-in Mappings and FindInMap functions of AWS Cloudformation to refer to the AMI ID set in the ImageID attribute of the Autoscaling::LaunchConfiguration resource. <br>  Autoscaling :: LaunchConfigurationリソースのImageID属性に設定されているAMI IDを参照するには、AWS Cloudformationの組み込みのMappingsおよびFindInMap関数を使用します。
Test3-57. <p>A user has created a VPC with the public and private subnets using the VPC wizard. The VPC has CIDR 20.0.0.0/16. The public subnet uses CIDR 20.0.1.0/24. The user is planning to host a web server in the public subnet with port 80 and a Database server in the private subnet with port 3306. The user is configuring a security group for the public subnet (WebSecGrp) and the private subnet (DBSecGrp). Which of the below-mentioned entries is required in the private subnet database security group DBSecGrp?</p> | <p>ユーザーは、VPCウィザードを使用してパブリックサブネットとプライベートサブネットを持つVPCを作成しました。VPCのCIDRは20.0.0.0/16です。パブリックサブネットはCIDR 20.0.1.0/24を使用します。ユーザーは、パブリックサブネット（WebSecGrp）とプライベートサブネット（DBSecGrp）のセキュリティグループを構成しています。ポート80を使用してパブリックサブネットにWebサーバーを、プライベートサブネットにデータベースサーバーをホストする予定です。プライベートサブネットデータベースセキュリティグループDBSecGrpには、以下のエントリのどれが必要ですか？</p>	sa:	Allow Inbound on port 3306 for the source Web Server Security Group WebSecGrp. <br>  ソースWebサーバーセキュリティグループWebSecGrpのポート3306の受信を許可する。|<p><br></p><p>The important point in this question is to allow the incoming traffic to the private subnet on port 3306 only for the instances in the private subnet.</p><p><br></p><p>Option A is CORRECT because (a) it allows the inbound traffic only for the required port 3306, and (b) it allows only the traffic from the instances in the public subnet (WebSecGrp).</p><p>Option B is incorrect because it is allowing the inbound traffic to all the instances in the VPC which is not the requirement.</p><p>Option C is incorrect because defining outbound traffic will not ensure the incoming traffic from the public subnet. Also, since the security groups are stateful, you just need to define the inbound traffic for the public subnet only (WebSecGrp). The outbound traffic would be automatically allowed.</p><p>Option D is incorrect because you do not need to open the port 80 in this case.</p><p><br></p><p><b>More information on Web Server and DB Server Security Group settings:</b></p><p>Since the Web server needs to talk to the database server on port 3306 that means that the database server should allow incoming traffic on port 3306. The below table from the AWS documentation shows how the security groups should be set up.</p><p><img src="data:image/png;base64,iVBO<p>For more information on security groups please visit the below link</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html</a><br><p></p>	Allow Inbound on port 3306 from source 20.0.0.0/16. <br>  送信元20.0.0.0/16からのポート3306の受信を許可します。	Allow Outbound on port 3306 for destination Web Server Security Group WebSecGrp. <br>  宛先WebサーバーセキュリティグループWebSecGrpのポート3306の送信を許可します。	Allow Outbound on port 80 for destination NAT instance IP. <br>  宛先NATインスタンスIPのポート80での送信を許可します。
Test3-58. <p>Your customer is implementing a video-on-demand streaming platform on AWS. The requirement is to be able to support multiple devices such as iOS, Android, and Windows as client devices, using a standard client player, using streaming technology and scalable architecture with cost-effectiveness.</p><p>Which architecture meets the requirements?</p> | <p>お客様の顧客は、AWS上でビデオオンデマンドストリーミングプラットフォームを実装しています。要件は、iOS、Android、Windowsなどの複数のデバイスをクライアントデバイスとして、標準のクライアントプレーヤーを使用して、ストリーミングテクノロジとスケーラブルなアーキテクチャを使用して費用対効果の高いものをサポートできるようにすることです。</p> <p>要件？</p>	sa:	Store the video contents to Amazon S3 as an origin server. Configure the Amazon CloudFront distribution with a download option to stream the video contents <br>  オリジナルのサーバーとしてAmazon S3にビデオコンテンツを保存します。ビデオコンテンツをストリーミングするダウンロードオプションを使用してAmazon CloudFrontディストリビューションを設定する|<p><br></p><p></p><div><div>Option A is incorrect because it uses CloudFront distribution with streaming option which does not work on all platforms; where as, it should use download option.</div><div>Option B is CORRECT because (a) it uses CloudFront distribution with download option for streaming the on demand videos using HLS on any mobile, and (b)&nbsp; it uses S3 as origin, so keeps the cost low.</div><div>Option C is incorrect because (a) provisioning&nbsp; streaming EC2 instances is a costly solution, (b) the videos are to be delivered on-demand, not live streaming.</div><div>Option D is incorrect because the videos are to be delivered on-demand, not live streaming. So, streaming server is not required.</div></div><br><p></p><p>For more information on live and on-demand streaming using CloudFront, please visit the below URL:</p><p></p><a href="https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/blogs/aws/using-amazon-cloudfront-for-video-streaming/</a><br><p></p>	Store the video contents to Amazon Simple Storage Service (S3) as an origin server. Configure the Amazon CloudFront distribution with a streaming option to stream the video contents. <br>  オリジナルのサーバーとしてAmazon Simple Storage Service（S3）にビデオコンテンツを保存します。ストリーミングオプションを使用してAmazon CloudFrontディストリビューションを設定し、ビデオコンテンツをストリーミングします。	Launch a streaming server on Amazon Elastic Compute Cloud (EC2) (for example, Adobe Media Server), and store the video contents as an origin server. Configure the Amazon CloudFront distribution with a download option to steam the video contents. <br>  Amazon EC2（Elastic Compute Cloud）（たとえば、Adobe Media Server）でストリーミングサーバーを起動し、ビデオコンテンツを元のサーバーとして格納します。ダウンロードオプションを使用してAmazon CloudFrontディストリビューションを設定し、ビデオコンテンツをスチームします。	Launch a steaming server on Amazon EC2 (for example, Adobe Media Server), and store the video contents as an origin server. Launch and configure the required amount of streaming servers on Amazon EC2 as an edge server to stream the video contents. <br>  Amazon EC2（例えば、Adobe Media Server）上でスティミングサーバーを起動し、ビデオコンテンツをオリジンサーバーとして保存します。ビデオコンテンツをストリーミングするためのエッジサーバーとして、Amazon EC2上のストリーミングサーバーの必要な量を起動して構成します。
Test3-59. <p>A document storage company is deploying their application to AWS and changing their business model to support both Free Tier and Premium Tier users. The premium Tier users will be allowed to store up to 200GB of data and Free Tier customers will be allowed to store only 5GB. The customer expects that billions of files will be stored. All users need to be alerted when approaching 75 percent quota utilization and again at 90 percent quota use.</p><p>To support the Free Tier and Premium Tier users, how should they architect their application?</p> | <p>ドキュメントストレージ会社は、AWSにアプリケーションを導入し、ビジネスモデルを変更して、フリー層とプレミアム層の両方のユーザーをサポートしています。プレミアムティアユーザーは最大200GBのデータを保存でき、フリーティアユーザーは5GBのみを保存できます。顧客は、何十億ものファイルが保存されることを期待しています。</p> <p>フリーティアとプレミアム層ユーザーをサポートするには、アプリケーションをどのように構築するのですか？</p>	sa:	The company should utilize an Amazon Simple Workflow Service activity worker that updates the user’s used data counter in Amazon DynamoDB. The Activity Worker will use Simple Email Service to send an email if the counter increases above the appropriate thresholds. <br>  Amazon DynamoDBでユーザーの使用済みデータカウンタを更新するAmazon Simple Workflow Serviceアクティビティワーカーを利用する必要があります。アクティビティワーカーは、カウンターが適切なしきい値を超えて増加した場合、簡易電子メールサービスを使用して電子メールを送信します。|<p><br></p><p>Option A is CORRECT because DynamoDB which is highly scalable service is best suitable in this scenario.&nbsp;</p><p>Option B is incorrect because RDS would not be a suitable solution for storing billions of files.</p><p>Option C and D are both incorrect because it uses object level storage and iterating over billions of objects for each operation is performance-wise not a good option at all.</p><p><br></p><ul></ul><p></p>	The company should deploy an Amazon Relational Database Service (RDS) relational database with a stored objects table that has a row for each stored object along with the size of each object. The upload server will query the aggregate consumption of the user in question (by first determining the files stored by the user, and then querying the stored objects table for respective file sizes) and send an email via Amazon Simple Email Service if the thresholds are breached. <br>  会社は、各オブジェクトのサイズとともに格納された各オブジェクトの行を持つストアドオブジェクトテーブルを持つAmazon Relational Database Service（RDS）リレーショナルデータベースを展開する必要があります。アップロードサーバーは、問題のユーザーの総消費量を照会します（最初にユーザーが格納したファイルを特定し、ファイルサイズごとに格納されたオブジェクトテーブルを照会する）、しきい値が破られた場合はAmazon Simple Email Service経由で電子メールを送信します。	The company should write both the content length and the username of the files owner as S3 metadata for the object. They should then create a file watcher to iterate over each object and aggregate the size for each user and send a notification via Amazon Simple Queue Service to an emailing service if the storage threshold is exceeded. <br>  会社は、コンテンツの長さとファイル所有者のユーザー名の両方をオブジェクトのS3メタデータとして書き込む必要があります。次に、ファイルウォッチャーを作成して各オブジェクトを繰り返し処理し、各ユーザーのサイズを集計し、ストレージのしきい値を超えた場合はAmazon Simple Queue Service経由で電子メールサービスに通知を送信する必要があります。	The company should create two separate Amazon Simple Storage Service buckets, one for date storage for Free Tier Users, and another for data storage for Premium Tier users. An Amazon Simple Workflow Service activity worker will query all objects for a given user based on the bucket the data is stored in and aggregate storage. The activity worker will notify the user via Amazon Simple Notification Service when necessary. <br>  同社は、Free Tier Users用の日付ストレージ用と、Premium Tierユーザー用のデータストレージ用の2​​つの別々のAmazon Simple Storage Serviceバケットを作成する必要があります。Amazon Simple Workflow Serviceアクティビティワーカーは、データが格納されたバケットとストレージを集約して、指定されたユーザーのすべてのオブジェクトを照会します。アクティビティ担当者は、必要に応じてAmazon Simple Notification Service経由でユーザーに通知します。
Test3-60. <p>You are designing security inside your VPC. You are considering the options for establishing separate security zones, and enforcing network traffic rules across the different zones to limit which instances can communicate. How would you accomplish these requirements?</p><p>Choose 2 options from the below:<br></p> | <p> VPC内にセキュリティを設計しています。別のセキュリティゾーンを設定し、異なるゾーン間でネットワークトラフィックルールを適用して通信できるインスタンスを制限するオプションを検討しています。これらの要件をどのように達成しますか？</p> <p>以下の2つのオプションを選択してください：<br> </p>	ma:	x:Configure a security group for every zone. Configure a default allow all rule. Configure explicit deny rules for the zones that shouldn’t be able to communicate with one another. <br>  ゾーンごとにセキュリティグループを構成します。デフォルトのすべて許可ルールを設定します。相互に通信できないゾーンの明示的な拒否ルールを構成します。|<p><br></p><p>Option A is incorrect because you cannot set up explicit deny rules in the Security Groups.</p><p>Option B is CORRECT because you can explicitly allow or deny traffic based on certain IP address range.</p><p>Option C is incorrect because you cannot delete the main route table, but you can replace the main route table with a custom table that you've created (so that this table is the default table each new subnet is associated with).</p><p>Option D is CORRECT because Security Group in this case would act like a Firewall that provides security and control at the port/protocol level, and have "implicit deny all" rule and only allow what is needed.</p><p><br></p><p>For more information on VPC and subnets, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><br><b>Note:</b><br><p>If we configure the default routing in a VPC, in such a way that each subnet only has routes to other subnets with which it needs to communicate and doesn't have routes to subnets with which it shouldn't be able to communicate.<br>&nbsp;By doing this we are restricting the communication of all the instances present in a subnet to communicate with yet another subnet instances; which is not the actual requirement.<br>Here there is no way to fine control the access to a particular instance where as if we configure the controls at the Security Group level and NACL level it gives us a fine control over the traffic to various instances.</p><p>Option D is correct because it will implicitly deny all the traffic and you can then open the port/protocol to only the traffic which is coming from the valid sources..<br></p><br><p></p>	o:NACLs to explicitly allow or deny communication between the different IP address ranges, as required for inter zone communication. <br>  ゾーン間通信に必要な異なるIPアドレス範囲間の通信を明示的に許可または拒否するNACL。	x:Configure multiple subnets in your VPC, one for each zone. Configure routing within your VPC in such a way that each subnet only has routes to other subnets with which it needs to communicate, and doesn’t have routes to subnets with which it shouldn’t be able to communicate. <br>  各ゾーンに1つずつ、VPC内の複数のサブネットを設定します。各サブネットが通信する必要がある他のサブネットへのルートしか持たず、通信できないサブネットへのルートを持たないように、VPC内のルーティングを設定します。	o:Configure a security group for every zone. Configure allow rules only between zones that need to be able to communicate with one another. Use the implicit deny all rule to block any other traffic. <br>  すべてのゾーンのセキュリティグループを構成します。相互に通信できる必要があるゾーン間でのみ許可ルールを設定します。暗黙の拒否のすべてのルールを使用して、他のトラフィックをブロックします。
Test3-61. <p>You’ve been tasked with moving an e-commerce web application from a customer’s data center into a VPC. The application must be fault tolerant and well as highly scalable. Moreover, the customer is adamant that service interruptions not affect the user experience. As you near launch, you discover that the application currently uses multicast to share session state between web servers. In order to handle session state within the VPC, you choose to which of the following option:</p> | <p>顧客のデータセンターから電子商取引WebアプリケーションをVPCに移行する必要があります。アプリケーションはフォールトトレラントで、スケーラビリティの高いものでなければなりません。さらに、サービスの中断がユーザーエクスペリエンスに影響を与えないことを顧客は忠告しています。起動すると、アプリケーションは現在、マルチキャストを使用してWebサーバー間でセッション状態を共有していることがわかります。VPC内のセッション状態を処理するには、次のいずれかのオプションを選択します。</p>	sa:	Store session state in Amazon ElastiCache for Redis. <br>  RedisのAmazon ElastiCacheにセッション状態を保存します。|<p><br></p><p>Option A is incorrect because ELB does not help in storing the state; it only routes the traffic by session cookie. If the EC2 instance fails, the session will be lost.</p><p>Option B is CORRECT because Redis is a fast, open source, in-memory data store and caching service. It is highly available, reliable, and with high performance suitable for the most demanding applications such as this one.</p><p>Option C is incorrect because Mesh VPN is just not fault tolerant or highly scalable - the client's real priorities. It's failure would impact users. The supernode that handles the registration is a single point of failure and in case of failure, new VPN nodes would not be able to register. Also, the nodes would't register across multiple AZs. Even if it is possible it is very cumbersome.</p><p>Option D is incorrect because RDS is not highly scalable.</p><p><br></p><p>For more information on Elastic Cache, please visit the below URL:</p><p><a href="https://aws.amazon.com/elasticache/">https://aws.amazon.com/elasticache/</a></p><p><br></p><p><b>Note:</b></p><p>Our main requirement is to provide fault tolerance and high scalability.&nbsp;<br><br>Redis data resides in-memory, in contrast to databases that store data on disk or SSDs. By eliminating the need to access disks, in-memory data stores such as Redis avoid seek time delays and can access data in microseconds. Redis is a popular choice for caching, session management, real-time analytics, geospatial, chat/messaging, media streaming, and gaming leaderboards.<br><br>ElastiCache Redis can provide high scalability and is fault tolerant.<br></p><p><br></p>	Enable session stickiness via Elastic Load Balancing. <br>  Elastic Load Balancingを使用してセッションのスティッキ性を有効にします。	Create a mesh VPN between instances and allow multicast on it. <br>  インスタンス間にメッシュVPNを作成し、マルチキャストを許可します。	Store session state in Amazon Relational Database Service. <br>  Amazon Relational Database Serviceにセッション状態を保存します。
Test3-62. <p>Your company is migrating infrastructure to AWS. A large number of developers and administrators will need to control this infrastructure using the AWS Management Console. The Identity Management team is objecting to creating an entirely new directory of IAM users for all employees, and the employees are reluctant to commit yet another password to memory.</p><p>Which of the following will satisfy both these stakeholders?</p> | <p>あなたの会社はインフラストラクチャをAWSに移行しています。 多数の開発者と管理者がAWS管理コンソールを使用してこのインフラストラクチャを制御する必要があります。 アイデンティティ管理チームは、すべての従業員のためにIAMユーザーの全く新しいディレクトリを作成することに反対しています。従業員は、メモリへのもう1つのパスワードをコミットすることに躊躇しています。<br>これらのステークホルダーの両方を満足させるものはどれですか？</p>	sa:	Users request a SAML assertion from your on-premises SAML 2.0-compliant identity provider (IdP) and use that assertion to obtain federated access to the AWS Management Console via the AWS single sign-on (SSO) endpoint. <br> ユーザーは、オンプレミスのSAML 2.0準拠アイデンティティプロバイダ（IdP）からSAMLアサーションを要求し、そのアサーションを使用して、AWSシングルサインオン（SSO）エンドポイント経由でAWS管理コンソールへのフェデレーションアクセスを取得します。|<p><br></p><p>Option A is incorrect because, although it is a workable solution, the users need not use the OpenID IdP (such as Facebook, Google, SalesForce etc.)in this scenario as they can use the on-premises 2.0 SAML compliant IdP and get the federated access to the AWS. Access via OpenID IdP is most suitable for the mobile apps.</p><p>Option B is incorrect because you cannot login to AWS using the IdP provided credentials. You need temporary credentials provided by Security Token Service (STS) for that.</p><p>Option C is incorrect because you should avoid using Access Key and Secret Key for the login. This is the least secure way to login.</p><p>Option D is CORRECT because it uses the on-premises 2.0 SAML compliant IdP and get the federated access to the AWS, thus avoiding creating any IAM User for the employees in the organization.</p><p><br></p><p>For more information on SAML Authentication in AWS, please visit the below URL:</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a><br><p></p>	Users log in directly to the AWS Management Console using the credentials from your on-premises Kerberos compliant Identity provider. <br>  ユーザーは、オンプレミスKerberos準拠のアイデンティティプロバイダの資格情報を使用して、AWS管理コンソールに直接ログインします。	Users log in to the AWS Management Console using the AWS Command Line Interface. <br>  ユーザーは、AWSコマンドラインインターフェイスを使用してAWS管理コンソールにログインします。	Users sign in using an OpenID Connect (OIDC) compatible IdP, receive an authentication token, then use that token to log in to the AWS Management Console. <br>  ユーザーは、OpenID Connect（OIDC）互換のIdPを使用してサインインし、認証トークンを受け取った後、そのトークンを使用してAWS Management Consoleにログインします。
Test3-63. <p>A gaming company adopted AWS Cloud Formation to automate load-testing of their games. They have created an AWS Cloud Formation template for each gaming environment including one for the load-testing stack. The load-testing stack creates an Amazon Relational Database Service (RDS) Postgres database and two web servers running on Amazon Elastic Compute Cloud (EC2) that send HTTP requests, measure response times, and write the results into the database. A test run usually takes between 15 and 30 minutes. Once the tests are done, the AWS Cloud Formation stacks are torn down immediately. The test results written to the Amazon RDS database must remain accessible for visualization and analysis. Select possible solutions that allow access to the test results after the AWS Cloud Formation load -testing stack is deleted.</p><p>Choose 2 options from the below:.</p> | <p>ゲーム会社がAWSクラウドフォーメーションを採用してゲームの負荷テストを自動化しました。彼らは、ロードテストスタック用のテンプレートを含む各ゲーム環境用のAWS Cloud Formationテンプレートを作成しました。負荷テストスタックは、Amazon Relational Database Service（RDS）Postgresデータベースと、Amazon Elastic Compute Cloud（EC2）上で実行される2つのWebサーバーを作成し、HTTP要求を送信し、応答時間を測定し、結果をデータベースに書き込みます。試運転には通常15〜30分かかります。テストが完了すると、AWS Cloud Formationスタックは直ちに切断されます。Amazon RDSデータベースに書き込まれたテスト結果は、可視化と分析のためにアクセス可能でなければなりません。AWS Cloud Formationの負荷テストスタックが削除された後にテスト結果にアクセスできるようにするソリューションを選択します。</p> <	ma:	x:Define an Amazon RDS Read-Replica in the load-testing AWS CloudFormation stack and define a dependency relation between master and replica via the DependsOn attribute. <br>  ロードテストAWS CloudFormationスタック内のAmazon RDS Read-Replicaを定義し、DependsOn属性を使用してマスターとレプリカ間の依存関係を定義します。|<p><br></p><p>Option A is incorrect because (a) creation of read replicas is not needed in this scenario, and (b) they would anyways be deleted after the stacks get deleted, so there is no need to define any dependency in the template.</p><p>Option B is incorrect because UpdatePolicy attribute is only applicable to certain resources like AutoScalingGroup, AWS Lambda Alias. It is not applicable to RDS.</p><p>Option C is CORRECT because, with Retain deletion policy, the RDS resources would be preserved for the visualization and analysis after the stack gets deleted.</p><p>Option D is CORRECT because, with the Snapshot deletion policy, a snapshot of the RDS instance would get created for visualization and analysis later after the stack gets deleted.</p><p>Option E is incorrect because automated snapshots are not needed in this case. All that is needed is a single snapshot of the RDS instance after the test gets finished - which can be taken via Snapshot deletion policy.</p><p><br></p><p>NOTE: This question is asking for two possible answers. It does not say that both need to be used at the same time. Hence both C and D are valid options.</p><p><br></p><p><span style="font-size: 1rem;">For more information on deletion policy, please visit the below URL:</span></p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a><br><p></p>	x:Define an update policy to prevent deletion of the Amazon RDS database after the AWS CloudFormation stack is deleted. <br>  AWS CloudFormationスタックが削除された後にAmazon RDSデータベースが削除されないようにするための更新ポリシーを定義します。	o:Define a deletion policy of type Retain for the Amazon RDS resource to assure that the RDS database is not deleted with the AWS CloudFormation stack. <br>  RDSデータベースがAWS CloudFormationスタックで削除されないように、Amazon RDSリソースのRetainタイプの削除ポリシーを定義します。	o:Define a deletion policy of type Snapshot for the Amazon RDS resource to assure that the RDS database can be restored after the AWS CloudFormation stack is deleted. <br>  Amazon RDSリソースのスナップショットタイプの削除ポリシーを定義し、AWS CloudFormationスタックの削除後にRDSデータベースを復元できるようにします。	x:Define automated backups with a backup retention period of 30 days for the Amazon RDS database and perform point-in-time recovery of the database after the AWS CloudFormation stack is deleted. <br>  Amazon RDSデータベースのバックアップ保存期間を30日間に設定して自動バックアップを定義し、AWS CloudFormationスタックの削除後にデータベースのポイントインタイムリカバリを実行します。
Test3-64. <p>A large enterprise wants to adopt Cloud Formation to automate administrative tasks and implement the security principles of least privilege and separation of duties. They have identified the following roles with the corresponding tasks in the company:</p><ul><li>Network administrators: create, modify and delete VPCs, subnets, NACLs, routing tables and security groups.</li><li>Application operators: deploy complete application stacks (ELB, Auto-Scaling groups, RDS) whereas all resources must be deployed in the VPCs managed by the network administrators.</li></ul><p>Both groups must maintain their own Cloud Formation templates and should be able to create, update and delete only their own Cloud Formation stacks.</p><p>The company has followed your advice to create two IAM groups, one for applications and one for networks. Both IAM groups are attached to IAM policies that grant rights to perform the necessary task of each group as well as the creation, update, and deletion of Cloud Formation stacks.&nbsp;&nbsp;</p><p>Given setup and requirements, which statements represent valid design considerations? <br></p><p>Choose 2 options from the below:<br></p> | <p>大企業は、管理タスクを自動化し、最低限の特権と職務分離のセキュリティ原則を実装するためにクラウドフォーメーションを採用したいと考えています。</p> <ul> <li>ネットワーク管理者：VPC、サブネット、NACL、ルーティングテーブル、およびセキュリティグループの作成、変更、削除を行います。</ li> <li> >アプリケーションオペレータ：完全なアプリケーションスタック（ELB、自動スケーリンググループ、RDS）を配備するのに対して、ネットワーク管理者が管理するVPCにはすべてのリソースを配備する必要があります。</ li> <p>クラウドフォーメーションのテンプレートを作成し、独自のクラウドフォーメーションスタックのみを作成、更新、削除することができます。</p> <p> 同社は、アプリケーション用とネットワーク用の2つのIAMグループを作成するためのアドバイスを続けてきました。どちらのIAMグループも、各グループの必要なタスクの実行権限と、Cloud Formationスタックの作成、更新、および削除を実行する権限を与えるIAMポリシーに添付されています。＆nbsp;＆nbsp; <p> <p>どのステートメントが有効な設計上の考慮事項を表していますか？<br> </p> <p>下記の2つのオプションを選択してください：<br> </p>	ma:	o:Network stack updates will fail upon attempts to delete a subnet with EC2 instances. <br>  EC2インスタンスを持つサブネットを削除しようとすると、ネットワークスタックの更新が失敗します。|<p><br></p><p></p><div>Option A is CORRECT because subnets cannot be deleted with instances in them.</div><div>Option B is CORRECT because to explicitly launch instances, we need IAM permissions.</div><div><span style="font-size: 1rem;">Option C is incorrect because even though stacks can be nested, Network group needs all the application group permissions.</span></div><div>Option D is incorrect because application stack can be deleted before network stack.</div><div>Option E is incorrect because network administrators have no rights to delete application stack.</div><br><p></p><p>For more information, please visit the below URL:</p><p></p><a href="https://aws.amazon.com/blogs/devops/aws-cloudformation-security-best-practices/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/blogs/devops/aws-cloudformation-security-best-practices/</a><br><p></p>	o:Restricting the launch of EC2 instances into VPCs requires resource level permissions in the IAM policy of the application group. <br>  EC2インスタンスのVPCへの起動を制限するには、アプリケーショングループのIAMポリシーにリソースレベルの権限が必要です。	x:Nesting network stacks within application stacks simplifies management and debugging, but requires resource level permissions in the IAM policy of the network group. <br>  アプリケーションスタック内にネットワークスタックをネストすると、管理とデバッグが簡単になりますが、ネットワークグループのIAMポリシーにはリソースレベルのアクセス許可が必要です。	x:The application stack cannot be deleted before all network stacks are deleted. <br>  すべてのネットワークスタックが削除される前にアプリケーションスタックを削除することはできません。	x:Unless resource level permissions are used on the cloud formation: Delete Stack action, network administrators could tear down application stacks. <br>  クラウドの構成でリソースレベルの権限が使用されていない限り、スタックの削除アクション。ネットワーク管理者はアプリケーションスタックを破棄できます。
Test3-65. <p>An Enterprise customer is starting their migration to the cloud, their main reason for migrating is agility, and they want to make their internal Microsoft Active Directory available to any applications running on AWS; this is so internal users only have to remember one set of credentials and as a central point of user control for leavers and joiners. How could they make their Active Directory secure, and highly available, with minimal on-premises infrastructure changes, in the most cost and time-efficient way? <br></p><p>Choose the most appropriate option from the below:</p> | <p>エンタープライズの顧客がクラウドへの移行を開始しているため、移行の主な理由は敏捷性であり、AWS上で動作するアプリケーションで社内のMicrosoft Active Directoryを利用できるようにすることです。これは、内部ユーザーが1組の資格情報を記憶し、退去者と参加者のためのユーザーコントロールの中心点として覚えておく必要があるためです。オンプレミスのインフラストラクチャの変更を最小限に抑えながら、コストと時間効率の高い方法で、Active Directoryをどのようにして安全かつ高可用性にすることができましたか？<br> </p> <p>下記から最適なオプションを選択してください：</p>	sa:	Using VPC, they could create an extension to their data center and make use of resilient hardware IPSec tunnels; they could then have two domain controller instances that are joined to their existing domain and reside within different subnets, in different Availability Zones. <br>  VPCを使用すると、データセンターの拡張機能を作成し、復元力のあるハードウェアIPSecトンネルを利用できます。既存のドメインに参加し、異なるサブネット内の異なるアベイラビリティゾーンに存在する2つのドメインコントローラインスタンスを持つことができます。|<p><br></p><p>Option A incorrect because it is just a complicated environment to setup and does not meet the purpose of the requirement.</p><p>Option B is CORRECT because using an IPSec tunnel can help decrypt all the traffic from the on-premise to AWS. The domain controllers in separate AZ’s can address high availability.</p><p>Option C is incorrect because the question mentions that they want minimal changes to the on-premise environment.</p><p>Option D is incorrect because it does not address the secure communication part from on-premise to AWS.</p><p><br></p><p>For more information on creating VPN tunnels using Hardware VPN and Virtual private gateways, please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a><br><p></p><p><br></p>	Using Amazon Elastic Compute Cloud (EC2), they could create a DMZ using a security group; within the security group they could provision two smaller Amazon EC2 instances that are running Openswan for resilient IPSec tunnels, and two larger instances that are domain controllers; they would use multiple Availability Zones. <br>  Amazon Elastic Compute Cloud（EC2）を使用すると、セキュリティグループを使用してDMZを作成できます。セキュリティグループ内では、回復力のあるIPSecトンネル用にOpenswanを実行している2つの小さいAmazon EC2インスタンスと、ドメインコントローラである2つの大きなインスタンスをプロビジョニングできます。複数の可用性ゾーンを使用します。	Within the customer’s existing infrastructure, they could provision new hardware to run Active Directory Federation Services; this would present Active Directory as a SAML2 endpoint on the internet; any new application on AWS could be written to authenticate using SAML2 <br>  顧客の既存のインフラストラクチャ内で、Active Directoryフェデレーションサービスを実行するための新しいハードウェアをプロビジョニングできます。これはActive Directoryをインターネット上のSAML2エンドポイントとして提示します。AWSの新しいアプリケーションをSAML2を使用して認証するように書くことができました	The customer could create a stand-alone VPC with its own Active Directory Domain Controllers; two domain controller instances could be configured, one in each Availability Zone; new applications would authenticate with those domain controllers. <br>  お客様は、独自のActive Directoryドメインコントローラを使用してスタンドアロンのVPCを作成することができます。各可用性ゾーンに1つずつ、2つのドメインコントローラインスタンスを構成できます。新しいアプリケーションはこれらのドメインコントローラで認証されます。
Test3-66. <p>An AWS customer is deploying a web application that is composed of a front end running on Amazon EC2 and confidential data that is stored on Amazon S3.</p><p>&nbsp;The customer's Security policy requires that the all-access operations to this sensitive data must be authenticated and authorized by a centralized access management system that is operated by a separate security team.</p><p>In addition, the web application team that owns and administers the EC2 web front-end instances is prohibited from having any ability to access the data that circumvents this centralized access management system.</p><p>Which of the following configurations will support these requirements:</p> | <p> AWSの顧客は、Amazon EC2上で動作するフロントエンドとAmazon S3に格納された機密データで構成されるWebアプリケーションを配備しています。</ p> <p>＆nbsp;お客様のセキュリティポリシーでは、 この機密データは、別個のセキュリティチームによって運営される集中アクセス管理システムによって認証され、認証されなければなりません。</ p> <p>さらに、EC2 Webフロントエンドインスタンスを所有し管理するWebアプリケーションチームは禁止されています この集中管理されたアクセス管理システムを迂回するデータにアクセスする能力はありません。</p> <p>これらの要件をサポートする以下の構成のどれを選択しますか：</p>	sa:	Configure the web application to authenticate end users against the centralized access management system. Have the web application provision trusted users STS tokens entitling the download of approved data directly from Amazon S3. <br>  集中管理されたアクセス管理システムに対してエンドユーザーを認証するようにWebアプリケーションを設定します。Webアプリケーションで、信頼できるユーザーにSTSトークンを提供し、Amazon S3から承認されたデータのダウンロードを許可するようにします。|<p><br></p><p>Option A is CORRECT because the access to the sensitive data on Amazon S3 is only given to the authenticated users.&nbsp;</p><p>Option B is incorrect because S3 doesn’t integrate directly with CloudHSM, also there is no centralized access management system control.</p><p>Option C is incorrect because this is an incorrect workflow of use of SAML and it does not mention if the centralized access management system is SAML complaint.</p><p>Option D is incorrect because with this configuration the web team would have access to the sensitive data on S3.</p><p><br></p><p>&nbsp;<span style="font-size: 1rem;">For more information on STS, please refer to the URL:</span></p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a><br><p></p>	Encrypt the data on Amazon S3 using a CloudHSM that is operated by the separate security team. Configure the web application to integrate with the CloudHSM for decrypting approved data access operations for trusted end users. <br>  別のセキュリティチームによって運営されているCloudHSMを使用して、Amazon S3でデータを暗号化します。信頼できるエンドユーザの承認済みデータアクセス操作を解読するためにCloudHSMと統合するようにWebアプリケーションを設定します。	Configure the web application to authenticate end users against the centralized access management system using SAML. Have the end users authenticate to IAM using their SAML token and download the approved data directly from Amazon S3. <br>  SAMLを使用して集中管理されたアクセス管理システムに対してエンド・ユーザーを認証するようにWebアプリケーションを構成します。エンドユーザはSAMLトークンを使用してIAMに認証され、Amazon S3から承認されたデータが直接ダウンロードされます。	Have the separate security team create an IAM Role that is entitled to access the data on Amazon S3. Have the web application team provision their instances with this Role while denying their IAM users access to the data on Amazon S3. <br>  別のセキュリティチームに、Amazon S3のデータにアクセスする権限を持つIAMロールを作成させてください。WebアプリケーションチームがAmazon S3のデータへのIAMユーザーのアクセスを拒否しながら、このロールでインスタンスをプロビジョニングするようにします。
Test3-67. <p>A customer is running an application in the US-West region and wants to set up disaster recovery failover to Singapore region. The customer is interested in achieving a low RPO for an RDS multi-AZ DB instance. Which approach is best suited to this need?</p> | <p>お客様は米国西部地域でアプリケーションを実行しており、シンガポール地域への障害復旧フェールオーバーを設定したいと考えています。顧客は、RDSマルチAZ DBインスタンスのRPOを低くすることに関心があります。どのアプローチがこのニーズに最も適していますか？</p>	sa:	Asynchronous replication <br>  非同期レプリケーション|<p>When you have cross-region replication for RDS, this is done Asynchronously. Having Synchronous replication would be too much of an overhead for a cross-region replication.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_67_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="285" width="482"></p><p><br></p><p>Please refer to a blog article for cross-region replication for MySQL</p><p></p><a href="https://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/</a><br><p></p>	Synchronous replication <br>  同期レプリケーション	Route53 health checks <br>  Route53ヘルスチェック	Copying of RDS incremental snapshots <br>  RDS増分スナップショットのコピー
Test3-68. <p>A public archives organization is about to move a pilot application they are running on AWS into production. You have been hired to analyze their application architecture and give cost-saving recommendations. The application displays scanned historical documents.</p><p>Each document is split into individual image tiles at multiple zoom levels to improve responsiveness and ease of use for the end users. At maximum zoom level the average document will be 8000 X 6000 pixels in size, split into multiple 40px X 40px image tiles. The tiles are batch processed by Amazon Elastic Compute Cloud (EC2) instances and put into an Amazon Simple Storage Service (S3) bucket. A browser-based JavaScript viewer fetches tiles from the Amazon (S3)&nbsp; bucket and displays them to users as they zoom and pan around each document. The average storage size of all zoom levels for a document is approximately 30MB of JPEG tiles. Originals of each document are archived in Amazon Glacier. The company expects to process and host over 500,000 scanned documents in the first year. What are your recommendations?</p><p>Choose 3 options from the below:<br></p> | <p>公開アーカイブ組織は、AWSで実行しているパイロットアプリケーションを本番環境に移行しようとしています。あなたは自分のアプリケーションアーキテクチャを分析し、コスト節約のための勧告をするために雇われています。アプリケーションはスキャンした履歴文書を表示します。</p> <p>各文書は複数のズームレベルで個々の画像タイルに分割され、エンドユーザの応答性と使いやすさを向上させます。最大ズームレベルでは、平均ドキュメントは8000×6000ピクセルのサイズで、複数の40ピクセル×40ピクセルのイメージタイルに分割されます。タイルはAmazon Elastic Compute Cloud（EC2）インスタンスによってバッチ処理され、Amazon Simple Storage Service（S3）バケットに入れられます。ブラウザベースのJavaScriptビューアは、Amazonからタイルをフェッチします（S3）。＆nbsp; それぞれのドキュメントの周囲をズームしてパンしながら、それらをユーザーに表示します。ドキュメントのすべてのズームレベルの平均ストレージサイズは約30MBのJPEGタイルです。Amazonの氷河には各文書の原本が保管されています。同社は初年度に50万件以上のスキャン文書を処理し、ホストする予定です。</p> <p>以下の3つのオプションを選択してください：<br> </p>	ma:	o:Deploy an Amazon CloudFront distribution in front of the Amazon S3 tiles bucket. <br>  Amazon S3タイルバケットの前にAmazon CloudFrontディストリビューションを配備します。|<p><br></p><p>Option A is CORRECT because the caching is done by CloudFront via the edge locations which reduces the load on the origin.</p><p>Option B is CORRECT because increasing the size of the images would help reduce the cost of number of GET/PUT requests on the origin server.</p><p>Option C is CORRECT because RRS is a low cost storage option and will help keeping the overall cost low.</p><p>Option D is incorrect because decreasing the size would require more requests and will increase the overall cost.</p><p>Option E is incorrect because Glacier is an archival solution and will not be suitable for frequent access of the tiles.</p><p><br></p><ul></ul><p></p>	o:Increase the size (width/height) of the individual tiles at the maximum zoom level. <br>  最大ズームレベルで個々のタイルのサイズ（幅/高さ）を増やします。	o:Use Amazon S3 Reduced Redundancy Storage for each zoom level. <br>  各ズームレベルでAmazon S3 Reduced Redundancy Storageを使用します。	x:Decrease the size (width/height) of the individual tiles at the maximum zoom level. <br>  最大ズームレベルで個々のタイルのサイズ（幅/高さ）を減らします。	x:Store the maximum zoom level in the low cost Amazon S3 Glacier option and only retrieve the most frequently access tiles as they are requested by users. <br>  低コストのAmazon S3 Glacierオプションに最大ズームレベルを格納し、ユーザーが要求したときに最も頻繁にアクセスするタイルのみを取得します。
Test3-69. <p><span style="font-size: 1rem;">A user has created a VPC with CIDR 20.0.0.0/16 using the wizard. The user has created a public subnet CIDR (20.0.0.0/24) and VPN only subnets CIDR (20.0.1.0/24) along with the VPN gateway (vgw-12345) to connect to the user’s data center. The user’s data center has CIDR 172.28.0.0/12. The user has also setup a NAT instance (i-12345) to allow traffic to the internet from the VPN subnet. Which of the below-mentioned options is not a valid entry for the main route table in this scenario?</span></p> | <p> <span style = "font-size：1rem;">ユーザーがウィザードを使用してCIDR 20.0.0.0/16のVPCを作成しました。ユーザーは、パブリックサブネットCIDR（20.0.0.0/24）およびVPN専用サブネットCIDR（20.0.1.0/24）をVPNゲートウェイ（vgw-12345）と共に作成して、ユーザーのデータセンターに接続しました。ユーザーのデータセンターのCIDRは172.28.0.0/12です。また、VPNサブネットからインターネットへのトラフィックを許可するNATインスタンス（i-12345）も設定しています。以下のオプションのどれがこのシナリオのメインルートテーブルの有効なエントリではありませんか？</ span> </p>	sa:	Destination: 20.0.1.0/24 and Target: i-12345 <br>  目的地：20.0.1.0/24および対象：i-12345|<p><br></p><p>Option A is CORRECT because the destination of private subnet with NAT instance as target is not needed in the route table. This is an invalid entry.</p><p>Option B is incorrect because you would need this entry to be able to communicate with the internet via NAT instance (e.g. for patch updates).</p><p>Option C is incorrect because you need this entry for communicating with customer network via the virtual private gateway.</p><p>Option D is incorrect because this entry is present by default to allow the resources in the VPC to communicate with each other.</p><p><br></p><p>The below diagram shows how a typical setup for a VPC with VPN and Internet gateway would look like. The only routing option which should have access to the internet gateway should be the 0.0.0.0/0 address. So Option A is the right answer.</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_69_1.png" alt="" role="presentation" class="img-responsive atto_image_button_text-bottom" height="532" width="702"></p><p><br></p><p>For more information on VPC with the option of VPN, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario3.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario3.html</a><br><br><p></p>	Destination: 0.0.0.0/0 and Target: i-12345 <br>  送り先：0.0.0.0/0、ターゲット：i-12345	Destination: 172.28.0.0/12 and Target: vgw-12345 <br>  送信先：172.28.0.0/12およびターゲット：vgw-12345	Destination: 20.0.0.0/16 and Target: local <br>  目的地：20.0.0.0/16および対象：ローカル
Test3-70. <p>A user has created a mobile application which makes calls to DynamoDB to fetch certain data. The application is using the DynamoDB SDK and root account access/secret access key to connect to DynamoDB from mobile. Which of the below-mentioned statements is true with respect to the best practice for security in this scenario?</p> | <p> DynamoDBを呼び出して特定のデータを取得するモバイルアプリケーションを作成しました。アプリケーションは、DynamoDB SDKとrootアカウントのアクセス/秘密のアクセスキーを使用して、モバイルからDynamoDBに接続しています。このシナリオでのセキュリティのベストプラクティスに関して、下記のステートメントのどれが当てはまるのですか？</p>	sa:	The application should use an IAM role with web identity federation which validates calls toDynamoDB with identity providers, such as Google, Amazon, and Facebook. <br>  アプリケーションでは、Google、Amazon、Facebookなどのアイデンティティプロバイダを使用してDynamoDBへの呼び出しを検証するWeb IDフェデレーションでIAMロールを使用する必要があります。|<p><br></p><p>Option A is incorrect because creating a separate user for each application user is not a feasible, secure, and recommended solution.</p><p>Option B is incorrect because the mobile users may not all be AWS users. You need to give access to the mobile application via federated identity provider.</p><p>Option C is CORRECT because it creates a role for Federated Users which enables the users to sign in to the app using their Amazon, Facebook, or Google identity and authorize them to seamlessly access DynamoDB.</p><p>Option D is incorrect because creating IAM Role is not sufficient. You need to authenticate the users of the application via web identity provider, then get the temporary credentials via a Security Token Service (STS) and then access DynamoDB.</p><p><br></p><p><b>More information on Web Identity Federation:</b></p><p>With Web Identity Federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known identity provider (IdP) —such as Login with Amazon, Facebook, Google, or any other&nbsp;<a href="http://openid.net/connect/" target="_blank">OpenID Connect (OIDC)</a>-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account.</p><p><br></p><p>For more information on Web Identity Federation, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a><br><p></p>	The user should create an IAM role with DynamoDB and EC2 access. Attach the role with EC2 and route all calls from the mobile through EC2. <br>  ユーザーは、DynamoDBおよびEC2アクセスを使用してIAMロールを作成する必要があります。EC2でロールを添付し、モバイルからのすべてのコールをEC2にルーティングします。	The user should create a separate IAM user for each mobile application and provide DynamoDB access with it. <br>  ユーザーは、モバイルアプリケーションごとに個別のIAMユーザーを作成し、DynamoDBにそのアプリケーションにアクセスする必要があります。	Create an IAM Role with DynamoDB access and attach it with the mobile application. <br>  DynamoDBアクセスを使用してIAMロールを作成し、モバイルアプリケーションに添付します。
Test3-71. <p>A user has created a VPC with CIDR 20.0.0.0/16. The user has created one subnet with CIDR 20.0.0.0/16 by mistake. The user is trying to create another subnet of CIDR 20.0.1.0/24. How can the user create the second subnet?</p> | <p>ユーザーがCIDR 20.0.0.0/16のVPCを作成しました。誤ってCIDR 20.0.0.0/16のサブネットを1つ作成しました。ユーザーがCIDR 20.0.1.0/24の別のサブネットを作成しようとしています。ユーザーはどのようにして2番目のサブネットを作成できますか？</p>	sa:	It is not possible to create a second subnet as one subnet with the same CIDR as the VPC has been created. <br>  VPCが作成されたのと同じCIDRを持つ1つのサブネットとして2つ目のサブネットを作成することはできません。|<p><br></p><p>Once you create a subnet, you cannot modify it, you have to delete it. Hence option B and D are incorrect.</p><p>Also, the VPC will not create the second subnet because of the overlapping CIDR and hence you need to delete and recreate the subnet again.</p><p>Below is the screenshot of what happens when you try to create a subnet of CIDR 20.0.1.0/24 on an existing subnet of 20.0.0.0/16</p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_71_1.png" alt="" width="1015" height="579" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p><br></p><p>For more information on VPC and subnets, please visit the link</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a><br><p></p>	The user can modify the first subnet CIDR from the console. <br>  ユーザーはコンソールから最初のサブネットCIDRを変更できます。	There is no need to update the subnet as VPC automatically adjusts the CIDR of the first subnet based on the second subnet’s CIDR. <br>  VPCが第2のサブネットのCIDRに基づいて第1のサブネットのCIDRを自動的に調整するので、サブネットを更新する必要はありません。	The user can modify the first subnet CIDR with AWS CLI. <br>  ユーザーは、AWS CLIを使用して最初のサブネットCIDRを変更できます。
Test3-72. <p>A user has launched an EC2 instance store-backed instance in the us-east-1a zone. The user created AMI #1 and copied it to the eu-west-1 region. After that, the user made a few updates to the application running in the us-east-1a zone. The user makes an AMI #2 after the changes. If the user launches a new instance in Europe from the AMI #1 copy, which of the below-mentioned statements is true?</p> | <p>ユーザーがus-east-1aゾーンにEC2インスタンスストアを使用したインスタンスを起動しました。ユーザーはAMI＃1を作成し、それをeu-west-1領域にコピーしました。その後、ユーザーはus-east-1aゾーンで実行されているアプリケーションを少し更新しました。ユーザは、変更後にAMI＃2を作成する。ユーザーがAMI＃1のコピーからヨーロッパの新しいインスタンスを起動した場合、以下のステートメントのどちらが当てはまりますか？</p>	sa:	The new instance in the eu-west-1 region will not have the changes made after the AMI copy. <br>  eu-west-1地域の新しいインスタンスは、AMIコピー後に変更されません。|<p><br></p><p>Option A is incorrect because (a) the changes made to the instance will not automatically get updated in the AMI in US-East-1, and (b) the already copied AMI will not have any reference to the AMI in the US-East-1 region.</p><p>Option B is incorrect because AWS does not automatically update the AMIs. It needs to be done manually.</p><p>Option C is incorrect because you can copy the instance store AMI between different regions.</p><p>Option D is CORRECT because the instance in the EU region will not have any changes made after copying the AMI. You will need to copy the AMI#2 to eu-west-1 and then launch the instance again to have all the changes.</p><p><br></p><p>For the entire details to copy AMI’s, please visit the link –</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</a><br><p></p>	The new instance will have the changes made after the AMI copy since AWS keeps updating the AMI. <br>  新しいインスタンスは、AWSがAMIを更新し続けるので、AMIコピー後に変更が加えられます。	It is not possible to copy the instance store backed AMI from one region to another. <br>  1つのリージョンから別のリージョンにバックアップされたインスタンス・ストアのAMIをコピーすることはできません。	The new instance will have the changes made after the AMI copy as AWS just copies the reference of the original AMI during the copying. Thus, the copied AMI will have all the updated data. <br>  新しいインスタンスは、AWSがコピー中に元のAMIの参照をコピーするだけなので、AMIコピー後に変更が加えられます。したがって、コピーされたAMIは、すべての更新されたデータを有することになる。
Test3-73. <p>Company B has created an e-commerce site using DynamoDB and is designing a table named Products that includes items purchased and the users who purchased them. When creating a primary key on this table which of the following can be selected as the best attribute for a Partition key?<br></p><p>Select the best possible answer:</p> | <p> B社はDynamoDBを使用してeコマースサイトを作成し、購入したアイテムと購入したユーザーを含むProductsという名前のテーブルを設計しています。このテーブルにプライマリキーを作成する際に、パーティションキーのための最良の属性として選択できるものはどれですか？<br> </p> <p>可能な最良の答えを選択してください：</p>	sa:	user_id where there are many users to few products <br>  多くのユーザーから少数の製品までのuser_id|<p><br></p><p>When defining primary keys, you should always use the&nbsp;"many to few principle".&nbsp;Hence, option A is the best answer.</p><p><br></p><p>For more information on DynamoDB, please visit the link</p><p></p><a href="https://aws.amazon.com/dynamodb/faqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/dynamodb/faqs/</a><br><p></p>	product_id where there are few products to many users <br>  多くのユーザーに商品が少ないproduct_id	category_id where there are few categories to many products <br>  category_id多くの製品にはカテゴリがほとんどありません	None of the above <br>  上記のどれでもない
Test3-74. <p>You are writing an AWS CloudFormation template and you want to assign values to properties that will not be available until runtime. You know that you can use intrinsic functions to do this but are unsure as to which part of the template they can be used in. Which of the following is correct in describing how you can currently use intrinsic functions in an AWS CloudFormation template? <br></p><p>Choose an option from the below:<br></p> | <p> AWS CloudFormationテンプレートを作成しており、実行時まで利用できないプロパティに値を割り当てる必要があります。組み込み関数を使用することができますが、テンプレートのどの部分を使用できるかは不明です。AWS CloudFormationテンプレートで組み込み関数を現在どのように使用できるかを説明するのは正しいですか？<br> </p> <p>下記からオプションを選択してください：<br> </p>	sa:	You can only use intrinsic functions in specific parts of a template. You can use intrinsic functions in resource properties, metadata attributes, and update policy attributes. <br>  組み込み関数は、テンプレートの特定の部分でのみ使用できます。組み込み関数は、リソースプロパティー、メタデータ属性、および更新ポリシー属性で使用できます。|<p><br></p><p>As per AWS documentation:</p><p>You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes. You can also use intrinsic functions to conditionally create stack resources.<br></p><p><br></p><p>Hence, B is the correct answer.</p><p><br></p><p>For more information on intrinsic function please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a><br><p></p>	You can use intrinsic functions in any part of a template. <br>  テンプレートのどの部分でも、組み込み関数を使用できます。	You can use intrinsic functions only in the resource properties part of a template. <br>  組み込み関数は、テンプレートのリソースプロパティ部分でのみ使用できます。	You can use intrinsic functions in any part of a template, except AWSTemplateFormatVersion and Description. <br>  AWSTemplateFormatVersionとDescriptionを除く、テンプレートの任意の部分で組み込み関数を使用できます。
Test3-75. <p>A corporate web application is deployed within an Amazon VPC and is connected to the corporate data center via IPSec VPN. The application must authenticate against the on-premise LDAP server. Once authenticated, logged-in users can only access an S3 keyspace specific to the user.</p><p>Choose 2 options from the below:</p> | <p>企業WebアプリケーションはAmazon VPC内に配置され、IPSec VPN経由で企業のデータセンターに接続されます。アプリケーションは、オンプレミスLDAPサーバーに対して認証する必要があります。</p> <p>下記の2つのオプションを選択してください：</p>	ma:	x:Develop an identity broker that authenticates against IAM Security Token Service (STS) to assume a IAM role in order to get temporary AWS security credentials The application calls the identity broker to get AWS temporary security credentials with access to the appropriate S3 bucket. <br>  一時的なAWSセキュリティ資格を取得するためにIAMセキュリティトークンサービス（STS）を認証するアイデンティティブローカーを開発します。アプリケーションはアイデンティティブローカーを呼び出して、適切なS3バケットにアクセスするAWS一時的セキュリティ資格情報を取得します。|<p><br></p><p></p><p>There are two architectural considerations here: (1) The users must be authenticated via the on-premise LDAP server, and (2) each user should have access to S3 only.</p><p>&nbsp;</p><p>With this information, it is important to first authenticate the users using LDAP, get the IAM Role name, then get the temporary credentials from STS, and finally access the S3 bucket using those credentials. And second, create an IAM Role that provides access to S3.</p><p>&nbsp;</p><p>Option A is incorrect because the users need to be authenticated using LDAP first, not STS. Also, the temporary credentials to log into AWS are provided by STS, not identity broker.</p><p><span style="font-size: 1rem;">Option B is CORRECT because it follows the correct sequence. It authenticates users using LDAP, gets the security token from STS, and then accesses the S3 bucket using the temporary credentials.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because it follows the correct sequence. It develops an identity broker that authenticates users against LDAP, gets the security token from STS, and then accesses the S3 bucket using the IAM federated user credentials.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because you cannot use the LDAP credentials to log into IAM.</span></p><p><br><span style="font-size: 1rem;">An example diagram of how this works from the AWS documentation is given below.</span></p><p>&nbsp;<img src="https://s3.amazonaws.com/awssap/3_75_1.png" alt="" width="723" height="692" role="presentation" class="img-responsive atto_image_button_text-bottom"><!--[endif]--></p><p>&nbsp;</p><p><br></p><p>For more information on federated access, please visit the below link:</p>  <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a><br><p></p><ul></ul><p></p>	o:The application authenticates against LDAP and retrieves the name of an IAM role associated with the user.? The application then calls the IAM Security Token Service (STS) to assume that IAM role. The application can use the temporary credentials to access the appropriate S3 bucket. <br>  アプリケーションはLDAPに対して認証を行い、ユーザーに関連付けられたIAMロールの名前を取得します。次に、アプリケーションはIAMセキュリティトークンサービス（STS）を呼び出して、そのIAMロールを想定します。アプリケーションは、一時資格情報を使用して適切なS3バケットにアクセスできます。	o:Develop an identity broker that authenticates against LDAP and then calls IAM Security Token Service (STS) to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket. <br>  LDAPに対して認証を行い、IAMセキュリティトークンサービス（STS）を呼び出してIAMフェデレーションされたユーザー資格情報を取得するIDブローカーを開発します。アプリケーションはIDブローカーを呼び出して、IAMフェデレーションされたユーザー資格情報を適切なS3バケットにアクセスできるようにします。	x:The application authenticates against LDAP the application then calls the AWS identity and Access?Management (IAM) Security service to log in to IAM using the LDAP credentials the application can use the IAM temporary credentials to access the appropriate S3 bucket. <br>  アプリケーションがLDAPに対して認証するアプリケーションは、AWS IDとアクセス管理（IAM）セキュリティサービスを呼び出してLDAP資格情報を使用してIAMにログインします。アプリケーションはIAM一時資格情報を使用して適切なS3バケットにアクセスできます。
Test3-76. <p>You have created an Elastic Load Balancer with Duration-Based sticky sessions enabled in front of your six EC2 web application instances in US-West-2. For High Availability, there are three web application instances in Availability Zone 1 and three web application instances in Availability Zone 2. To load test, you set up a software-based load tester in Availability Zone 2 to send traffic to the Elastic Load Balancer, as well as letting several hundred users browse to the ELB’s hostname.</p><p>After a while, you notice that the users’ sessions are spread evenly across the EC2 instances in both AZ’s, but the software-based load tester’s traffic is hitting only the instances in Availability Zone 2. What steps can you take to resolve this problem?</p><p>Choose 2 correct options from the below:<br></p> | <p> US-West-2の6つのEC2 Webアプリケーションインスタンスの前に、期間ベースのスティッキセッションを有効にしたElastic Load Balancerを作成しました。 ハイアベイラビリティの場合、可用性ゾーン1に3つのWebアプリケーションインスタンスがあり、可用性ゾーン2に3つのWebアプリケーションインスタンスがあります。テストをロードするには、可用性ゾーン2にソフトウェアベースのロードテスターを設定して、Elastic Load Balancer、 </ p> <p>しばらくすると、両方のAZのEC2インスタンスにユーザーのセッションが均等に分散されていることがわかりますが、ソフトウェアベースの負荷テスト担当者のトラフィック </ p> <p>以下の2つの正しいオプションを選択してください：	ma:	x:Create a software-based load tester in US-East-1 and test from there <br>  US-East-1にソフトウェアベースの負荷テスターを作成し、そこからテストします|<p></p><p></p><p><br></p><p>When you create an elastic load balancer, a default level of capacity is allocated and configured. As Elastic Load Balancing sees changes in the traffic profile, it will scale up or down. The time required for Elastic Load Balancing to scale can range from 1 to 7 minutes, depending on the changes in the traffic profile. When Elastic Load Balancing scales, it updates the DNS record with the new list of IP addresses. To ensure that clients are taking advantage of the increased capacity, Elastic Load Balancing uses a TTL setting on the DNS record of 60 seconds. It is critical that you factor this changing DNS record into your tests. If you do not ensure that DNS is re-resolved or use multiple test clients to simulate increased load, the test may continue to hit a single IP address when Elastic Load Balancing has actually allocated many more IP addresses. Because your end users will not all be resolving to that single IP address, your test will not be a realistic sampling of real-world behavior.</p><p>&nbsp;</p><p>Option A is incorrect because creating load tester in US-East-1 will face the same problem of traffic hitting only the instances in that AZ.</p><p>Option B is CORRECT because if you do not ensure that DNS is re-resolved the test may continue to hit the single IP address.</p><p>Option C is CORRECT because if the requests come from globally distributed users, the DNS will not be resolved to a single IP address and the traffic would be distributed evenly across multiple instances.</p><p>Option D is incorrect because the traffic will be routed to the same back-end instances as the users continue to access your application. The load will not be evenly distributed across the AZs.</p><p>&nbsp;</p><p>Please refer to the below article for more information:</p>  <a href="http://aws.amazon.com/articles/1636185810492479" target="_blank">http://aws.amazon.com/articles/1636185810492479</a><a href="http://aws.amazon.com/articles/1636185810492479" target="_blank"></a><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://aws.amazon.com/articles/1636185810492479" target="_blank"></a></span></li></ul><p></p>	o:Force the software-based load tester to re-resolve DNS before every request <br>  ソフトウェアベースの負荷テスト担当者がすべての要求の前にDNSを再解決するように強制する	o:Use a third party load-testing service to send requests from globally distributed clients <br>  サードパーティの負荷テストサービスを使用して、グローバルに分散しているクライアントから要求を送信する	x:Switch to Application-Controlled sticky sessions <br>  アプリケーション制御のスティッキセッションに切り替えます。
Test3-77. <p></p><span id="docs-internal-guid-846a0cce-51f3-e616-7c8b-e1c7e018b16a">Your application is using an ELB in front of an Auto Scaling group of web/application servers deployed across two AZs and a Multi-AZ RDS Instance for data persistence. The database CPU is often above 80% usage and 90% of I/O operations on the database are reads. To improve the performance, you recently added a single-node Memcached ElastiCache Cluster to cache frequent DB query results. In the next weeks the overall workload is expected to grow by 30%. Do you need to change anything in the architecture to maintain the high availability, or the application with the anticipated additional load and why?</span><p></p> | アプリケーションでは、2つのAZに分散しているWeb /アプリケーションサーバーのAuto Scalingグループの前にELBを使用し、データの永続性を維持するために複数のAZ RDSインスタンスを使用しています。 データベースCPUの使用率は80％を超えることが多く、データベースのI / O操作の90％が読み取りです。 パフォーマンスを向上させるために、最近、単一ノードのMemcached ElastiCache Clusterを追加して、頻繁なDBクエリ結果をキャッシュしました。 次の数週間で、全体的な作業負荷は30％増加すると予想されます。 高可用性を維持するためにアーキテクチャー内の何かを変更する必要がありますか、または予想される追加の負荷でアプリケーションを変更する必要がありますか？</ span> <p> </p>	sa:	Yes. You should deploy two Memcached ElastiCache Clusters in different AZs, because the RDS Instance will not be able to handle the load if the cache node fails. <br>  はい。 キャッシュノードに障害が発生した場合、RDSインスタンスは負荷を処理できないため、2つのMemcached ElastiCacheクラスタを異なるAZに配置する必要があります。|<p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is CORRECT because having two clusters in different AZs provide high availability of the cache nodes which removes the single point of failure. It will help caching the data; hence, reducing the overload on the database, maintaining the availability and reducing the impact.</p><p dir="ltr" style=""><span style="font-size: 1rem;">Option B is incorrect because, even though AWS will automatically recover the failed node, there are no other nodes in the cluster once the failure happens. So, the data from the cluster would be lost once that single node fails. For higher availability, there should be multiple nodes. Also, once the cache node fails all the cached read load will go to the database which will not be able to handle the load with 30% increase to current levels. This means there will be availability impact.</span></p><p dir="ltr" style="">Option C is incorrect because provisioning the nodes in the same AZ does not provide the tolerance for an AZ failure. For higher availability, the nodes should be spread across multiple AZs.</p><p dir="ltr" style="">Option D is incorrect because the very purpose of the cache node was to reduce the impact on the database by not overloading it. If the cache node fails, the database will not be able to handle the 30% increase in the load; so, it will have an availability impact.</p><br><p dir="ltr" style=""><b>More information on this topic from AWS Documentation:</b></p><br><p dir="ltr" style=""><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/BestPractices.html" target="_blank">http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/BestPractices.html</a></p><p dir="ltr" style=""><br></p><p dir="ltr" style=""><span><u>Mitigating Node Failures</u></span></p><p dir="ltr" style="">To mitigate the impact of a node failure, spread your cached data over more nodes. Because Memcached does not support replication, a node failure will always result in some data loss from your cluster.</p><p dir="ltr" style="">When you create your Memcached cluster you can create it with 1 to 20 nodes, or more by special request. Partitioning your data across a greater number of nodes means you’ll lose less data if a node fails. For example, if you partition your data across 10 nodes, any single node stores approximately 10% of your cached data. In this case, a node failure loses approximately 10% of your cache which needs to be replaced when a replacement node is created and provisioned.</p><p dir="ltr" style=""><br></p><p dir="ltr" style=""><u>Mitigating Availability Zone Failures</u></p><p dir="ltr" style="">To mitigate the impact of an availability zone failure, locate your nodes in as many availability zones as possible. In the unlikely event of an AZ failure, you will lose only the data cached in that AZ, not the data cached in the other AZs.</p></span><br><p></p>	No. If the cache node fails, the automated ElastiCache node recovery feature will prevent any availability impact. <br>  いいえ。キャッシュノードに障害が発生した場合、自動ElastiCacheノードリカバリ機能は可用性に影響を与えません。	Yes. You should deploy the Memcached ElastiCache Cluster with two nodes in the same AZ as the RDS DB master instance to handle the load if one cache node fails. <br>  はい。1つのキャッシュノードに障害が発生した場合に負荷を処理するには、RDS DBマスターインスタンスと同じAZに2つのノードを持つMemcached ElastiCacheクラスタを展開する必要があります。	No. If the cache node fails you can always get the same data from the DB without having any availability impact. <br>  いいえ。キャッシュノードに障害が発生しても、可用性に影響を与えることなく、常にDBから同じデータを取得できます。
Test3-78. <p>You are an architect for a new sharing mobile application. Anywhere in the world, your users can see local news on topics they chose. They can post pictures and videos from inside the application. Since the application is being used on a mobile phone, connection stability is required for uploading content and delivery should be quick.</p><p>Content is accessed a lot in the first minutes after it has been posted but is quickly replaced by new content before disappearing. The local nature of the news means that 90% of the uploaded content is then read locally.</p><p>What solution will optimize the user experience when users upload and view content (by minimizing page load times and minimizing upload times)?</p> | <p>あなたは新しい共有モバイルアプリケーションのアーキテクトです。 世界のどこにいても、ユーザーは自分が選択したトピックに関するローカルニュースを見ることができます。 彼らは、アプリケーション内から写真やビデオを投稿することができます。 アプリケーションは携帯電話で使用されているため、コンテンツのアップロードには接続の安定性が必要で、配信は迅速に行う必要があります。</ p> <p>コンテンツは投稿後最初の数分で多くアクセスされます。 消える前に新しいコンテンツ。 </ p> <p>ユーザーがコンテンツをアップロードして表示するときに、ページの読み込み時間を最小限に抑え、アップロード時間を最小限に抑えることで、ユーザーエクスペリエンスを最適化するソリューションにはどのようなものがありますか？ ？</p>	sa:	Use CloudFront for uploading the content to S3 bucket and for content delivery. <br>  コンテンツをS3バケットにアップロードしたり、コンテンツ配信にCloudFrontを使用します。|<p><br></p><p>Option A is incorrect because, even though it is a workable solution, a better approach is to use CloudFront for both uploading as well as distributing the content (not just distributing) - which is done in option D.</p><p>Option B and C are both incorrect because you do not need to upload the content to the source that is coser to the user. CloudFront will take care of that.</p><p>Option D is CORRECT because it uses CloudFront for both uploading as well as distributing the content (not just distributing) which is the most efficient use of the service.</p><p><br></p><p>For more information on Cloudfront please refer to the below URL</p><p></p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html</a><br><p></p>	Upload and store in S3 in the region closest to the user and then use multiple distributions of CloudFront. <br>  ユーザーに最も近い地域にS3でアップロードして保存してから、CloudFrontの複数のディストリビューションを使用します。	Upload to EC2 in regions closer to the user, send content to S3, use CloudFront. <br>  ユーザーに近い地域のEC2にアップロードし、S3にコンテンツを送信し、CloudFrontを使用します。	Upload and store in S3, and use CloudFront. <br>  S3でアップロードして保存し、CloudFrontを使用します。
Test3-79. <p>You are maintaining an application that is spread across multiple web servers and has incoming traffic balanced by ELB. The application allows users to upload pictures. Currently, each web server stores the image and a background task synchronizes the data between servers. However, the synchronization task can no longer keep up with the number of images uploaded</p><p>What change could you make so that all web servers have a place to store and read images at the same time?</p><p>Choose an option from the below:</p> | <p>複数のWebサーバーにまたがってELBによって受信トラフィックのバランスがとられているアプリケーションをメンテナンスしています。このアプリケーションでは、ユーザーは写真をアップロードできます。現在、各Webサーバーはイメージを保存し、バックグラウンドタスクはサーバー間でデータを同期させます。しかし、同期タスクはもはやアップロードされた画像の数に追いつくことができません</p> <p>すべてのWebサーバが同時に画像を保存して読む場所を持つように変更できますか？</p> <p>下記からオプションを選択してください：</p>	sa:	Store the images in Amazon S3. <br>  Amazon S3に画像を保存します。|<p><br></p><p>Option A is CORRECT because S3 provides a durable, secure, cost effective, and highly available storage service for the uploaded pictures.</p><p>Option B is incorrect because the application needs just a storage solution, not a global content distribution service. CloudFront is also costlier solution compared to S3.</p><p>Option C is incorrect because you cannot share EBS volumes among multiple EC2 instances.</p><p>Option D is incorrect because ELB cannot be used as a storage service.</p><p><br></p><p>For more information on AWS S3, please refer to the below url:</p><p></p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html</a><br><br><p></p>	Store the images on Amazon CloudFront. <br>  Amazon CloudFrontに画像を保存します。	Store the images on Amazon EBS. <br>  Amazon EBSに画像を保存します。	Store the images on the ELB. <br>  画像をELBに保存します。
Test3-80. <p>The company runs a complex customer system and consists of 10 different software components all backed up by RDS. You adopted Opswork to simplify management and deployment of that application and created a stack and layers for each component.</p><p>A security policy requires that all instances should run on the latest AMI and that instances must be replaced within one month after the latest AMI has been released. AMI replacements should be done without incurring application downtime or capacity problems. You decide to write a script to be run as soon as the new AMI is released.</p><p>Choose 2 options which meet your requirements:</p> | <p>同社は複雑な顧客システムを運営しており、RDSによってバックアップされた10種類のソフトウェアコンポーネントで構成されています。</p> <p>セキュリティポリシーでは、すべてのインスタンスを最新のAMIで実行する必要があり、インスタンスを1か月以内に置き換える必要があります最新のAMIがリリースされた後 AMIの置き換えは、アプリケーションのダウンタイムや容量の問題を引き起こすことなく行う必要があります。</p> <p>要件を満たす2つのオプションを選択してください：</p> <p>新しいAMIがリリースされるとすぐに実行するスクリプトを作成します。	ma:	x:Assign a custom recipe to each layer which replaces the underlying AMI. Use OpsWorks life-cycle events to incrementally execute this custom recipe and update the instances with the new AMI. <br>  基礎となるAMIを置き換える各レシピにカスタムレシピを割り当てます。OpsWorksのライフサイクルイベントを使用して、このカスタムレシピを段階的に実行し、新しいAMIでインスタンスを更新します。|<p><br></p><p>Option A is incorrect because to change the AMI you would have to re-launch new instances and you can't do that with chef recipes only.</p><p>Option B is incorrect because the AMI replacements should be done without incurring application downtime or capacity problems. So if you shutdown the stack, all applications will be stopped.</p><p>Option C is incorrect because the application could face the problem of insufficient capacity.</p><p>Option D is CORRECT because it represents a common practice of Blue-Green Deployment which is carried out for reducing the downtime and risk by running two identical production environments called Blue and Green. Please see "More information.." section for additional details.</p><p>Option E is CORRECT because you can only add new instances at the layer level by specifying to use Custom AMI at the stack level.</p><p><br></p><p><b>More information on Blue-Green Deployment:</b></p><p><span style="font-size: 1rem;">Blue-green deployment is a technique that reduces downtime and risk by running two identical production environments called Blue and Green.</span></p><p><span style="font-size: 1rem;">At any time, only one of the environments is live, with the live environment serving all production traffic. For this example, Blue is currently live and Green is idle.</span></p><p><span style="font-size: 1rem;">As you prepare a new version of your software, deployment and the final stage of testing takes place in the environment that is not live: in this example, Green. Once you have deployed and fully tested the software in Green, you switch the router so all incoming requests now go to Green instead of Blue. Green is now live, and Blue is idle.</span></p><p><span style="font-size: 1rem;">This technique can eliminate downtime due to application deployment. In addition, blue-green deployment reduces risk: if something unexpected happens with your new version on Green, you can immediately roll back to the last version by switching back&nbsp;</span><span style="font-size: 1rem;">to Blue.</span></p><p><img src="https://s3.amazonaws.com/awssap/3_80_1.jpeg" alt="" width="1076" height="696" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>Please refer to the below URL for more details</p><p></p><a href="https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf" target="_blank" style="font-size: 1rem;">https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a><br><p></p>	x:Specify the latest AMI as the custom AMI at the stack level terminates instances of the stack and let OpsWork launch new instances with the new AMI. <br>  最新のAMIをスタックレベルのカスタムAMIとして指定すると、スタックのインスタンスが終了し、OpsWorkは新しいAMIで新しいインスタンスを起動します。	x:Identify all EC2 instances of your OpsWork stack, stop each instance, replace the AMI ID property with the latest AMI ID, and restart the instance. To avoid down time, make sure no more than one instance is stopped at the same time. <br>  OpsWorkスタックのすべてのEC2インスタンスを特定し、各インスタンスを停止し、AMI IDプロパティを最新のAMI IDに置き換えてインスタンスを再起動します。停止時間を避けるために、複数のインスタンスが同時に停止していないことを確認してください。	o:Create a new stack and layers with identical configuration, add instances with the latest AMI specified as a custom AMI to the new layers, switch DNS to the new stack, and tear down the old stack. <br>  新しいスタックとレイヤーを同一の構成で作成し、カスタムAMIとして指定された最新のAMIを持つインスタンスを新しいレイヤーに追加し、DNSを新しいスタックに切り替え、古いスタックを破棄します。	o:Add new instances with the latest Amazon AMI as a custom AMI to all OpsWork layers of your stack and terminate the old ones. <br>  最新のAmazon AMIをカスタムAMIとして新しいインスタンスをスタックのすべてのOpsWorkレイヤに追加し、古いものを終了します。
