#format:table
#title:AWS SAP
#question_count:5
#shuffle_questions:false
#shuffle_choices:true
Diag-01. <p>Your company asked you to create a mobile application. The application is built to work with DynamoDB as the backend and Javascript as the frontend. During the usage of the application, you notice that there are spikes in the application, especially in the DynamoDB area. Which option provides the most cost-effective and scalable architecture for this application? Choose an answer from the options below.</p> |  <p>モバイルアプリケーションの作成を依頼しました。このアプリケーションは、バックエンドとしてDynamoDBを使用し、フロントエンドとしてJavascriptを使用するように構築されています。アプリケーションの使用中に、アプリケーションに、特にDynamoDB領域にスパイクが存在することがわかります。このアプリケーションで最も費用対効果の高いスケーラブルなアーキテクチャを提供するオプションはどれですか？下記のオプションから回答を選択してください。</p>	sa:	Create a service that pulls SQS messages and writes these to DynamoDB to handle sudden spikes in DynamoDB. <br>  SQSメッセージをプルするサービスを作成し、これらをDynamo DBに書き込み、DynamoDBの急激なスパイクを処理します。|<p><br></p><p>When the idea comes to scalability then SQS is the best option. Normally DynamoDB is scalable, but since one is looking for a cost-effective solution, the messaging in SQS can assist in managing the situation mentioned in the question.</p><p><br></p><p>Option A is incorrect because as DynamoDB is a managed service, autoscaling part of it is AWS's responsibility. You can only adjust the provisioned capacity as needed. </p><p>Option B is incorrect because even though increasing the write capacity would solve the issue, it is not cost efficient.</p><p>Option C is CORRECT because SQS is a scalable and very cost effective solution where it can store the data as messages in a queue for DynamoDB to be processed later when it has the sufficient capacity. This way the application would not lose any data due to sudden increase in the traffic.</p><p>Option D is incorrect because there is no configuration for multi-AZ deployment of DynamoDB.</p><p><br></p><p><b style="font-size: 1rem;">More information on SQS:</b></p><p>Amazon Simple Queue Service (SQS) is a fully-managed message queuing service for reliably communicating among distributed software components and microservices at any scale. Building applications from individual components that each perform a discrete function improve scalability and reliability, and is the best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.</p><p><br></p><p>For more information on SQS, please refer to the below url<br></p><a href="https://aws.amazon.com/sqs/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/sqs/</a><br><br>Note: AWS exams generally do not include questions fully based on latest updates on their services. Hardly one or two questions may have these updated services but only as one of the options (still very unlikely). For DynamoDB AutoScaling, it is highly unlikely that a scenario or correct answer would be based on it. Just knowing what the option is for (in brief) should be sufficient. <br><br> <p></p>	Increase write capacity of DynamoDB to meet the peak loads.  <br>  ピーク負荷を満たすためにDynamo DBの書き込み容量を増やします。	Autoscale DynamoDB to meet the requirements. <br>  要件を満たすためにDynamoDBをオートスケールします。	Launch DynamoDB in Multi-AZ configuration with a global index to balance writes. <br>  グローバルインデックスを使用して複数のAZ構成でDynamoDBを起動し、書き込みのバランスをとります。
Diag-02. <p>There are 2 companies that have their own AWS accounts. How can they connect to a central VPC for identity validation? How would you best design this solution? Choose an answer from the options below</p> |  <p>独自のAWSアカウントを持つ企業は2社あります。 ID検証のために中央のVPCにどのように接続できますか？どのようにこのソリューションを設計するのが最適でしょうか？下記のオプションから回答を選択してください。</p>	sa:	Create a VPC peering connection with the central VPC  <br>  中央VPCとのVPCピアリング接続を作成する|<p><br></p><p>Option A is incorrect because each VPC has its own purpose (responsibility) and setting of the resources inside - such as NACL, Security Groups, EC2 instances, NAT Instance/Gateway, Routing Tables etc., So merging into single VPC would defeat its purpose.</p><p>Option B is CORRECT because VPC peering allows the resources in peer VPCs to communicate with each other and in this case, can validate the identities of the resources. See the image below. Also, VPCs from different regions can be peered as well (Inter-Region VPC Peering).</p><p><span style="font-size: 1rem;">Option C is incorrect because Direct Connect should be used when there is a need for dedicated connection between a customer data center and AWS/VPC.</span></p><p>Option D is incorrect because VPN should be used when there is a need for connecting customer data center to AWS/VPC resources via customer gateway.</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/d_02_1.png" alt="" width="312" height="183" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><br></p><p><b>More information on VPC Peering</b></p><p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an <em>inter-region</em> VPC peering connection).</p><p>For more information on VPC Peering please see the below link</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p><p><span style="font-size: 1rem;"><a href="https://aws.amazon.com/blogs/aws/new-almost-inter-region-vpc-peering/" target="_blank">https://aws.amazon.com/blogs/aws/new-almost-inter-region-vpc-peering/</a></span></p>	Migrate each VPC resources to the central VPC using migration tools such as Import/Export, Snapshot, AMI Copy, and S3 sharing. <br>  インポート/エクスポート、スナップショット、AMIコピー、S3共有などの移行ツールを使用して、各VPCリソースを中央のVPCに移行します。	Create a Direct Connect connection from each VPC endpoint to the central VPC. <br>  各VPCエンドポイントから中央VPCへのダイレクトコネクト接続を作成します。	Create an OpenVPN instance in central VPC and establish an IPSec tunnel between VPCs. <br>  中央のVPCにOpenVPNインスタンスを作成し、VPC間にIPSecトンネルを確立します。
Diag-03. <p></p><span id="docs-internal-guid-846a0cce-557f-52e6-b387-6594f1a8eb46">As AWS grows, most of your clients' main concerns seem to be about security, especially when all of their competitors also seem to be using AWS. One of your clients asks you whether having a competitor who hosts their EC2 instances on the same physical host would make it easier for the competitor to hack into the client's data. Which of the following statements would be the best choice to put your client's mind at rest? </span><p></p> |  <p> <span id = "docs-internal-guid-846a0cce-557f-52e6-b387-6594f1a8eb46"> AWSが成長するにつれて、クライアントの主な懸案事項のほとんどはセキュリティに関するものです。競合他社もAWSを使用しているようです。クライアントの1人が、同じ物理ホスト上でEC2インスタンスをホストしている競合他社を持つことによって、競合他社がクライアントのデータを簡単にハッキングできるようにするかどうかを尋ねます。あなたのクライアントの心を安らぎに置く最良の選択肢はどれですか？ </ span> <p> </p>	sa:	Different instances running on the same physical machine are isolated from each other via the Xen hypervisor.  <br>  同じ物理マシン上で実行されている異なるインスタンスは、Xenハイパーバイザーを介して互いに分離されています。|<p><br></p><p>Options A and B are incorrect because 256-bit AES is used for encrypting the data. Ensuring the isolation of the VMs running on a hypervisor is not its responsibility. </p><p>Option C is CORRECT because it is the hypervisor that hosts the VMs responsible for ensuring that the VMs are isolated from each other despite being hosted on the same underlying hypervisor.</p><p>Option D is incorrect because IAM permissions has nothing to do with the isolation of the VMs running on a hypervisor.</p><p><br></p><p><b>More information on this topic:</b></p><p><span>The shared responsibility model for infrastructure services, such as Amazon Elastic Compute Cloud (Amazon EC2) for example, specifies that AWS manages the security of the following assets: </span></p><p><span>• Facilities </span></p><p><span>• Physical security of hardware </span></p><p><span>• Network infrastructure </span></p><p><span>• Virtualization infrastructure<br></span></p><p><span><a href="https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/aws-security-best-practices.pdf</a><br></span></p><p><span><br></span></p>	Different instances running on the same physical machine are isolated from each other via the Xen hypervisor and via a 256-bit Advanced Encryption Standard (AES-256). <br>  Xenハイパーバイザーと、256ビットの高度暗号化規格（AES-256）を使用します。	Different instances running on the same physical machine are isolated from each other via a 256- bit Advanced Encryption Standard (AES-256). <br>  同じ物理マシン上で実行されている異なるインスタンスは、256ビットAdvanced Encryption Standard（AES-256）を介して互いに分離されています。	Different instances running on the same physical machine are isolated from each other via IAM permissions. <br>  同じ物理マシン上で実行されている異なるインスタンスは、IAM権限を使用して互いに分離されています。
Diag-04. <p>You are building a large-scale confidential documentation web server on AWS, and all of the documentation for it will be stored on S3. One of the requirements is that it cannot be publicly accessible from S3 directly, and you will need to use CloudFront to accomplish this. Which of the methods listed below would satisfy the requirements as outlined? Choose an answer from the options below</p> |  <p> AWS上に大規模な機密文書Webサーバーを構築しており、そのドキュメントはすべてS3に保存されます。要件の1つは、S3から直接公開することはできず、CloudFrontを使用してこれを達成する必要があるということです。以下に列挙された方法のどれが、要約された要件を満たすか？下記のオプションから回答を選択してください。</p>	sa:	Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.  <br>  CloudFrontのOrigin Access Identity（OAI）を作成し、そのOAIへのS3バケット内のオブジェクトへのアクセスを許可します。|<p><br></p><p></p><p>There are two main points (1) the files should not be accessed directly via S3 as they are confidential, and (2) the files should be accessible via CloudFront.</p><p>If you want to use CloudFront signed URLs or signed cookies to provide access to objects in your Amazon S3 bucket, you probably also want to prevent users from accessing your Amazon S3 objects using Amazon S3 URLs. If users access your objects directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies, for example, control over the date and time that a user can no longer access your content and control over which IP addresses can be used to access content. In addition, if users access objects both through CloudFront and directly by using Amazon S3 URLs, CloudFront access logs are less useful because they're incomplete. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/d_04_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><br></p><p></p><p><span style="font-size: 1rem;">Option A is incorrect because it does not give CloudFront the exclusive access to S3 bucket.</span></p><p>Option B is CORRECT because it gives CloudFront the exclusive access to S3 bucket, and prevents other users from accessing the public content of S3 directly via S3 URL.</p><p>Option C is incorrect because you do not need to create any individual policies for each bucket.</p>  Option D is incorrect because (a) creating a bucket policy is unnecessary and (b) it does not prevent other users from accessing the public content of S3 directly via S3 URL.<br><br><p></p><p><span style="font-size: 1rem;">For more information on Origin Access Identity, please see the below link</span></p><p></p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a><br><br><p></p>	Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM User. <br>  CloudFrontのIDとアクセス管理（IAM）ユーザーを作成し、S3バケット内のオブジェクトへのアクセスをIAMユーザーに許可します。	Create individual policies for each bucket that stores documents and in that policy grant access to only CloudFront. <br>  バケットCloudFrontごとに個別のポリシーを作成します。	Create an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN). <br>  CloudFrontの配布IDをプリンシパルとして、ターゲットバケットをAmazonリソース名（ARN）としてリストするS3バケットポリシーを作成します。
Diag-05. <p></p><span id="docs-internal-guid-846a0cce-3197-ece7-45e0-efcb4bc2e2d8">A customer is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage all of their Amazon EC2 instances running in both the public and private subnets. They have only authorized the bastion-security-group with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC. Which of the following Bastion deployment scenarios will meet this requirement?</span><p></p> | 顧客は、企業ネットワークに接続されていない仮想プライベートクラウド（VPC）内の複数層のWebアプリケーションファームを実行しています。 彼らは、パブリックサブネットとプライベートサブネットの両方で動作するすべてのAmazon EC2インスタンスを管理するために、インターネット経由でVPCに接続しています。 彼らは、アプリケーションインスタンスセキュリティグループへのMicrosoftリモートデスクトッププロトコル（RDP）アクセスで砦セキュリティグループを承認しただけですが、VPC内のすべてのインスタンスへの管理アクセスをさらに制限したいと考えています。 この要件を満たすBastionの次の配備シナリオはどれですか？<p> </p> <span id = "docs-internal-guid-846a0cce-3197-ece7-45e0-efcb4bc2e2d8"企業ネットワークに接続されていません。彼らは、パブリックサブネットとプライベートサブネットの両方で動作するすべてのAmazon EC2インスタンスを管理するために、インターネット経由でVPCに接続しています。彼らは、アプリケーションインスタンスセキュリティグループへのMicrosoftリモートデスクトッププロトコル（RDP）アクセスで砦セキュリティグループを承認しただけですが、VPC内のすべてのインスタンスへの管理アクセスをさらに制限したいと考えています。この要件を満たすBastionの次の配備シナリオはどれですか？</ span> <p> </p>	sa:	Deploy a Windows Bastion host with an auto-assigned Public IP address in the public subnet, and allow RDP access to the bastion from only the corporate public IP addresses.  <br>  パブリックサブネットに自動的に割り当てられたパブリックIPアドレスを持つWindows Bastionホストを展開し、企業のパブリックIPアドレスのみからの要塞へのRDPアクセスを許可します。|<br><p dir="ltr" style="">Option A is incorrect because a bastion host should only access the instances in the private subnet, not all the instances.</p><p dir="ltr" style="">Option B is incorrect because the access should be RDP, not SSH, since the bastion host is a Windows machine.</p><p dir="ltr" style="">Option C is incorrect because the bastion host needs to be placed in the public subnet, not private.</p><p dir="ltr" style="">Option D is CORRECT because (a) it places the bastion host in the public subnet, and (b) only the corporate IP addresses has RDP access to it.</p><br><p dir="ltr" style="">For more information on controlling network access to EC2 instances using a bastion server, please see the link below:</p></span><a href="https://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/" target="_blank">https://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/</a><br><br><p></p>	Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere. <br>  パブリックサブネットにElastic IPアドレスを持つWindows Bastionホストを展開し、任意の場所からの要塞へのSSHアクセスを許可します。	Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses. <br>  プライベートサブネットにElastic IPアドレスを持つWindows Bastionホストを展開し、企業のパブリックIPアドレスのみからの要塞へのRDPアクセスを制限します。	Deploy a Windows Bastion host on the corporate network that has RDP access to all instances in the VPC. <br>  VPCのすべてのインスタンスへのRDPアクセス権を持つWindows Bastionホストを企業ネットワークに展開します。
Diag-06. <p>You have been tasked with creating file level restore on your EC2 instances. You already have the access to all the frequent snapshots of the EBS volume. You need to be able to restore an individual lost file on an EC2 instance within 15 minutes of a reported loss of information. The acceptable RPO is several hours. How would you perform this on an EC2 instance? Choose an answer from the options below</p> |  <p> EC2インスタンスでファイルレベルのリストアを作成する必要があります。すでにEBSボリュームのすべての頻繁なスナップショットにアクセスできます。報告された情報が失われてから15分以内に、EC2インスタンス上の個々の失われたファイルを復元できる必要があります。容認できるRPOは数時間です。 EC2インスタンスでこれをどのように実行しますか？下記のオプションから回答を選択してください。</p>	sa:	Create a volume from the source snapshot and attach the EBS volume to the same EC2 instance at a different mount location, browse the file system on the newly attached volume and select the file that needs to be restored, copy it from the new volume to the original source volume.  <br>  ソーススナップショットからボリュームを作成し、別のマウント場所にある同じEC2インスタンスにEBSボリュームを接続します。新しく接続されたボリューム上のファイルシステムを参照して、復元する必要があるファイルを選択し、新しいボリュームから元のソースボリュームにコピーします。|<p><br></p><p>Option A is incorrect because there is an assumption that the EC2 instance can read files from S3.</p><p>Option B is incorrect because the old volume that is connected to the EC2 could have different files compared to the snapshot that we are recovering from (i.e. some files may have been removed, some may have been newly added). If that volume is removed, we may lose some files and the final volume will not be in latest state. So, the best solution is to just copy the file from the recovery volume, and add it to the old connected/backup volume.</p><p>Option C is CORRECT because it mounts the EBS snapshot - that contains the file - as a volume and copies the file to the already attached volume. This way, the already attached volume always stays up-to-date. Once the file is copied, the volume - that was attached for copying the file - can be removed.</p><p>Option D is invalid because Amazon Data Lifecycle Manager (Amazon DLM) is used to automate the creation, retention, and deletion of snapshots taken to back up your Amazon EBS volumes. But for restoring a single file, we can mount the volume created from a snapshot to another mount location of the same EC2 instance and copy the file across to the initial volume from where the file is deleted.</p><p> </p><p>For more information on EBS snapshots please visit the below URL</p><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html</a></p>	Turn off the frequent snapshots of EBS volumes.?Create a volume from an EBS snapshot, attach the EBS volume to the EC2 instance at a different mount location, cutover the application to look at the new backup volume and remove the old volume <br>  EBSボリュームの頻繁なスナップショットをオフにします。EBSスナップショットからボリュームを作成し、EBSボリュームを別のマウント場所にあるEC 2インスタンスに接続し、アプリケーションを切り取って新しいボリュームを確認します	Setup a cron that runs aws s3 cp on the files and copy the files from the EBS volume to S3 <br>  ファイルに対してaws s3 cpを実行するcronをセットアップし、ファイルをEBSボリュームからS3にコピーします	Enable auto snapshots on Amazon EC2 and restore the EC2 instance upon single file failure <br>  Amazon EC 2で自動スナップショットを有効にし、単一ファイル障害時にEC 2インスタンスを復元する
Diag-07. <p><span id="docs-internal-guid-846a0cce-31b9-40fe-19c5-581c8823693f"></span></p><p dir="ltr" style="">Your company is in the process of developing a next generation pet collar that collects biometric information to assist families with promoting healthy lifestyles for their pets. Each collar will push 30kb of biometric data in JSON format every 2 seconds to a collection platform that will process and analyze the data providing health trending information back to the pet owners and veterinarians via a web portal. Management has tasked you to architect the collection platform ensuring the following requirements are met - </p><ul style=""><li dir="ltr"><p dir="ltr">Provide the ability for real-time analytics of the inbound biometric data </p></li><li dir="ltr"><p dir="ltr">Ensure that the processing of the biometric data is highly durable, elastic and parallel </p></li><li dir="ltr"><p dir="ltr">The results of the analytic processing should be persisted for data mining </p></li></ul><p dir="ltr" style="">Which architecture outlined below win meet the initial requirements for the collection platform?</p><br><p></p> |  <p> <span id = "docs-internal-guid-846a0cce-31b9-40fe-19c5-581c8823693f"> </ span> </p> <p dir = "ltr" style = "">ペットの健康的なライフスタイルを促進するために、生体情報を収集する次世代のペットカラーを開発するプロセス。各カラーは30KBのバイオメトリックデータをJSON形式で2秒ごとに収集プラットフォームに送り、データを処理して分析し、ウェブポータルを介してペットの所有者と獣医師に健康動向情報を返します。経営陣は収集プラットフォームを設計し、次の要件が満たされていることを確認しました。</p><ul style=""><li dir="ltr"><p dir="ltr">インバウンドバイオメトリックデータのリアルタイム分析機能を提供する </p></li><li dir="ltr"><p dir="ltr">バイオメトリックデータの処理が非常に耐久性があり、弾力的かつ平行であることを確認する</p></li><li dir="ltr"><p dir="ltr">分析処理の結果は、データマイニングのために保持する必要があります</p></li></ul><p dir="ltr" style="">下記のどのアーキテクチャが、収集プラットフォームの初期要件を満たしていますか？</p><br><p></p>	sa:	Utilize Amazon Kinesis to collect the inbound sensor data, analyze the data with Kinesis clients and save the results to a Redshift cluster using EMR.  <br>  Amazon Kinesisを利用して受信センサーデータを収集し、Kinesisクライアントでデータを分析し、結果をEMRを使用してRedshiftクラスターに保存します。|<br><p dir="ltr" style="">The main point to consider here is that the information is to be analyzed in real-time, the solution should be highly durable, elastic and and be processed in parallel, and the result should be persisted for data mining after the analysis. Whenever the question requires real-time processing of data, always think about using Amazon Kinesis!</p><br><p dir="ltr" style="">Option A is incorrect because (a) S3 is not efficient for collecting and storing real time data, and (b) daily scheduled data pipeline is not a real-time analytic solution.</p><p dir="ltr" style="">Option B is CORRECT because (a) Amazon Kinesis is ideal for capturing and processing real time data captured by the sensor, (b) it also stores the result of analysis later, and (c) Redshift cluster can be used for processing (data mining) the information captured by the Kinesis and copied via EMR.</p><p dir="ltr" style=""><span id="docs-internal-guid-0f201ac0-84b2-a5c5-c944-bb7574358393">Option C is incorrect because (a) S3 is not efficient for collecting and storing real time data, and (b) MSSQL Server RDS is not ideal for storing the information for data mining.</span><b id="docs-internal-guid-0f201ac0-84b2-a5c5-c944-bb7574358393"></b></p><p dir="ltr" style="">Option D is incorrect because (a) EMR alone is not ideal to capture data and would need specific frameworks like Kafka to capture data for processing. Also real time analytics needs to done using Spark Streaming and not EMR alone, and (b) DynamoDB is not used for data mining.</p><br><p dir="ltr" style="">More information on Amazon Kinesis with Redshift:</p></span><a href="https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html" target="_blank">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a><br><br><p></p>	Utilize S3 to collect the inbound sensor data analyze the data from S3 with a daily scheduled Data Pipeline and save the results to a Redshift Cluster. <br>  S3を使用してインバウンドセンサーデータを収集し、日単位のスケジュールされたデータパイプラインでS3からデータを分析し、その結果をRedshift Clusterに保存します。	Utilize S3 to collect the inbound sensor data analyze the data from SQS with Amazon Kinesis and save the results to a Microsoft SQL Server RDS instance. <br>  S3を利用してインバウンドセンサーデータを収集し、Amazon KinesisでSQSからデータを分析し、その結果をMicrosoft SQL Server RDSインスタンスに保存します。	Utilize EMR to collect the inbound sensor data, analyze the data from EUR with Amazon Kinesis and save me results to DynamoDB. <br>  EMRを使用してインバウンドセンサーデータを収集し、Amazon KinesisでEURのデータを分析し、結果をDynamoDBに保存します。
Diag-08. <p>A legacy software is hosted on an EC2 instance which has the license tied to the MAC address. From your experience with AWS, you know that every time an instance is restarted, it will almost certainly lose it's MAC address. What will be a possible solution to this? Choose an answer from the options below</p> |  <p>レガシーソフトウェアは、MACアドレスに関連付けられたライセンスを持つEC2インスタンスでホストされています。 AWSの経験から、インスタンスが再起動されるたびに、MACアドレスがほとんど失われることがわかります。これに可能な解決策は何でしょうか？下記のオプションから回答を選択してください。</p>	sa:	Use a VPC with an elastic network interface that has a fixed MAC Address.  <br>  弾性ネットワークインターフェイスを持つVPCを使用するには、固定MACアドレスが必要です。|<p><br></p><p></p><p>Option A is incorrect because you cannot map a static IP address to a MAC address.</p><p>Option B is incorrect because putting license server in private subnet would not resolve the dependency on the license that is based on a MAC address.</p><p>Option C is incorrect because MAC addresses cannot be tied to subnets.</p>  Option D is CORRECT because you should use Elastic Network Interface that is associated with a fixed MAC address. This will ensure that the legacy license based software would always work and not lose the MAC address any point in future.<br><p></p><p><br></p><p>For more information on elastic network interfaces please visit the below URL</p><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p>	Use a VPC with a private subnet for the license and a public subnet for the EC2. <br>  ライセンスにはプライベートサブネットを、EC 2にはパブリックサブネットを持つVPCを使用します。	Use a VPC with a private subnet and configure the MAC address to be tied to that subnet. <br>  プライベートサブネットでVPCを使用し、そのサブネットに結び付けるMACアドレスを設定します。	Make sure that EC2 Instance you deploy has a static IP address that is mapped to the MAC address. <br>  展開したEC 2インスタンスが、静的IPアドレスをMACアドレスにマップされていることを確認します。
Diag-09. <p>Your final task, that will complete a cloud migration for a customer, is to set up an Active Directory service for them so that they can use Microsoft Active Directory with the newly-deployed AWS services. After reading the AWS documentation for this, you discover that three options are available to set up the AWS Directory Service. You call the customer to collect more information about their requirements, and they tell you that they have 1,000 users on their AD service and want to be able to use their existing on-premises directory with AWS services. </p><p><br></p><p>Which of the following options would be the most appropriate to set up the AWS Directory Service for your customer?</p> |  <p>顧客のクラウド移行を完了する最後のタスクは、Active Directoryサービスを設定して、新しく導入されたAWSサービスでMicrosoft Active Directoryを使用できるようにすることです。このためのAWSドキュメントを読んだ後、AWSディレクトリサービスを設定するための3つのオプションがあることがわかりました。お客様は、要件についての詳細情報を収集するように顧客に電話し、ADサービスに1,000人のユーザーがおり、既存のオンプレミスディレクトリをAWSサービスで使用できるようにしたいとします。   </p> <p > <br> </p> <p>あなたの顧客にAWSディレクトリサービスを設定するのに最も適切なオプションはどれですか？</p>	sa:	AD Connector <br>  ADコネクタ|<p></p><p><br></p><p>For the exam, remember the usage of the following AD options:</p>  <ul type="disc">  <li>SimpleAD:      Microsoft Active Directory compatible directory from AWS Directory Service      and supports common features of an active directory.</li>  <li>AWS      Directory Service for Microsoft Active Directory: Managed Microsoft Active      Directory that is hosted on AWS cloud.</li>  <li>AD      Connector: Proxy service for connecting your on-premises Microsoft Active      Directory to the AWS cloud.</li> </ul>  <p><br> <!--[endif]--></p><p>Option A is incorrect because SimpleAD does not connect existing on-premises AD to AWS.</p><p>Option B is incorrect because AWS Directory Service for Microsoft AD is an AWS managed service that is hosted on the AWS cloud, it does not connect your AD with AWS.</p><p>Option C is CORRECT because AD Connector helps connecting your on-premises Microsoft Active Directory to the AWS cloud.</p>  Option D is incorrect because none of the options above are acceptable except AD Connector.<br><p></p><p> <img src="https://s3.amazonaws.com/awssap/d_09_1.png" alt="" width="764" height="436" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p><br></p><p>For an example of setup of AD connector, please visit the below URL:</p><p></p><ul><li><a href="https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/architecture.html" rel="noreferrer" style="font-size: 1rem;">https://docs.aws.amazon.com/quickstart/latest/active-directory-ds/architecture.html</a></li></ul><p></p>	AWS Directory Service for Microsoft Active Directory (Enterprise Edition)  <br>  Microsoft Active Directory（Enterprise Edition）用のAWSディレクトリサービス	Simple AD <br>  シンプルなAD	?Any of these options are acceptable as long as they configured correctly for 1,000 customers. <br>  これらのオプションは、1,000人の顧客に対して正しく構成されていれば、どれでもかまいません。
Diag-10. <p>You have two different groups to analyze data of a petabyte-scale data warehouse using Redshift. Each query issued by the first group takes approximately 1-2 hours to analyze the data while the second group's queries only take between 5-10 minutes to analyze data. You don’t want the second group's queries to wait until the first group's queries are finished. You need to design a solution so that this does not happen. Which of the following will be the best and cheapest solution to solve this dilemma? Choose an answer from the options below:</p> |  <p> Redshiftを使用して1ペタバイト規模のデータウェアハウスのデータを分析する2つの異なるグループがあります。第1のグループによって発行された各クエリは、データを分析するのに約1〜2時間かかるが、第2のグループのクエリは、データを分析するのに5〜10分しかかからない。最初のグループのクエリが終了するまで、2番目のグループのクエリを待たせたくありません。これが起こらないようにソリューションを設計する必要があります。次のうちどれがこのジレンマを解決する最も簡単で安価なソリューションですか？下記のオプションから回答を選択してください：</p>	sa:	Create two separate workload management groups and assign them to the respective groups. <br>  2つの別々のワークロード管理グループを作成し、それぞれのグループに割り当てます。|<p><br></p><p></p><p>Whenever the question gives you scenario where, in Redshift, there are two processes - one fast and one slow, and you are asked to ensure that there is no impact on the queries of a process, always think about creating two separate workload management groups.</p><p> </p><p>Option A is incorrect because Redshift does not have read replicas.</p><p>Option B is CORRECT because the best solution - without any effect on performance - is to create two separate workload management groups - one for each department and run the queries on them. See the image below.</p><p>Option C is incorrect because queries cannot be paused in Redshift.</p><p>Option D is <span style="font-size: 1rem;">incorrect because this will affect the performance of the current Redshift cluster.</span></p><p><span style="font-size: 1rem;"><br></span></p><img src="https://s3.amazonaws.com/awssap/d_10_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br><p></p><p><br></p><p></p><p><b>More information on Amazon Redshift Workload Management</b></p><p></p><p>Amazon Redshift workload management (WLM) enables users to manage priorities flexibly within workloads so that short, fast-running queries won't get stuck in queues behind long-running queries.</p><p>Amazon Redshift WLM creates query queues at runtime according to service classes, which define the configuration parameters for various types of queues, including internal system queues and user-accessible queues. From user's perspective, a user-accessible service class and a queue are functionally equivalent. For consistency, this documentation uses the term queue to mean a user-accessible service class as well as a runtime queue.</p><p>For more information on redshift workload management, please refer to the below URL<br> <a href="http://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html</a></p><p><br></p>	Create a read replica of Redshift and run the second team's queries on the read replica. <br>  Redshiftの読み取りレプリカを作成し、読み取りレプリカで2番目のチームのクエリを実行します。	Pause the long queries when necessary and resume them when no query is running.  <br>  必要なときに長い問合せを一時停止し、問合せが実行されていないときに再開します。	Start another Redshift cluster from a snapshot for the second team if the current Redshift cluster is busy processing long queries. <br>  現在のRedshiftクラスタが長いクエリを処理している場合、別のRedshiftクラスタを2番目のチームのスナップショットから開始します。
Diag-11. <p>You have just developed a new mobile application that handles analytics workloads on large-scale datasets that are stored on Amazon Redshift. Consequently, the application needs to access Amazon Redshift tables. Which of the following methods would be the best, both practically and security-wise, to access the tables? Choose the correct answer from the options below</p> |  <p> Amazon Redshiftに保存されている大規模データセットの分析ワークロードを処理する新しいモバイルアプリケーションを開発しました。したがって、アプリケーションはAmazonのRedshiftテーブルにアクセスする必要があります。テーブルにアクセスするのに、実際にもセキュリティ上も、以下の方法のどれがベストであろうか？下記のオプションから正解を選択してください</p>	sa:	Use roles that allow a web identity federated user to assume a role that allows access to the Redshift table by providing temporary credentials.  <br>  Web IDフェデレーションユーザーが一時的な資格情報を提供することによってRedshiftテーブルへのアクセスを許可する役割を引き受ける役割を使用します。|<p><br></p><p></p><p><span style="font-size: 1rem;">Tip: When a service, user, or application needs to access any AWS resource, always prefer creating an IAM Role over creating an IAM User.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because embedding keys in the application to access AWS resource is not a good architectural practice as it creates security concerns.</span></p><p>Option B is incorrect because HSM certificate is used by Redshift cluster to connect to the client's HSM in order to store and retrieve the keys used to encrypt the cluster databases.</p><p>Option C is incorrect because read-only policy is insufficient and embedding keys in the application to access AWS resource is not a good architectural practice as it creates security concerns.</p><p>Option D is CORRECT because (a) IAM role allows the least privileged access to the AWS resource, (b) web identity federation ensures the identity of the user, and (c) the user is given temporary credentials to access the AWS resource.</p><br><p></p><p></p><p>For more information on IAM policies please refer to the below link:</p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a><br></p><p>For more information on web identity federation please refer to the below link:</p><p></p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a><br><br><br><p></p>	Create a HSM client certificate in Redshift and authenticate using this certificate. <br>  RedshiftでHSMクライアント証明書を作成し、この証明書を使用して認証します。	Create a Redshift read-only access policy in IAM and embed those credentials in the application. <br>  Redshift読み取り専用アクセスポリシーをIAMで作成し、これらの資格情報をアプリケーションに埋め込みます。	Create an IAM user and generate encryption keys for that user. Create a policy for Redshift read-only access. Embed the keys in the application. <br>  IAMユーザーを作成し、そのユーザーの暗号化キーを生成します。アプリケーションにキーを埋め込みます。
Diag-12. <p><span id="docs-internal-guid-846a0cce-31b5-65e3-c76d-b44a1100ee7c">Your company has HQ in Tokyo and branch offices all over the world and is using a logistics software with a multi-regional deployment on AWS in Japan, Europe and USA. The logistic software has a 3-tier architecture and currently uses MySQL 5.6 for data persistence. Each region has deployed its own database. In the HQ region you run an hourly batch process reading data from every region to compute cross-regional reports that are sent by email to all offices. This batch process must be completed as fast as possible, to quickly optimize logistics. How do you build the database architecture in order to meet the requirements? </span></p> |  <p> <span id = "docs-internal-guid-846a0cce-31b5-65e3-c76d-b44a1100ee7c">あなたの会社は東京と支社に世界中に本拠を置き、複数の地域に分散したロジスティクスソフトウェアを使用しています日本、ヨーロッパ、米国のAWSに掲載されています。ロジスティックソフトウェアは3層アーキテクチャであり、現在はデータの永続化のためにMySQL 5.6を使用しています。各地域は独自のデータベースを展開しています。 HQ地域では、毎時のバッチ処理を実行して、各地域のデータを読み込んで、電子メールですべてのオフィスに送信される地域別レポートを計算します。このバッチプロセスは、ロジスティクスを迅速に最適化するために、できるだけ早く完了する必要があります。どのようにして要件を満たすためにデータベースアーキテクチャを構築しますか？ </ span> </p>	sa:	For each regional deployment, use RDS MySQL with a master in the region and a read replica in the HQ region. <br>  各地域の展開では、地域のマスターとRDS MySQLを使用し、HQ地域の読み取りレプリカを使用します。|<br><p dir="ltr" style="">The problem in the scenario is that, currently, an hourly batch process is run at the HQ region that reads the data from every region to compute cross-regional reports. This is a slow process and need to be quickened. The most ideal scenario would be to have the replicated database in </p><br><p dir="ltr" style="">Option A is incorrect because copying the data hourly to HQ region would be slow compared to the best option, which is D.</p><p dir="ltr" style="">Option B is incorrect because (a) taking hourly EBS snapshots would affect the performance of the database in its master region, and (b)  copying the snapshots hourly across the region would be a slow process.</p><p dir="ltr" style="">Option C is incorrect because (a) taking hourly RDS snapshots would affect the performance of the database in its master region, and (b)  sending the snapshots hourly across the region would be a slow and very costly process.</p><p dir="ltr" style="">Option D is CORRECT because (a) it creates a read replica in the HQ region which is updated asynchronously. This way, generating the reports would be very quick, and (b) it does not affect the performance of the databases in their respective master region.</p><p dir="ltr" style="">Option E is incorrect because AWS Direct Connect is very expensive option and it would take considerable time to setup.</p><br><p dir="ltr" style="">For more information on cross-region read replicas, please visit the link below:</p></span><a href="https://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/" target="_blank">https://aws.amazon.com/blogs/aws/cross-region-read-replicas-for-amazon-rds-for-mysql/</a><br><br><p></p>	For each regional deployment, use MySQL on EC2 with a master in the region and send hourly EBS snapshots to the HQ region. <br>  各地域の展開では、地域のマスターと一緒にEC 2のMySQLを使用し、HQ地域に毎時EBSスナップショットを送信します。	For each regional deployment, use RDS MySQL with a master in the region and send hourly RDS snapshots to the HQ region. <br>  各地域の展開では、地域のマスターと一緒にRDS MySQLを使用し、HQ地域に毎時RDSスナップショットを送信します。	For each regional deployment, use MySQL on EC2 with a master in the region and use S3 to copy data files hourly to the HQ region.  <br>  各地域の展開では、地域のマスターと一緒にEC 2のMySQLを使用し、S3を使用してHQ地域にデータファイルを1時間ごとにコピーします。	Use Direct Connect to connect all regional MySQL deployments to the HQ region and reduce network latency for the batch process. <br>  ダイレクトコネクトを使用して、地域のMySQL展開をすべてHQ領域に接続し、バッチ処理のネットワーク待ち時間を短縮します。
Diag-13. <p>A legacy application with licensing is attached to a single MAC address. Since an EC2 instance can receive a new MAC address while launching new instances. How can you ensure that your EC2 instance can maintain a single MAC address for licensing? Choose the correct answer from the options below:</p> |  <p>ライセンス付きレガシーアプリケーションは、単一のMACアドレスに添付されています。 EC2インスタンスは新しいインスタンスを起動しながら新しいMACアドレスを受け取ることができるためです。 EC2インスタンスがライセンスのために単一のMACアドレスを維持できることをどのように確認できますか？下記のオプションから正解を選んでください：</p>	sa:	Create an ENI and assign it to the EC2 instance. The ENI will have a static MAC address and can be detached and reattached to a new instance if the current instance becomes unavailable.  <br>  ENIを作成し、それをEC 2インスタンスに割り当てます。ENIは静的MACアドレスを持ち、現在のインスタンスが利用できなくなった場合に、新しいインスタンスに切り離して再接続することができます。|<p><span style="font-size: 1rem;">Tip: Whenever a question has a scenario where you need to use fixed MAC address for EC2 instances, always think about using Elastic Network Interface (ENI).</span></p><p>If a static MAC address is assigned to an ENI, it remains unchanged. As long as the EC2 has that ENI, it's MAC address will not change.</p><p><br></p><p><span style="font-size: 1rem;">Option A is CORRECT because, as mentioned above, as ENI with static MAC address can be assigned to the EC2 instance. If the instance becomes unavilable or needs to be replaced, the ENI can be detached and re-attached to another EC2 while maintaining the same MAC address.</span></p><p>Option B is incorrect because subnets have CIDR, not static MAC addresses.</p><p>Option C is incorrect because if the EC2 instance fails or becomes unavailable, its MAC address cannot be reused with another EC2 instance.</p><p>Option D is incorrect because you can avail ENI in order to have static MAC address for the EC2 instances.</p><p><br></p><p><b style="font-size: 1rem;"> More information on ENI on AWS Documentation:</b></p><p><img src="https://s3.amazonaws.com/awssap/d_13_1.png" alt="" width="763" height="262" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on elastic network interfaces please visit the below URL</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a><br><br><p></p>	Private subnets have static MAC addresses. Launch the EC2 instance in a private subnet and, if required, use a NAT to serve data over the internet. <br>  プライベートサブネットにはスタティックMACアドレスがあります。プライベートサブネットでEC 2インスタンスを起動し、必要に応じてNATを使用してインターネット経由でデータを配信します。	Configure a manual MAC address for each EC2 instance and report that to the licensing company. <br>  各EC 2インスタンスの手動MACアドレスを設定し、それをライセンス会社に報告します。	AWS cannot have a fixed MAC address; the best solution is to create a dedicated VPN/VGW gateway to serve data from the legacy application. <br>  AWSは固定MACアドレスを持つことはできません。最善のソリューションは、レガシーアプリケーションからのデータを提供する専用のVPN / VGWゲートウェイを作成することです。
Diag-14. <p>Your company has just set up a new document server on its AWS VPC, and it has four very important clients that it wants to give access to. These clients also have VPCs on AWS and it is through these VPCs that they will be given access to the document server. In addition, each of the clients should not have access to any of the other clients' VPCs. Choose the correct answer from the options below</p> |  <p>あなたの会社は、AWS VPC上に新しいドキュメントサーバーを設定しました。そして、アクセスしたい4つの非常に重要なクライアントがあります。これらのクライアントにはAWS上のVPCもあり、これらのVPCを介してドキュメントサーバへのアクセス権が与えられます。さらに、各クライアントは、他のクライアントのVPCのいずれにもアクセスすべきではありません。下記のオプションから正解を選択してください</p>	sa:	Set up VPC peering between your company's VPC and each of the clients' VPCs. <br>  会社のVPCと各クライアントのVPC間のVPCピアリングを設定します。|<p><br></p><p>In this scenario, you are asked how resources from 4 VPCs can access resources from another VPC. This is a use case of "Star-Shaped" VPC peering shown in the image below. In this configuration, VPCs that have non-overlapping CIDR with your VPC, are peered for the intent of accessing the resources using their private IP addresses.</p><p><img src="https://s3.amazonaws.com/awssap/d_14_1.png" alt="" width="787" height="661" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><br></p><p>Option A is CORRECT because, as mentioned above, the peered VPCs can share and access the resources within each other via their private IP addresses.</p><p>Option B is incorrect because you do not need to block any IP addresses in this scenario.</p><p>Option C is incorrect because the peering among the client VPCs is unnecessary. The only peering that is needed is between each client and your VPC.</p><p>Option D is incorrect because, for VPC Peering, the VPCs should not have overlapping CIDRs. So, VPCs having the same CIDR cannot be peered.</p><p><br></p><p>For more information on VPC Peering please see the below link</p><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p>	Set up VPC peering between your company's VPC and each of the clients' VPCs, but block the IPs from CIDR of the clients' VPCs to deny access between each other.  <br>  企業のVPCと各クライアントのVPC間でVPCピアリングを設定しますが、クライアントのVPCのCIDRからのIPをブロックして相互間のアクセスを拒否します。	Set up VPC peering between your company's VPC and each of the clients' VPC. Each client should have VPC peering set up between each other to speed up access time. <br>  クライアントのVPC間でVPCピアリングを設定します。各クライアントは、アクセス時間を短縮するために、相互にVPCピアリングを設定する必要があります。	Set up all the VPCs with the same CIDR but have your company's VPC as a centralized VPC. <br>  すべてのVPCを同じCIDRで設定しますが、会社のVPCを集中VPCとして使用します。
Diag-15. <p>A company has a library of on-demand MP4 files needing to be streamed publicly on their new video webinar website. The video files are archived and are expected to be streamed globally, primarily on mobile devices. Given the requirements what would be the best architecture for the company to design?</p><p>Select 2 Answers.</p> |  <p>会社には、新しいビデオウェブセミナーのウェブサイトに公開される必要があるオンデマンドMP4ファイルのライブラリがあります。ビデオファイルはアーカイブされてお​​り、主にモバイルデバイス上で全世界にストリーミングされる予定です。要件を考えれば、企業が設計するための最良のアーキテクチャは何ですか？</p> <p> 2つの回答を選択してください。</p>	ma:	x:Provision streaming EC2 instances which use S3 as the source for the HLS on-demand transcoding on the servers. Provision a new CloudFront streaming distribution with the streaming server as the origin.  <br>  S3をサーバー上のHLSオンデマンドトランスコードのソースとして使用するストリーミングEC2インスタンスをプロビジョニングします。 ストリーミングサーバーを起点として新しいCloudFrontストリーミング配信を提供します。|<p><br></p><p><span style="font-size: 1rem;">Tip: In exam, if the question presents a scenario, where the media is to be streamed globally in MP4 format, on multiple platform devices, always think about using Elastic Transcoder.</span></p><p>Option A is incorrect because (a) provisioning of streaming EC2 instances is a costly solution, (b) the videos are to be delivered on-demand, not live streaming.</p><p>Option B is incorrect because the videos are to be delivered on-demand, not live streaming. So, streaming server is not required.</p><p>Option C is CORRECT because it (a) it uses Elastic Transcoder for transcoding the videos to different formats, (b) it uses CloudFront distribution with download option for streaming the on demand videos using HLS on any mobile, and (c)  it uses S3 as origin, so keeps the cost low.</p><p>In the on demand streaming case, your video content is stored in Amazon S3. Viewers can choose to watch it at any desired time. A complete on-demand streaming solution typically makes use of Amazon S3 for storage, AWS Elemental MediaConvert for file-based video processing, and Amazon CloudFront for delivery.<br>Once uploaded, you may need to convert your video into the size, resolution, or format needed by a particular television or connected device. AWS Elemental MediaConvert will take care of this for you. MediaConvert takes content from S3, transcodes it per your request, and stores the result back in S3. Transcoding processes video files, creating compressed versions of the original content to reduce its size, change its format, or increase playback device compatibility.You can also create assets that vary in resolution and bitrate for adaptive bitrate streaming, which adjusts the viewing quality depending on the viewer's available bandwidth. AWS Elemental MediaConvert outputs the transcoded video to an S3 bucket.<br>The next step is global delivery with Amazon CloudFront. CloudFront caches content at the edges for low latency and high throughput video delivery.  This delivery can be made in two different ways. You can deliver the entire video file to the device before playing it, or you can stream it to the device.<br><br>More information is available at:<br> <a href="https://aws.amazon.com/cloudfront/streaming/" rel="noreferrer">https://aws.amazon.com/cloudfront/streaming/</a><br><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-video.html" rel="noreferrer">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-video.html</a><br></p><p><span><br></span></p><p><span style="font-size: 1rem;"><b>More information on Elastic Transcoder:</b></span></p><p><span style="font-size: 1rem;">Amazon Elastic Transcoder manages all aspects of the media transcoding process for you transparently and automatically. There’s no need to use administer software, scale hardware, tune performance, or otherwise manage transcoding infrastructure. You can simply create a transcoding “job” specifying the location of your source media file and how you want it transcoded. Amazon Elastic Transcoder also provides transcoding presets for popular output formats, which means that you don’t need to guess about which settings work best on particular devices. </span></p><p>For more information on Elastic transcoder, please see the below link</p><p><a href="https://aws.amazon.com/elastictranscoder/" target="_blank">https://aws.amazon.com/elastictranscoder/</a></p>	x:Provision streaming EC2 instances which use S3 as the source for the HLS on-demand transcoding on the servers. Provision a new CloudFront download distribution with the WOWZA streaming server as the origin. <br>  S3をサーバー上のHLSオンデマンドトランスコードのソースとして使用するストリーミングEC2インスタンスをプロビジョニングします。 WOWZAストリーミングサーバーを起点として、新しいCloudFrontダウンロード配布を提供します。	o:Upload the MP4 files to S3 and create an Elastic Transcoder job that transcodes the MP4 source into HLS chunks. Store the HLS output in S3 and create an on-demand video streaming CloudFront distribution with download option to serve the HLS file to end users. <br>  MP4ファイルをS3にアップロードし、MP4ソースをHLSチャンクにトランスコードするElastic Transcoderジョブを作成します。 S3にHLS出力を保存し、ダウンロードオプション付きのオンデマンドビデオストリーミングCloudFrontディストリビューションを作成し、HLSファイルをエンドユーザに配信します。	o: Upload the MP4 files to S3 and create an Elastic Transcoder job that transcodes the MP4 source into HLS chunks. Store the HLS output in S3 and create an RTMP CloudFront distribution with a live video streaming option to stream the video contents.  <br>  MP4ファイルをS3にアップロードし、MP4ソースをHLSチャンクにトランスコードするElastic Transcoderジョブを作成します。 S3にHLS出力を保存し、ビデオコンテンツをストリーミングするライブビデオストリーミングオプション付きのRTMP CloudFrontディストリビューションを作成します。
Diag-16. <p>A company needs to configure a NAT instance for its internal AWS applications to be able to download patches and package software. Currently, they are running a NAT instance that is using the floating IP scripting configuration to create fault tolerance for the NAT. The NAT instance needs to be built with fault tolerance in mind. What is the best way to configure the NAT instance with fault tolerance? Choose the correct answer from the options below:</p> |  <p>企業はパッチをダウンロードしてソフトウェアをパッケージ化できるように、内部AWSアプリケーションのNATインスタンスを設定する必要があります。現在、フローティングIPスクリプティング設定を使用してNATのフォールトトレランスを作成しているNATインスタンスが実行されています。 NATインスタンスはフォールトトレランスを考慮して構築する必要があります。フォールトトレランスでNATインスタンスを設定する最も良い方法は何ですか？下記のオプションから正解を選んでください：</p>	sa:	Create two NAT instances in two separate public subnet; create a route from the private subnet to each NAT instance for fault tolerance  <br>  2つの別個のパブリックサブネットに2つのNATインスタンスを作成します。フォールトトレランスのためにプライベートサブネットから各NATインスタンスへのルートを作成する|<p></p><br><p>Option A is incorrect because you would need at least two NAT instances for fault tolerance.</p><p>Option B is incorrect because if you put both NAT instances in a single public subnet and that subnet becomes unavailable or unreachable to the other instances, the architecture would not be fault tolerant.</p><p>Option C is CORRECT because you should place two NAT instances in two separate public subnets, and create route from instances via each NAT instance for achieving fault tolerance.</p><p>Option D is incorrect because you should not be putting the NAT instances in private subnet as they need to communicate with the internet. They should be in public subnet.</p><p><br></p><p><span style="font-size: 1rem;"><b>More information on NAT instances:</b></span></p><p></p><p>One approach to this situation is to leverage multiple NAT instances that can take over for each other if the other NAT instance should fail. This walkthrough and associated monitoring script (nat_monitor.sh) provide instructions for building an HA scenario where two NAT instances in separate Availability Zones (AZ) continuously monitor each other. If one NAT instance fails, this script enables the working NAT instance to take over outbound traffic and attempts to fix the failed instance by stopping and restarting it.</p><p>Below is a diagram for fault tolerant NAT instances.</p><p> <img src="https://s3.amazonaws.com/awssap/d_16_1.png" alt="" width="375" height="274" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on fault-tolerant NAT gateways please see the below link</p><p></p><a href="https://aws.amazon.com/articles/2781451301784570" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/articles/2781451301784570</a><br><p></p>	Create two NAT instances in a public subnet; create a route from the private subnet to each NAT instance for fault tolerance. <br>  パブリックサブネットに2つのNATインスタンスを作成します。フォールトトレランスのために、プライベートサブネットから各NATインスタンスへのルートを作成します。	Create one NAT instance in a public subnet; create a route from the private subnet to the NAT instance <br>  パブリックサブネットに1つのNATインスタンスを作成します。プライベートサブネットからNATインスタンスへのルートを作成する	Create two NAT instances in two separate private subnets. <br>  2つの別個のプライベートサブネットに2つのNATインスタンスを作成します。
Diag-17. <p>When you create a subnet, you specify the CIDR block for the subnet. The CIDR block of a subnet can be the same as the CIDR block for the VPC (for a single subnet in the VPC), or a subset (to enable multiple subnets). The allowed block size is between a /28 netmask and /16 netmasks. You decide to you create a VPC with CIDR block 10.0.0.0/24. Therefore, what is the maximum and the minimum number of IP addresses according to AWS and what is the number of IP addresses supported by the created VPC? Choose the correct answer from the options below:</p> |  <p>サブネットを作成するときは、サブネットのCIDRブロックを指定します。サブネットのCIDRブロックは、VPCの単一のサブネットのVPCのCIDRブロックと同じでも、複数のサブネットを使用可能にするサブセットでもかまいません。許可されるブロックサイズは、a / 28ネットマスクと/ 16ネットマスクの間です。 CIDRブロック10.0.0.0/24を使用してVPCを作成することに決めました。したがって、AWSによるIPアドレスの最大数と最小数は何ですか？また、作成されたVPCでサポートされるIPアドレスの数はいくらですか？下記のオプションから正解を選んでください：</p>	sa:	Maximum is 65,536 and the minimum is 16 and the created VPC supports 256 IP addresses  <br>  最大値は65,536、最小値は16で、作成されたVPCは256のIPアドレスをサポートしています|<p><br></p><p>First let us calculate the current number of IP addresses. The CIDR block is 10.0.0.0/24. Hence, out of 32 bits of address, 24 bits are set/masked. Hence, the remaining 8 bits indicate the remaining available IP addresses. Hence, the total number of current available instances is 2^(32-24) = 2^8 = 256.</p><p>Now, the maximum allowed block size is a /16 netmask. i.e. Out of 32, first 16 bits are set/masked, leaving 16 bits available. Hence, the total number of maximum available instances = 2^(32-16) = 2^16 = 65,536.</p><p>Now, the minimum allowed block size is a /28 netmask. i.e Out of 32, first 28 bits are set/masked, leaving 4 bits available. Hence, the total number of minimum available instances = 2^(32-28) = 2^4 = 16.</p><p><br></p><p>For more information on VPC and subnets please see the below link:</p><p></p><span style="font-size: 1rem;"><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></span><br><p></p>	Maximum is 256 and the minimum is 16 and the created VPC supports 24 IP addresses <br>  最大値は256、最小値は16、作成されたVPCは24のIPアドレスをサポート	Maximum is 65,536 and the minimum is 24 and the created VPC supports 28 IP addresses <br>  最大値は65,536、最小値は24、作成されたVPCは28のIPアドレスをサポートしています	Maximum is 28 and the minimum is 16 and the created VPC supports 24 IP addresses <br>  最大値は28、最小値は16、作成されたVPCは24のIPアドレスをサポート
Diag-18. <p><span id="docs-internal-guid-846a0cce-319a-b057-b638-91ff072d81ea">A 3-tier e-commerce web application is current deployed on-premises and will be migrated to AWS for greater scalability and elasticity. The web server currently shares read-only data using a network distributed file system. The app server tier uses a clustering mechanism for discovery and shared session state that depends on IP multicast. The database tier uses shared-storage clustering to provide database failover capability, and uses several read slaves for scaling. Data on all servers and the distributed file system directory is backed up weekly to off-site tapes. Which AWS storage and database architecture meets the requirements of the application?</span></p> |  <p> <span id = "docs-internal-guid-846a0cce-319a-b057-b638-91ff072d81ea"> 3層のeコマースWebアプリケーションは、現在の構内展開型であり、拡張性と弾力性を高めるためにAWSに移行されます。 Webサーバーは現在、ネットワーク分散ファイルシステムを使用して読み取り専用データを共有しています。 アプリケーションサーバー層は、IPマルチキャストに依存する検出および共有セッション状態のためのクラスタリングメカニズムを使用します。 データベース層は、共有ストレージ・クラスタリングを使用してデータベース・フェイルオーバー機能を提供し、複数の読み取りスレーブを使用してスケーリングを行います。 すべてのサーバーおよび分散ファイルシステムディレクトリのデータは、毎週オフサイトのテープにバックアップされます。 どのAWSストレージとデータベースアーキテクチャがアプリケーションの要件を満たしていますか？</ span> </p>	sa:	Web servers store read-only data in S3, and copy from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment and one or more Read Replicas. Backup web and app servers backed up weekly via AMIs, database backed up via DB snapshots.  <br>  WebサーバーはS3に読み取り専用データを格納し、ブート時にS3からルートボリュームにコピーします。 Appサーバーは、DynamoDBとIPユニキャストの組み合わせを使用して状態を共有します。 データベースでは、複数のAZ配備と1つ以上の読み取りレプリカでRDSを使用します。 AMIを介して毎週バックアップされるWebサーバーとAppサーバーをバックアップします。データベースはDBスナップショットを介してバックアップされます。|<br><p dir="ltr" style="">The main requirements of this scenario are: (1) the application should be scalable and elastic, (2) app servers should be able to share the state, (3) need read replicas, and (4) weekly backup of the data.</p><br><p dir="ltr" style="">Option A is CORRECT because (a) the overall architecture is highly available, elastic, and scalable, (b) web servers share state using DynamoDB and IP unicast that is supported by AWS, (c) it supports read replicas, and (d) weekly backup for servers using AMIs and data using DB snapshots.</p><p dir="ltr" style="">Option B is incorrect because you cannot backup data to Glacier using snapshots.</p><p dir="ltr" style="">Option C is incorrect because it does not address the requirement of having read replicas for elasticity and scalability.</p><p dir="ltr" style="">Option D is incorrect because AWS does not support IP multicast or broadcast. </p><br><p dir="ltr" style="">For more information on this topic, please visit the links below:</p></span><a href="https://d0.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/Storage/AWS%20Storage%20Services%20Whitepaper-v9.pdf</a><br><br><p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html" target="_blank"></a></p>	Web servers store read-only data in S3, and copy from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment and one or more Read replicas. Backup web servers app servers, and database backed up weekly to Glacier using snapshots. <br>  WebサーバーはS3に読み取り専用データを格納し、ブート時にS3からルートボリュームにコピーします。 Appサーバーは、DynamoDBとIPユニキャストの組み合わせを使用して状態を共有します。 データベースは、複数のAZ配備と1つ以上の読み取りレプリカでRDSを使用します。 スナップショットを使用して、Webサーバーのアプリケーションサーバーとデータベースを毎週氷河にバックアップします。	Web servers store read-only data in S3 and copy from S3 to root volume at boot time. App servers share state using a combination of DynamoDB and IP unicast. Database use RDS with multi-AZ deployment. Backup web and app servers backed up weekly via AMIs. Database backed up via DB snapshots. <br>  Webサーバーは、ブート時にS3に読み取り専用データを格納し、S3からルートボリュームにコピーします。 Appサーバーは、DynamoDBとIPユニキャストの組み合わせを使用して状態を共有します。 データベースは、マルチAZ配備のRDSを使用します。 AMI経由で毎週バックアップされたWebサーバーとAppサーバーをバックアップします。 データベースは、DBスナップショットを介してバックアップされます。	Web servers, store read-only data in an EC2 NFS server, mount to each web server at boot time. App servers share state using a combination of DynamoDB and IP multicast. Database use RDS with multi-AZ deployment and one or more Read Replicas. Backup web and app servers backed up weekly via AMIs, and database backed up via DB snapshots. <br>  Webサーバーは、読み取り専用データをEC2 NFSサーバーに保管し、起動時に各Webサーバーにマウントします。 Appサーバーは、DynamoDBとIPマルチキャストの組み合わせを使用して状態を共有します。 データベースでは、複数のAZ配備と1つ以上の読み取りレプリカでRDSを使用します。 AMIを介して毎週バックアップされるWebサーバーとAppサーバーをバックアップし、DBスナップショットを使用してデータベースをバックアップします。
Diag-19. <p>You have a legacy application running that uses an m4.large instance size and cannot scale with Auto Scaling, but only has peak performance 5% of the time. This is a huge waste of resources and money so your Senior Technical Manager has set you the task of trying to reduce costs while still keeping the legacy application running as it should. Which of the following will best accomplish the task your manager has assigned you? Choose the correct answer from the options below:</p> |  <p>従来のアプリケーションでは、m4.largeインスタンスサイズを使用し、自動スケーリングでは拡張できませんが、最大パフォーマンスは5％です。これは膨大なリソースとコストを浪費しているため、シニアテクニカルマネージャーがレガシーアプリケーションを実行しながらコストを削減しようとしています。あなたのマネージャーがあなたに割り当てたタスクを最も良く達成するのは次のうちどれですか？下記のオプションから正解を選んでください：</p>	sa:	Use a T2 burstable performance instance.  <br>  T2バースト可能なパフォーマンスインスタンスを使用します。|<p>The AWS documentation clearly indicates using T2 EC2 instance types for those instances which don’t use CPU that often.</p><p> <img src="https://s3.amazonaws.com/awssap/d_19_1.png" alt="" width="753" height="522" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on EC2 instance types please see the below link:</p><p></p><a href="https://aws.amazon.com/ec2/instance-types/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/ec2/instance-types/</a><br><p></p>	Use a C4.large instance with enhanced networking. <br>  拡張ネットワークでC4.largeインスタンスを使用します。	Use two t2.nano instances that have single Root I/O Virtualization. <br>  2つのt2を使用します。単一のルートI / O仮想化を持つナノインスタンス。	Use t2.nano instance and add spot instances when they are required. <br>  t2を使用します。Nanoインスタンスを作成し、必要なときにスポットインスタンスを追加します。
Diag-20. <p>The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. You can have multiple sets of DHCP options, but you can associate only one set of DHCP options with a VPC at a time. You have just created your first set of DHCP options, associated it with your VPC but now realize that you have made an error in setting them up and you need to change the options. Which of the following options do you need to take to achieve this? Choose the correct answer from the options below</p> |  <p> DHCP（Dynamic Host Configuration Protocol）は、TCP / IPネットワーク上のホストに構成情報を渡すための標準を提供します。複数のDHCPオプションセットを使用できますが、一度に1つのDHCPオプションセットのみをVPCに関連付けることができます。 DHCPオプションの最初のセットを作成してVPCに関連付けましたが、設定に間違いがあり、オプションを変更する必要があることが分かりました。これを達成するために次のオプションのうちどれを取る必要がありますか？下記のオプションから正解を選択してください</p>	sa:	You must create a new set of DHCP options and associate them with your VPC.  <br>  DHCPオプションの新しいセットを作成し、VPCに関連付ける必要があります。|<p><br></p><p>Option A, B, and D are incorrect because you cannot modify the DHCP options - neither via the console nor via CLI.</p><p>Option C is CORRECT because once you create a set of DHCP options, you cannot modify them. You must create a new set of DHCP options and associate it with your VPC.</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/d_20_1.png" alt="" width="905" height="320" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p> </p><p>For more information on DHCP Options set please see the below link:</p><p></p><span style="font-size: 1rem;"><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html" target="_blank">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_DHCP_Options.html</a></span><br><p></p>	You can modify the options from the console or the CLI. <br>  オプションは、コンソールまたはCLIから変更できます。	You need to stop all the instances in the VPC. You can then change the options, and they will take effect when you start the instances. <br>  VPC内のすべてのインスタンスを停止する必要があります。その後、オプションを変更することができ、インスタンスの起動時に有効になります。	You can modify the options from the CLI only, not from the console. <br>  コンソールからではなく、CLIからのみオプションを変更できます。
Diag-21. <p>The company you work for has a huge amount of infrastructure built on AWS. However, there has been some concerns recently about the security of this infrastructure, and an external auditor has been given the task of running a thorough check of all of your company's AWS assets. The auditor will be in the USA while your company's infrastructure resides in the Asia Pacific (Sydney) region on AWS. Initially, he needs to check all of your VPC assets, specifically, security groups and NACLs You have been assigned the task of providing the auditor with a login to be able to do this. Which of the following would be the best and most secure solution to provide the auditor with so he can begin his initial investigations? Choose the correct answer from the options below</p> |  <p>あなたが働いている会社は、AWS上に構築された膨大なインフラストラクチャを持っています。しかし、最近このインフラストラクチャのセキュリティに関する懸念がありました。外部監査人は、あなたの会社のすべてのAWS資産を徹底的にチェックする作業を行っています。あなたの会社のインフラストラクチャがAWSのアジア太平洋（シドニー）地域にある間、監査人は米国に居住します。最初に、彼はすべてのVPC資産、特にセキュリティグループとNACLをチェックする必要があります。これを行うには、監査人にログインを提供するタスクが割り当てられています。審査員に最初の捜査を開始できるようにするために、以下のうちどれが最高で最も安全な解決策になるでしょうか？下記のオプションから正解を選択してください</p>	sa:	Create an IAM Role with the read only permissions to access the AWS VPC infrastructure and assign that role to the auditor.  <br>  AWS VPCインフラストラクチャにアクセスし、そのロールを監査人に割り当てるための読み取り専用権限を持つIAMロールを作成します。|<p><br></p><p>Generally, you should refrain from giving high-level permissions and give only the required permissions. In this case, option C fits well by just providing the relevant access which is required.</p><p><br></p><p>Option A is incorrect because you should create an IAM Role with the needed permissions.  </p><p>Option B is incorrect because you should not give the root access as it will give the user full access to all AWS resources.</p><p>Option C is CORRECT because IAM Role gives just the minimum required permissions (read-only) to audit the VPC infrastructure to the auditor.</p><p>Option D is incorrect because you should not give the auditor full access to the VPC.</p><p><br></p><p>For more information on IAM, please see the below link</p><p><a href="https://aws.amazon.com/iam/" target="_blank">https://aws.amazon.com/iam/</a></p>	Give him root access to your AWS Infrastructure, because he is an auditor he will need access to every service. <br>  彼はすべてのサービスにアクセスする必要がある監査員であるため、AWSインフラストラクチャへのルートアクセスを許可します。	Create an IAM user tied to an administrator role. Also provide an additional level of security with MFA. <br>  管理者ロールに関連付けられたIAMユーザーを作成します。また、MFAのセキュリティをさらに強化します。	Create an IAM user with full VPC access but set a condition that will not allow him to modify anything if the request is from any IP other than his own. <br>  完全なVPCアクセスを持つIAMユーザーを作成しますが、要求が自分以外のIPからのものであれば、何も変更できない条件を設定します。
Diag-22. <p>You want to set up a public website on AWS. The things that you require are as follows:</p> <p> </p> <p>- You want the database and the application server running on AWS VPC.</p> <p>- You want the database to be able to connect to the Internet, specifically for any patch upgrades.</p> <p>- You do not want to receive any incoming requests from the Internet to the database.</p> <p> </p> <p>Which of the following solutions would be the best to satisfy all the above requirements for your planned public website on AWS? Choose the correct answer from the options below</p> |  <p>AWSで公開ウェブサイトを設定したいとします。 必要なものは次のとおりです。<br>- AWS VPC上でデータベースとアプリケーションサーバーを実行したい。<br>- 特にパッチのアップグレードのために、データベースをインターネットに接続できるようにする。<br>- インターネットからの着信要求をデータベースに受信する必要はありません。<br>AWS上で予定されているパブリックウェブサイトの上記の要件をすべて満たすには、次のどのソリューションが最適でしょうか？ 下のオプションから正解を選んでください	Set up the public website on a public subnet and set up the database in a private subnet which connects to the Internet via a NAT instance. <br> パブリック・サブネット上にパブリック・ウェブサイトを設定し、NATインスタンスを介してインターネットに接続するプライベート・サブネットにデータベースをセットアップします。 |<p><br></p><p>Option A is incorrect because you need NAT instance or NAT gateway in the public subnet to be able to download the required patches.</p><p>Option B is incorrect because (a) you need NAT instance or NAT gateway to be able to download the required patches, and (b) you cannot allow or deny only inbound traffic via security group as it is stateful.</p><p>Option C is incorrect because you do not need to set up any local data center.</p><p>Option D is CORRECT because you should set up the data server in private subnet as it needs only the traffic from NAT instance or NAT Gateway, and not from the internet.</p><p><br></p><p>For more information on the VPC Scenario for public and private subnets please see the below link</p><p><a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\" target=\"_blank\">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html</a></p>	Set up the database in a public subnet with a security group which only allows inbound traffic. <br>  受信トラフィックのみを許可するセキュリティグループを使用して、パブリックサブネットにデータベースを設定します。	Set up the database in a local data center and use a private gateway to connect the application to the database. <br>  ローカルのデータセンターにデータベースをセットアップし、プライベートゲートウェイを使用してアプリケーションをデータベースに接続します。	Set up the database in a private subnet with a security group which only allows outbound traffic. <br>  プライベートサブネットに、送信トラフィックのみを許可するセキュリティグループを使用してデータベースを設定します。
Diag-23. <p><span><img src="https://s3.amazonaws.com/awssap/d_23_1.png" alt="" width="822" height="602" role="presentation" class="atto_image_button_text-bottom"><br></span></p><p><br></p><p><span id="docs-internal-guid-846a0cce-31cb-81af-86a7-069e9be355eb">Refer to the architecture diagram above of a batch processing solution using Simple Queue Service (SQS) to set up a message queue between EC2 instances which are used as batch processors. CloudWatch monitors the number of job requests (queued messages) and an Auto Scaling group adds or deletes batch servers automatically based on the parameters set in the CloudWatch alarms. You can use this architecture to implement which of the following features in a cost effective and efficient manner? </span><br></p> |  <p> <span> <img src = "https://learning.whizlabs.com/pluginfile.php/1/question/questiontext/1335502/23/28295/SQS.png" alt = "" width = "822" <span id = "docs-internal" height = "602" role = "presentation" class = "atto_image_button_text-bottom"> <br> </ span> </p>シンプルキューサービス（SQS）を使用したバッチ処理ソリューションのアーキテクチャ図を参照して、バッチプロセッサとして使用されるEC2インスタンス間でメッセージキューをセットアップします。 CloudWatchはジョブ要求（キューに入れられたメッセージ）の数を監視し、Auto ScalingグループはCloudWatchアラームで設定されたパラメータに基づいて自動的にバッチサーバーを追加または削除します。このアーキテクチャを使用して、費用効果が高く効率的な方法で次の機能のいずれかを実装できますか？ </p>	sa:	Coordinate number of EC2 instances with number of job requests automatically thus improving cost effectiveness.  <br>  自動的にジョブ要求の数でEC2インスタンスの数を調整し、コスト効率を改善します。|<br><p dir="ltr" style="">Option A is incorrect because the EC2 instances are part of auto scaling group and may be terminated based on the load. So, the messages cannot be passed amongst the instances in daisy-chain setup.</p><p dir="ltr" style="">Option B is incorrect because it is not implementing any fault tolerance against SQS failure by backing up messages to S3.</p><p dir="ltr" style="">Option C is incorrect because the messages cannot be passed amongst the instances as they are part of an auto scaling group and may be terminated based on the load.</p><p dir="ltr" style="">Option D is CORRECT because the EC2 instances are created/terminated by auto scaling group based on the CloudWatch alarm that is triggered based on the threshold set on the number of messages in the SQS queue.</p><p dir="ltr" style="">Option E is incorrect because there are no priority metadata fields in SQS messages.</p></span><br><p><span lang="EN-GB"><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html" target="_blank"></a></span></p>	Implement fault tolerance against EC2 instance failure since messages would remain in SQS and worn can continue with recovery of EC2 instances implement fault tolerance against SQS failure by backing up messages to S3. <br>  メッセージがSQSに残っているためEC2インスタンスの障害に対するフォールトトレランスを実装し、S3にメッセージをバックアップすることでSQSの障害に対するフォールトトレランスを実装するEC2インスタンスの回復を継続できます。	Implement message passing between EC2 instances within a batch by exchanging messages through SQS. <br>  SQSを介してメッセージを交換することによって、バッチ内のEC 2インスタンス間でメッセージを受け渡しを実装します。	Reduce the overall time for executing jobs through parallel processing by allowing a busy EC2 instance that receives a message to pass it to the next instance in a daisy-chain setup. <br>  デイジーチェーン接続のセットアップで次のインスタンスに渡すメッセージを受信しているビジーなEC2インスタンスによる並列処理によるジョブの実行時間を短縮します。	Handle high priority jobs before lower priority jobs by assigning a priority metadata field to SQS messages. <br>  プライオリティメタデータフィールドをSQSメッセージに割り当てることによって、優先度の低いジョブの前に高優先度ジョブを処理します。
Diag-24. <p>You're building a mobile application game. The application needs permissions for each user to communicate and store data in DynamoDB tables. What is the best method for granting each mobile device that installs your application to access DynamoDB tables for storage when required? Choose the correct answer from the options below</p> |  <p>モバイルアプリケーションゲームを構築しています。このアプリケーションには、各ユーザーが通信してDynamoDBテーブルにデータを格納するためのアクセス権が必要です。必要に応じてストレージ用のDynamoDBテーブルにアクセスするためにアプリケーションをインストールする各モバイルデバイスに付与する最も良い方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Create an IAM role with the proper permission policy to communicate with the DynamoDB table. Use web identity federation, which assumes the IAM role using AssumeRoleWithWebIdentity, when the user signs in, granting temporary security credentials using STS.  <br>  適切なアクセス権ポリシーを使用してIAMロールを作成し、DynamoDBテーブルと通信します。 ユーザーがサインインしたときにAssumeRoleWithWebIdentityを使用してIAMロールを想定し、STSを使用して一時的なセキュリティ資格情報を付与するWeb IDフェデレーションを使用します。|<p><br></p><p>Option A is incorrect because IAM Roles are preferred over IAM Users, because IAM Users have to access the AWS resources using access and secret keys, which is a security concern.</p><p>Option B is this is not a feasible configuration.</p><p>Option C is CORRECT because it (a) creates an IAM Role with the needed permissions to connect to DynamoDB, (b) it authenticates the users with Web Identity Federation, and (c) the application accesses the DynamoDB with temporary credentials that are given by STS.</p><p>Option D is incorrect because the step to create the Active Directory (AD) server and using AD for authenticating is unnecessary and costly.</p><p><br></p><p>See the image below for more information on AssumeRoleWithWebIdentity API.</p><p><img src="https://s3.amazonaws.com/awssap/d_24_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><br></p><p><span style="font-size: 1rem;">For more information on web identity federation please refer to the below link</span></p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p>	Create an IAM group that only gives access to your application and to the DynamoDB tables. Then, when writing to DynamoDB, simply include the unique device ID to associate the data with that specific user. <br>  アプリケーションとDynamoDBテーブルにのみアクセスできるIAMグループを作成します。 次に、DynamoDBに書き込むときは、固有のデバイスIDを指定して、その特定のユーザーとデータを関連付けるだけです。	During the install and game configuration process, each user create an IAM credential and assign the IAM user to a group with proper permissions to communicate with DynamoDB. <br>  インストールとゲームの設定プロセス中、各ユーザーはIAM資格を作成し、DynamoDBと通信するための適切な権限を持つグループにIAMユーザーを割り当てます。	Create an Active Directory server and an AD user for each mobile application user. When the user signs in to the AD sign-on, allow the AD server to federate using SAML 2.0 to IAM and assign a role to the AD user which is the assumed with AssumeRoleWithSAML. <br>  モバイルアプリケーションユーザーごとにActive DirectoryサーバーとADユーザーを作成します。 ユーザーがADサインオンにサインインすると、ADサーバーはSAML 2.0を使用してIAMにフェデレートし、AssumeRoleWithSAMLで想定されているADユーザーに役割を割り当てます。
Diag-25. <p>In an attempt to cut costs your accounts manager has come to you and tells you that he thinks that if the company starts to use consolidated billing that it will save some money. He also wants the billing set up in such a way that it is relatively simple, and it gives insights into the environment regarding utilization of resources. Which of the following consolidated billing setups would satisfy your account manager's needs?</p> <p>Choose two answers from the options below</p> |  <p>コストを削減しようとすると、アカウントマネージャーがあなたに来て、会社が統合請求を使用していくと、お金を節約できると考えていると言います。彼はまた、請求のセットアップが比較的単純であるようにしたいと思っており、リソースの利用に関して環境への洞察を与えています。 </p> <p>下記のオプションから2つの回答を選択してください。</p>	ma:	o:Use multiple VPC's to break out environments, tag the resources and use a single account. <br>  複数のVPCを使用して環境を分割し、リソースにタグを付け、単一のアカウントを使用します。|<p>Each organization in AWS Organizations has a <i>master account</i> that pays the charges of all the <i>member accounts</i>. If you have access to the master account, you can see a combined view of the AWS charges that are incurred by the member accounts. <br><a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html" rel="noreferrer">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a><br><br>We can have multiple VPC's serving various departments and we can use tags to define them and can have one billing account. The tags associated with the VPC's will distinguish each department or environment.<br></p><p>There are two main considerations in this scenario: (1) Use of Consolidated Billing offered by AWS, and (2) ability to get the insights into the environment ragarding the utilization of the resources.</p><p>Option A is CORRECT because VPC helps you seggregate and organize your resources as per the functionality or domain, thus enabling the account owner to get the insight of the costing of the resources within the logical grouping of the resources. e.g. If an organization has separate VPC for each department - Finance, Development, Sales etc. It will be convinient to get the billing details per department.</p><p>Option B is incorrect because if all the resources are created under a single account, it will be difficult for the accounts manager to get insights into the utilization of resources as per the domains or functionality. e.g. Instead of having a separate department such as Finance, Development, Sales etc., if an organization has a single account, it will be tedious to get the details on billing of each departmental resource.</p><p>Option C is CORRECT as having linked account would enable the accounts manager to leverage the Consolidated Billing for multiple AWS accounts. With Consolidated Billing, you can see a combined view of AWS charges incurred by all accounts, as well as get a cost report for each account assicuated with your payer account.</p><p>Option D is incorrect because, only IAM Roles will not be sufficient for Consolidated Billing.</p><p><br></p><p>For more information on consolidated billing, please refer to the below link</p><p></p><a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a><br><p></p><p>You can also have the option of segregating the resources via multiple VPC’s and have the billing estimates done via each VPC.</p><p>For more information on AWS VPC please refer to the below link</p><p><a href="https://aws.amazon.com/vpc/" target="_blank">https://aws.amazon.com/vpc/</a></p>	x:Use one master account and no?linked accounts. <br>  1つのマスターアカウントを使用し、アカウントはリンクしない。	o:Use one master account and many member accounts  <br>  1つのマスターアカウントと多数のメンバーアカウントを使用する	x:Use roles for IAM account simplicity across multiple AWS linked accounts.  <br>  複数のAWSリンクアカウントにわたってIAMアカウントのシンプルさに役割を使用する。
Diag-26. <p>The DDoS attacks that happen at the application layer commonly target web applications with lower volumes of traffic compared to infrastructure attacks. To mitigate these types of attacks, you should probably want to include a WAF (Web Application Firewall) as part of your infrastructure. To inspect all HTTP requests, WAFs sit in-line with your application traffic. Unfortunately, this creates a scenario where WAFs can become a point of failure or bottleneck. To mitigate this problem, you need the ability to run multiple WAFs on demand during traffic spikes. This type of scaling for WAF is done via a “WAF sandwich.” Which of the following statements best describes what a “WAF sandwich" is? Choose the correct answer from the options below</p> |  <p>アプリケーション層で発生するDDoS攻撃は、通常、インフラストラクチャ攻撃と比較してトラフィック量の少ないWebアプリケーションをターゲットにしています。この種の攻撃を軽減するには、インフラストラクチャの一部としてWAF（Web Application Firewall）を含めることをお勧めします。すべてのHTTP要求を検査するために、WAFはアプリケーショントラフィックと一直線に並んでいます。残念ながら、これはWAFが障害のポイントまたはボトルネックになるシナリオを作成します。この問題を軽減するには、トラフィックが急増しているときにオンデマンドで複数のWAFを実行する能力が必要です。このタイプのWAFのスケーリングは、「WAFサンドウィッチ」を介して行われます。「WAFサンドウィッチ」の意味を最もよく説明する次の文のうち、以下のオプションから正解を選択してください。</p>	sa:	The EC2 instance running your WAF software is included in an Auto Scaling group and placed in between two Elastic load balancers.  <br>  WAFソフトウェアを実行するEC 2インスタンスは、Auto Scalingグループに含まれ、2台のElasticロードバランサの間に配置されます。|<p><br></p><p>As shown in the "WAF Sandwich" diagram, the WAF EC2 instances are placed in an auto-scaling group, thus it can scale according to the increase in the incoming traffic load. The ELB on the left of the WAF is a public facing ELB which accepts the incoming traffic and sends to the WAF, which inspects and filters the malicious traffic, and forwards the safe traffic to the ELB on its right side - which is an internal ELB. This ELB then distributes the traffic among the EC2 instances for further processing.</p><p><br></p><p>The only correct option is D, because it has the WAF EC2 instance is placed in an autoscaling group and between two ELBs. </p><p> <img src="https://s3.amazonaws.com/awssap/d_26_1.png" alt="" width="837" height="443" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p> </p><p>For more information on a WAF sandwich please refer to the below link</p><p><a href="https://www.cloudaxis.com/2016/11/21/waf-sandwich/" target="_blank">https://www.cloudaxis.com/2016/11/21/waf-sandwich/</a></p>	The EC2 instance running your WAF software is placed between your public subnets and your Internet Gateway. <br>  WAFソフトウェアを実行するEC 2インスタンスは、パブリックサブネットとインターネットゲートウェイの間に格納されます。	The EC2 instance running your WAF software is placed between your public subnets and your private subnets. <br>  WAFソフトウェアを実行しているEC 2インスタンスは、パブリックサブネットとプライベートサブネットの間に格納されます。	The EC2 instance running your WAF software is placed between your private subnets and any NATed connections to the Internet. <br>  WAFソフトウェアを実行するEC 2インスタンスは、プライベートサブネットとインターネットへのNAT接続の間に配置されます。
Diag-27. <p>A company is designing a high availability solution for a customer. This customer requires that their application needs to be able to handle an unexpected amount of load and allow site visitors to read data from a DynamoDB table, which contains the results of an online polling system. At any given time as many as 10,000 requests need to be handled by the application. Given this information, what would be the best and most cost-saving method for architecting and developing this application? Choose the correct answer from the options below</p> |  <p>企業は、顧客のための高可用性ソリューションを設計しています。この顧客は、アプリケーションが予想外の負荷を処理し、サイト訪問者がオンラインポーリングシステムの結果を含むDynamoDBテーブルからデータを読み取ることができるようにする必要があります。何時でも、10,000件のリクエストがアプリケーションによって処理される必要があります。この情報があれば、このアプリケーションのアーキテクチャーと開発のための最高のコスト削減方法は何でしょうか？下記のオプションから正解を選択してください</p>	sa:	Use the JavaScript SDK and build a static HTML page, hosted inside of an Amazon S3 bucket; use CloudFront and Route 53 to serve the website, which uses JavaScript client-side language to communicate with DynamoDB. <br>  CloudFrontとRoute 53を使用して、JavaScriptクライアント側言語を使用してDynamo DBと通信するWebサイトを提供します。|<p><br></p><p>The most important design consideration of this question is to have a highly scalable, cost-saving architecture that provides an application that can communicate with DynamoDB. </p><p><br></p><p>Option A is CORRECT because (a) to show the polling results, a static HTML page that is stored in S3 bucket is sufficient as well as cost-effective, (b) CloudFront and Route53 are AWS managed services that are highly available and scalable, and (c) it uses the JavaScript to communicate with DynamoDB.</p><p>Option B is incorrect because (a)it will require large number of EC2 instances to handle the load of incoming traffic, and (b) setting up the EC2 instances and ELB is not a cost-effective solution compared to static web page in S3.</p><p>Option C is incorrect because architecting this with ELB and EC2 instances will not be as cost effective as the static HTML page - that communicates with DynamoDB - hosted in S3.</p><p>Option D is incorrect as the continuously polling Lambda script will be a costly solution as it charges based on the time the script is run. Also, the limitation on the number of concurrent executions makes the use of AWS Lambda unsuitable for this scenario.</p><p><br></p><p>For more information on AWS s3 please refer to the below link</p><p><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html" target="_blank">http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html</a></p>	Create a CloudFront distribution that serves the HTML web page, but send the visitors to an Auto Scaling ELB application pointing to EC2 instances. <br>  HTML Webページを提供するCloudFrontディストリビューションを作成しますが、訪問者をEC 2インスタンスを指すAuto Scaling ELBアプリケーションに送信します。	Deploy an Auto Scaling application with Elastic Load Balancer pointing to EC2 instances that use a server-side SDK to communicate with the DynamoDB table.  <br>  DynamoDBテーブルと通信するためにサーバー側SDKを使用するEC 2インスタンスを指すElastic Load Balancerを使用して自動スケーリング・アプリケーションをデプロイします。	Create a Lamba script, which pulls the most recent DynamoDB polling results and creates a custom HTML page, inside of Amazon S3 and use CloudFront and Route 53 to serve the static website. <br>  最新のDynamo DBポーリング結果を取得し、Amazon S3内にカスタムHTMLページを作成し、CloudFrontとRoute 53を使用して静的なWebサイトを提供するLambaスクリプトを作成します。
Diag-28. <p>You're migrating an existing application to the AWS cloud. <span>The application will be only communicating with the EC2 instances with in the VPC</span>. This application needs to be built with the highest availability architecture available. The application currently relies on hardcoded hostnames for intercommunication between the three tiers. You've migrated the application and configured the multi-tiers using the internal Elastic Load Balancer for serving the traffic. The load balancer hostname is demo-app.us-east-1.elb.amazonaws.com. The current hard-coded hostname in your application used to communicate between your multi-tier application is demolayer.example.com. What is the best method for architecting this setup to have as much high availability as possible? Choose the correct answer from the options below</p> |  <p>既存のアプリケーションをAWSクラウドに移行しようとしています。 <span>アプリケーションはVPC内のEC2インスタンスとのみ通信します</ span>。このアプリケーションは、利用可能な最高の可用性アーキテクチャで構築する必要があります。アプリケーションは現在、3つの層の相互通信のためにハードコードされたホスト名に依存しています。アプリケーションを移行し、トラフィックを処理するために内部Elastic Load Balancerを使用してマルチティアを設定しました。ロードバランサのホスト名はdemo-app.us-east-1.elb.amazonaws.comです。マルチレイヤアプリケーション間の通信に使用される、アプリケーション内の現在のハードコードされたホスト名は、demolayer.example.comです。できるだけ多くの高可用性を持たせるためにこのセットアップを設計する最良の方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Create a private resource record set using Route 53 with a hostname of demolayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com  <br>  ルート53を使用して、ホスト名がdemolayer.example.com、エイリアスレコードがdemo-app.us-east-1.elb.amazonaws.comのプライベートリソースレコードセットを作成します|<p>Since demolayer.example.com is an internal DNS record, the best way is Route 53 to create an internal resource record. One can then point the resource record to the create ELB.</p><p>While ordinary Amazon Route 53 resource record sets are standard DNS resource record sets, <em>alias resource record sets</em> provide an Amazon Route 53–specific extension to DNS functionality. Instead of an IP address or a domain name, an alias resource record set contains a pointer to a CloudFront distribution, an Elastic Beanstalk environment, an ELB Classic or Application Load Balancer, an Amazon S3 bucket that is configured as a static website, or another Amazon Route 53 resource record set in the same hosted zone. </p><p><br></p><p>Option A is incorrect because it does not mention how the mapping between the existing hard-coded host name and the ELB host name.</p><p>Option B is CORRECT because it creates an internal ALIAS record set where it defines the mapping between the hard-coded host name and the ELB host name that is to be used.</p><p>Option C and D are incorrect because it should create a private record set, not public, since the mapping between the hard-coded host name and ELB host name should be done internally.</p><p><br></p><p>For more information on alias and non-alias records please refer to the below link</p><p><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html" target="_blank">http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p>	Create an environment variable passed to the EC2 instances using user-data with the ELB hostname, demo-app.us-east-1.elb.amazonaws.com. <br>  ELBホスト名demo-app.us-east-1.elb.amazonaws.comを持つユーザーデータを使用して、EC2インスタンスに渡される環境変数を作成します。	Create a public resource record set using Route 53 with a hostname of demolayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com <br>  ホスト名がdemolayer.example.com、エイリアスレコードがdemo-app.us-east-1.elb.amazonaws.comのルート53を使用してパブリックリソースレコードセットを作成する	Add a cname record to the existing on-premise DNS server with a value of demo-app.us-east-1.elb.amazonaws.com. Create a public resource record set using Route 53 with a hostname of applayer.example.com and an alias record to demo-app.us-east-1.elb.amazonaws.com. <br>  demo-app.us-east-1.elb.amazonaws.comの値を使用して、既存のオンプレミスDNSサーバーにcnameレコードを追加します。applayer.example.comのホスト名とdemo-app.us-east-1.elb.amazonaws.comのエイリアスレコードを持つRoute 53を使用してパブリックリソースレコードセットを作成します。
Diag-29. <p>When it comes to KMS, which of the following best describes how the AWS Key Management Service works? Choose the correct answer from the options below</p> |  <p> KMSに関しては、AWS Key Management Serviceの仕組みを次に示すベストのどれですか？下記のオプションから正解を選択してください</p>	sa:	AWS KMS supports two kinds of keys - master keys and data keys. Master keys can be used to directly encrypt and decrypt up to 4 kilobytes of data and can also be used to protect data keys. The data keys are then used to encrypt and decrypt customer data.  <br>  AWS KMSは、マスターキーとデータキーの2種類のキーをサポートしています。 マスターキーを使用して最大4キロバイトのデータを直接暗号化および復号化することができ、またデータキーを保護するために使用することもできます。 データキーは、顧客データの暗号化と復号化に使用されます。|<p><br></p><p>AWS KMS supports two types of keys - Master Keys and Data Keys. A Data Key is used to encrypt and decrypt the actual data; whereas, the Master Key is used to protect (encrypt and decrypt) the data key as well as some data upto 4Kib. See the image below:</p><p><img src="https://s3.amazonaws.com/awssap/d_29_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><span style="font-size: 1rem;">Based on this, option B is CORRECT.</span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">For more information on the AWS KMS Concepts, please refer to the link below:</span></p><p><span style="font-size: 1rem;"><a href="https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html" target="_blank">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html</a></span></p>	AWS KMS supports two kinds of keys - master keys and data keys. Master keys can be used to directly encrypt and decrypt up to 4 kilobytes of data and can also be used to protect data keys. The master keys are then used to encrypt and decrypt customer data. <br>  AWS KMSは、マスターキーとデータキーの2種類のキーをサポートしています。 マスターキーを使用して最大4キロバイトのデータを直接暗号化および復号化することができ、またデータキーを保護するために使用することもできます。 マスターキーは、顧客データの暗号化と復号化に使用されます。	AWS KMS supports two kinds of keys - master keys and data keys. Master keys can be used to directly encrypt and decrypt up to 4 kilobytes of data and can also be used to protect data keys. The data keys are then used to decrypt the customer data, and the master keys are used to encrypt the customer data. <br>  AWS KMSは、マスターキーとデータキーの2種類のキーをサポートしています。 マスターキーを使用して最大4キロバイトのデータを直接暗号化および復号化することができ、またデータキーを保護するために使用することもできます。 次に、データキーを使用して顧客データを復号化し、マスターキーを使用して顧客データを暗号化する。	AWS KMS supports two kinds of keys - master keys and data keys. Master keys can be used to directly encrypt and decrypt up to 4 kilobytes of data and can also be used to protect data keys. The data keys are then used to encrypt the customer data and the master keys are used to decrypt the customer data. <br>  AWS KMSは、マスターキーとデータキーの2種類のキーをサポートしています。 マスターキーを使用して最大4キロバイトのデータを直接暗号化および復号化することができ、またデータキーを保護するために使用することもできます。 次に、データキーを使用して顧客データを暗号化し、マスターキーを使用して顧客データを復号化する。
Diag-30. <p>A company is building an AWS Cloud Environment for a financial regulatory firm. Part of the requirements is being able to monitor all changes in an environment and all traffic sent to and from the environment. What suggestions would you make to ensure all the requirements for monitoring the financial architecture are satisfied?</p> <p>Choose the 2 correct answers from the options below</p> |  <p>企業は金融規制機関向けのAWSクラウド環境を構築しています。要件の一部は、環境内のすべての変更と、環境との間で送受信されるすべてのトラフィックを監視できます。 </p> <p>下記のオプションから2つの正解を選択してください：</p>	ma:	x:Configure an IPS/IDS in promiscuous mode, which will listen to all packet traffic and API changes. <br>  プロミスキャスモードでIPS / IDSを設定します。このモードでは、すべてのパケットトラフィックとAPIの変更を監視します。|<p><br></p><p>Option A and B both are incorrect because promiscuous mode is not supported in AWS.</p><p>Option C is CORRECT because (a) it detects and blocks the malicious traffic coming into and out of VPC, and (b) it also leverages CloudTrail logs and CloudWatch to monitor all the changes in the environment.</p><p>option D is CORRECT because it monitors, filters, and alerts about the potentially hazardous traffic leaving from VPC.</p><p></p><p><br></p><p></p><p>Please find the below developer forums thread on the same.</p><p><a href="https://forums.aws.amazon.com/thread.jspa?threadID=35683" target="_blank">https://forums.aws.amazon.com/thread.jspa?threadID=35683</a></p><p>Please find the below URL to a good slide deck from AWS for getting IDS in place.</p><p><a href="https://awsmedia.s3.amazonaws.com/SEC402.pdf" target="_blank">https://awsmedia.s3.amazonaws.com/SEC402.pdf</a></p>	x:Configure an IPS/IDS system, such as Palo Alto Networks, using promiscous mode that monitors, filters, and alerts of all potential hazard traffic leaving the VPC. <br>  パロアルトネットワークなどのIPS / IDSシステムを、VPCから出るすべての潜在的なハザードトラフィックを監視、フィルタリング、アラートする無差別モードを使用して設定します。	o:Configure an IPS/IDS to listen and block all suspected bad traffic coming into and out of the VPC. Configure CloudTrail with CloudWatch Logs to monitor all changes within an environment.  <br>  IPS / IDSを設定して、VPCとの間で送受信されるすべての疑いのあるトラフィックを監視してブロックします。CloudWatchログを使用してCloudTrailを設定して、環境内のすべての変更を監視します。	o:Configure an IPS/IDS system, such as Palo Alto Networks, that monitors, filters, and alerts of all potential hazard traffic leaving the VPC.  <br>  VPCから出る潜在的な危険トラフィックをすべて監視し、フィルタし、アラートする、パロアルトネットワークなどのIPS / IDSシステムを構成します。
Diag-31. <p>You have acquired a new contract from a client to move all of his existing infrastructures onto AWS. You notice that he is running some of his applications using multicast, and he needs to keep it running as such when it is migrated to AWS. You discover that multicast is not available on AWS, as you cannot manage multiple subnets on a single interface on AWS and a subnet can only belong to one availability zone. Which of the following would enable you to deploy legacy applications on AWS that require multicast? Choose the correct answer from the options below</p> |  <p>既存のインフラストラクチャをすべてAWSに移行するためにクライアントから新しい契約を取得しました。彼はマルチキャストを使用してアプリケーションの一部を実行していることに気がつき、AWSに移行するときにそのアプリケーションをそのまま実行しておく必要があります。 AWS上の1つのインタフェースで複数のサブネットを管理することはできず、サブネットは1つのアベイラビリティゾーンにしか属していないため、AWS上でマルチキャストを使用できないことがわかります。マルチキャストが必要なレガシーアプリケーションをAWSに導入するには、次のうちどれですか？下記のオプションから正解を選択してください</p>	sa:	Create a virtual overlay network that runs on the OS level of the instance.  <br>  インスタンスのOSレベルで実行される仮想オーバーレイネットワークを作成します。|<p><br></p><p>Option A is incorrect because just providing ENIs between the subnets would not resolve the dependency on multicast.</p><p>Option B is CORRECT because overlay multicast is a method of building IP level multicast across a network fabric supporting unicast IP routing, such as Amazon Virtual Private Cloud (Amazon VPC).</p><p>Option C is incorrect because the only option that will work in this scenario is creating a virtual overlay network.</p><p>Option D is incorrect because VPC peering and multicast are not the same.</p><p><br></p><p>For more information on Overlay Multicast in Amazon VPC, please visit the URL below:</p><p><u><a href="https://aws.amazon.com/articles/6234671078671125" target="_blank">https://aws.amazon.com/articles/6234671078671125</a></u></p>	Provide Elastic Network Interfaces between the subnets. <br>  サブネット間の弾性ネットワークインターフェイスを提供します。	All of the answers listed will help in deploying applications that require multicast on AWS. <br>  リストされているすべての回答は、AWSでのマルチキャストが必要なアプリケーションの導入に役立ちます。	Create all the subnets on a different VPC and use VPC peering between them. <br>  異なるVPC上のすべてのサブネットを作成し、それらの間のVPCピアリングを使用します。
Diag-32. <p>A company has three consolidated billing accounts; development, staging, and production. The development account has purchased three reserved instances with instance type of m4.large in Availability Zone us-east-1a. However, no instance is running on the development account, but has five m4.large instances  running in the staging account which is also in Availability Zone 1a. Who can receive the benefit of the reserved instance pricing? Choose the correct answer from the options below</p> |  <p>会社には3つの統合請求口座があります。開発、ステージング、およびプロダクションが含まれます。開発勘定では、可用性ゾーンus-east-1aにインスタンスタイプm4.largeの予約済みインスタンスが3つ購入されています。ただし、開発アカウントで実行されているインスタンスはありませんが、m4.largeインスタンスが5つあります。  可用性ゾーン1aにもあるステージングアカウントで実行されます。予約されたインスタンスの価格設定の恩恵を受けることができるのは誰ですか？下記のオプションから正解を選択してください</p>	sa:	The reserved instance pricing will be applied to the instances in the staging account because the staging account is running an instance that matches the reserved instance type.  <br>  ステージングアカウントが予約されたインスタンスの種類と一致するインスタンスを実行しているため、予約済みインスタンスの価格はステージングアカウントのインスタンスに適用されます。|<p><br></p><p>Option A is incorrect because the benefit of reserved instance pricing will be applicable to any three EC2 instances across all the accounts in the Consolidated Billing group. Since the staging account - which is part of the "account family" - has 5 EC2 instances running, only 3 of those will receive the reserved pricing benefit.</p><p><span style="font-size: 1rem;">Option B is incorrect because even though there are no EC2 instances running in the development account, the instances running in the staging account will still receive the reservation pricing benefit since it is the part of the Consolidated Billing group.</span></p><p><span style="font-size: 1rem;">Option C is CORRECT because the reserved instance pricing be applied to the staging account as it is part of the Consolidated Billing group and only three EC2 instances will be charged with the reserved instance price.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because the reserved Consolidated Billing advantage is applied to all the accounts that are linked to the primary account, not just the primary account.</span></p><p><b style="font-size: 1rem;">More information on Consolidated Billing Group</b></p><p> <img src="https://s3.amazonaws.com/awssap/d_32_1.png" alt="" width="750" height="363" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on consolidating billing please visit the below link</p><p></p><ul><li><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html" target="_blank">http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a></span></li><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html" target="_blank">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html</a></span></li></ul><p></p>	No account will receive the reservation pricing because the reservation was purchased on the development account and no instances that match the reservation are running in the development account. <br>  アカウントは予約を受け取りません開発アカウントで購入し、予約と一致するインスタンスは開発アカウントで実行されていません。	All the instances in all the accounts running the m4.large will receive the pricing even if there is only one reserved instance purchase. <br>  m4.largeを実行しているすべての勘定科目のすべてのインスタンスは、予約されたインスタンスの購入のみがある場合でも価格を受け取ります。	Only the primary account (the consolidated billing primary account) will receive the discounted pricing if the instance is running in the primary billing account. <br>  インスタンスがメインの請求先アカウントで実行されている場合、プライマリアカウント（統合請求メインアカウント）のみが割引価格を受け取ります。
Diag-33. <p>A company has developed a Ruby on Rails content management platform. Currently, OpsWorks with several stacks for dev, staging, and production is being used to deploy and manage the application. Now, the company wants to start using Python instead of Ruby. How should the company manage the new deployment such that it should be able to revert back to the old application with Ruby if the new deployment starts adversely impacting the existing customers? Choose the correct answer from the options below</p> |  <p>ある企業がRuby on Railsコンテンツ管理プラットフォームを開発しました。現在、開発、ステージング、およびプロダクション用の複数のスタックを備えたOpsWorksが、アプリケーションの配備と管理に使用されています。今、同社はRubyの代わりにPythonを使用したいと考えています。新しい展開が既存の顧客に悪影響を及ぼすようになった場合、Rubyを使用して古いアプリケーションに戻すことができるように、新しい展開をどのように管理する必要がありますか？下記のオプションから正解を選択してください</p>	sa:	Create a new stack that contains a new layer with the Python code. Route only a small portion of the production traffic to use the new deployment stack. Once the application is validated, slowly increase the production traffic to the new stack using the Canary Deployment. Revert to the old stack, if the new stack deployment fails or does not work. <br>  Pythonコードで新しいレイヤーを含む新しいスタックを作成します。 新しい展開スタックを使用するために、本番トラフィックのほんの一部のみをルーティングします。 アプリケーションが検証されたら、カナリアンデプロイメントを使用して、新しいスタックにプロダクショントラフィックをゆっくりと増加させます。 新しいスタックの配備が失敗した場合や動作しない場合は、古いスタックに戻してください。|<p><br></p><p>Option A is incorrect because it mentions how the code would be deployed using the deploy lifecycle event, however it does not mention how the system can revert back to the old application deployment stack if there is any failure.</p><p>Option B is CORRECT because it deploys the new stack via the canary deployment method where the new stack is tested only on a small portion production traffic first. If the new deployment has any errors it reverses back to the old deployment stack.</p><p>Option C is incorrect even though create the new stack you should always test it a small portion of production traffic with the new stack rather than routing all the production traffic to it.</p><p>Option D is incorrect because updating all the production instances at once is risky and it does not give an option to revert back to the old stack in case of any errors.</p><p><br></p><p><b>More information on Canary Deployment</b></p><p></p>Canary deployments are a pattern for rolling out releases to a subset of users or servers. The idea is to first deploy the change to a small subset of servers, test it, and then roll the change out to the rest of the servers. The canary deployment serves as an early warning indicator with less impact on downtime: if the canary deployment fails, the rest of the servers aren't impacted.<p></p><p><br></p><ul></ul><p></p>	Create a new stack that contains the Python application code and manages separate deployments of the application via the secondary stack using the deploy lifecycle action to implement the application code. <br>  Pythonアプリケーションコードを含む新しいスタックを作成し、deployライフサイクルアクションを使用してアプリケーションコードを実装するセカンダリスタックを介してアプリケーションの別々のデプロイメントを管理します。	Create a new stack that contains the Python application code. Route all the traffic to the new stack at once so that all the customers get to access the updated application.  <br>  Pythonアプリケーションコードを含む新しいスタックを作成します。 すべてのトラフィックを新しいスタックに一度にルーティングして、すべての顧客が更新されたアプリケーションにアクセスできるようにします。	Update the existing host instances of the application with the new Python code. This will save the cost of having to maintain two stacks, hence cutting down on the costs. <br>  アプリケーションの既存のホストインスタンスを新しいPythonコードで更新します。 これにより、2つのスタックを維持する必要がなくなり、コストが削減されます。
Diag-34. <p>Your application is having a very high traffic, so you have enabled autoscaling in the multi-availability zone to suffice the needs of your application but you observe that one of the availability zones is not receiving any traffic. What can be wrong here?</p> |  <p>アプリケーションのトラフィックが非常に多いため、マルチアベイラビリティゾーンで自動拡張を有効にしてアプリケーションのニーズを満たしていますが、可用性ゾーンの1つにトラフィックがないことがわかります。ここで何が間違っていますか？</p>	sa:	Availability zone is not added to Elastic load balancer  <br>  可用性ゾーンはエラスティックロードバランサに追加されません|<p><br></p><p>In order to make sure that all the EC2 instances behind a cross-zone ELB receive the requests, make sure that all the applicable availability zones (AZs) are added to that ELB.</p><p> </p><p>Option A is incorrect because autoscaling can work with multiple AZs.</p><p>Option B is incorrect because autoscaling can be enabled for multi AZ in any single region, not just N. Virginia. (see the image below)</p><p>Option C is CORRECT because most likely the reason is that the AZ – whose EC2 instances are not receiving requests – is not added to the ELB. </p><p>Option D is incorrect because instances need not be added manually to AZ (they should already be there!).</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/d_34_1.png" alt="" width="637" height="410" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><b>More information on adding AZs to ELB</b></p><p><span>When you add an Availability Zone to your load balancer, Elastic Load Balancing creates a load balancer node in the Availability Zone. Load balancer nodes accept traffic from clients and forward requests to the healthy registered instances in one or more Availability Zones.</span></p><p>For more information on adding AZ’s to ELB, please refer to the below URL<br> <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html" target="_blank">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html</a></p>	Autoscaling can be enabled for multi AZ only in north Virginia region <br>  自動バージョニングは、バージニア州北部の複数のAZでのみ有効にすることができます	Autoscaling only works for single availability zone <br>  オートスケーリングは単一の可用性ゾーンでのみ機能します	Instances need to manually added to availability zone <br>  インスタンスを手動で可用性ゾーンに追加する必要がある
Diag-35. <p>A company has hired a third-party security auditor, and the auditor needs read-only access to all AWS resources and logs of all VPC records and events that will occur on AWS. How can the company meet the auditor's requirements without comprising with the security in the AWS environment? Choose the correct answer from the options below</p> |  <p>ある企業が第三者のセキュリティ監査員を雇っており、AWSで発生するすべてのVPCレコードとイベントのすべてのAWSリソースとログへの読み取り専用アクセスが必要です。 AWS環境のセキュリティを構成することなく、監査人の要求をどのように満たすことができますか？下記のオプションから正解を選択してください</p>	sa:	Create a trail and specify the S3 bucket for your log file delivery.? Create an IAM user who has read only permission to the required AWS resources including the bucket containing CloudTrail logs.  <br>  トレイルを作成し、ログファイル配信用のS3バケットを指定します。CloudTrailログを含むバケットを含む、必要なAWSリソースに対する読み取り専用アクセス権を持つIAMユーザーを作成します。|<p><br></p><p>Option A is incorrect. <span>IAM roles are a secure way to grant permissions to entities that you trust.  But the entities should be an IAM user in another account or an User from a corporate directory who use identity federation with SAML. None of these are specified in the question.</span></p><p>Option B is incorrect because sending the logs via email is not a good architecture.</p><p>Option C is incorrect because granting the auditor access to AWS resources is not AWS's responsibility. It is the AWS user or account owner's responsibility.</p><p>AWS CloudTrail is now enabled <b>by default</b> for <b>ALL CUSTOMERS</b> and will provide visibility into the past seven days of account activity without the need for you to configure a trail in the service to get started. <br>But if you want to access your CloudTrail log files directly or archive your logs for auditing purposes, you can still create a trail and specify the S3 bucket for your log file delivery. <br><br><a href="https://aws.amazon.com/blogs/aws/new-amazon-web-services-extends-cloudtrail-to-all-aws-customers/" rel="noreferrer">https://aws.amazon.com/blogs/aws/new-amazon-web-services-extends-cloudtrail-to-all-aws-customers/</a></p><p><br></p><p><b>More information on AWS CloudTrail</b></p><p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting.</p><p>For more information on CloudTrail, please visit the below URL:</p><p><a href="https://aws.amazon.com/cloudtrail/" target="_blank">https://aws.amazon.com/cloudtrail/</a></p>	Create an SNS notification that sends the CloudTrail log files to the auditor's email when CloudTrail delivers the logs to S3, but do not allow the auditor access to the AWS environment. <br>  CloudTrailがS3にログを配信するが、監査人がAWS環境にアクセスすることを許可していないときに、CloudTrailログファイルを監査人の電子メールに送信するSNS通知を作成する。	The company should contact AWS as part of the shared responsibility model, and AWS will grant required access to the third-party auditor. <br>  会社は共有責任モデルの一環としてAWSに連絡し、AWSは第三者監査人に必要なアクセス権を付与します。	Create a role that has the required permissions for the auditor. <br>  監査人に必要な権限を持つ役割を作成します。
Diag-36. <p>After configuring a whole site CDN on CloudFront you receive the following error: This distribution is not configured to allow the HTTP request method that was used for this request. The distribution supports only cachable requests. What is the most likely cause of this? Choose the correct answer from the options below</p> |  <p> CloudFrontでサイト全体のCDNを設定すると、次のエラーが表示されます。この配信には、このリクエストに使用されたHTTPリクエストメソッドを許可するように設定されていません。このディストリビューションは、キャッシュ可能な要求のみをサポートします。これの最も可能性の高い原因は何ですか？下記のオプションから正解を選択してください</p>	sa:	Allowed HTTP methods on that specific origin is only accepting GET, HEAD <br>  その特定の発信元に対して許可されるHTTPメソッドは、GET、HEAD|<p></p><p>The AWS documentation also states that "CloudFront caches responses to <span style="caret-color: rgb(86, 131, 173); font-size: 12.600000381469727px; background-color: rgba(232, 241, 248, 0.0980392);"><b><i>GET</i></b> </span>and <span style="caret-color: rgb(86, 131, 173); font-size: 12.600000381469727px; background-color: rgba(232, 241, 248, 0.0980392);"><b><i>HEAD</i></b></span> requests" and, optionally, OPTIONS requests. CloudFront <b>does not cache responses to requests that use the other methods.</b><br><br>As per AWS documentation,</p><p>Allowed HTTP Methods</p><p><span style="font-size: 1rem;">Specify the HTTP methods that you want CloudFront to process and forward to your origin:</span></p><ul dir="auto"><li><p><b>GET, HEAD:</b> You can use CloudFront only to get objects from your origin or to get object headers.</p></li><li><p><b>GET, HEAD, OPTIONS:</b> You can use CloudFront only to get objects from your origin, get object headers, or retrieve a list of the options that your origin server supports.</p></li><li><p><b>GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE:</b> You can use CloudFront to get, add, update, and delete objects, and to get object headers. In addition, you can perform other POST operations such as submitting data from a web form.</p><p>Note</p><p><span><span>CloudFront caches responses to </span><span style="caret-color: rgb(86, 131, 173); font-size: 12.600000381469727px; background-color: rgba(232, 241, 248, 0.0980392);"><i><b>GET</b></i> </span><span>and </span><span style="caret-color: rgb(86, 131, 173); font-size: 12.600000381469727px; background-color: rgba(232, 241, 248, 0.0980392);"><span><i><b>HEAD</b> </i></span></span><span>requests and, optionally, <span style="caret-color: rgb(86, 131, 173); font-size: 12.600000381469727px; background-color: rgba(232, 241, 248, 0.0980392);"><b><i>OPTIONS</i></b></span></span><span> requests. CloudFront does not cache responses to requests that use the other methods.</span></span></p></li></ul><p><br>Based on this, Option B seems to be a better choice than C.<br><br>For more information please visit:<br></p><ul><li><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesAllowedHTTPMethods" target="_blank" rel="noreferrer" style="font-size: 1rem;">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesAllowedHTTPMethods</a></li></ul><p><br></p><p></p><p></p><p>When the CloudFront Distribution supports only "cacheable requests", it means that it supports only GET and HEAD HTTP requests (read only). For the HTTP requests such as OPTIONS, PUT, POST, PATCH AND DELETE, the CloudFront will give an error "The distribution supports only cacheable requests".</p><p>Hence, option B is the correct answer.</p><p>There is a good question posted on StackOverflow which explains what should be done in this situation.</p><p><a href="http://stackoverflow.com/questions/31253694/this-distribution-is-not-configured-to-allow-the-http-request" target="_blank">http://stackoverflow.com/questions/31253694/this-distribution-is-not-configured-to-allow-the-http-request</a></p>	The CloudFront distribution is configured to the wrong origin <br>  CloudFrontディストリビューションが間違った起点に設定されています	Allowed HTTP methods on that specific origin is only accepting GET, HEAD, OPTIONS <br>  その特定の発信元に対して許可されるHTTPメソッドは、GET、HEAD、OPTIONSのみを受け入れます	Allowed HTTP methods on that specific origin is only accepting GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE  <br>  その特定のHTTPメソッドは、GET、HEAD、OPTIONS、PUT、POST、PATCH、DELETEを受け付けるだけです
Diag-37. <p>You're running a financial application on an EC2 instance. Data is stored in the instance is critical and in the event of a failure of an EBS volume, the RTO and RPO are less than 1 minute. How would you architect this application given the RTO and RPO requirements? Choose the correct answer from the options below</p> |  <p> EC2インスタンスで金融アプリケーションを実行しています。インスタンスに格納されるデータは重要であり、EBSボリュームに障害が発生した場合、RTOとRPOは1分未満です。 RTOとRPOの要件を考慮して、このアプリケーションをどのように設計しますか？下記のオプションから正解を選択してください</p>	sa:	Mirror the data using RAID 1 configuration, which provides fault tolerance on EBS volumes. <br>  EBSボリュームのフォールトトレランス機能を提供するRAID 1構成を使用してデータをミラー化します。|<p><br></p><p>To meet the requirement of RTO and RPO less than 1 minute, the best way is to have a configuration where the data is backed up on another EBS volumes. In case of failure, the backup EBS volumes can be used and not data would be lost. </p><p><br></p><p>Option A is CORRECT because, as mentioned above, RAID 1 configuration maintains the exact copy of the data (via mirroring) in a backup EBS volume which can be used in case of the failure of the main volume, providing redundancy and fault tolerance. In case of failure, the old EBS volume can quickly be replaced with the backup volume and the RTO and RPO requirement can be met within a minute.</p><p>Option B is incorrect because, although each Amazon EBS volume is automatically replicated within its Availability Zone, it is done so to protect it from any component failure from AWS side. It does not withstand any failures at user level. It is user's responsibility to replicate the data that is stored on the EBS volume.</p><p>Option C is incorrect because automated snapshots every minute will not meet this RTO and RPO requirement.</p><p>Option D is incorrect because RAID 0 configuration helps improve the performance, but does not provide redundancy or mirroring of the data across disks. As a result of having data striped across all disks, any failure will result in total data loss. </p><p><br></p><p><b>More information on RAID Configurations with EBS volumes:</b></p><p>As per the AWS documentation, it is clearly given to use RAID 1 configuration for fault tolerance of EBS volumes.</p><p> <img src="https://s3.amazonaws.com/awssap/d_37_1.png" alt="" width="743" height="122" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on RAID configuration, please visit the below URL</p><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>	Nothing is required since EBS volumes are durability backed up to additional hardware in the same availability zone. <br>  EBSボリュームは、同じアベイラビリティゾーンで追加のハードウェアにバックアップされているため、何も必要ありません。	Write a script to create automated snapshots of the EBS volumes every minute. In the event of failure have an automated script that detects failure and launches a new volume from the most recent snapshot.  <br>  毎分EBSボリュームの自動スナップショットを作成するスクリプトを作成します。自動化されたスクリプトに障害が検出された場合、最新のスナップショットから新しいボリュームを起動します。	Stripe multiple EBS volumes together with RAID 0, which provides fault tolerance on EBS volumes. <br>  複数のEBSボリュームをRAID 0とともにストライプ化し、EBSボリュームのフォールトトレランス機能を提供します。
Diag-38. <p>A company is running a MySQL RDS instance inside AWS; however, a new requirement for disaster recovery is keeping a read replica of the production RDS instance in an on-premise data center. What is the securest way of performing this replication? Choose the correct answer from the options below</p> |  <p>企業はAWS内でMySQL RDSインスタンスを実行しています。ただし、災害復旧の新たな要件として、運用中のRDSインスタンスの読み取りレプリカを社内のデータセンターに保持することがあります。このレプリケーションを実行する安全な方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Create an IPSec VPN connection using VPN/VGW through the Virtual Private Cloud service. <br>  Virtual Private Cloudサービスを通じてVPN / VGWを使用してIPSec VPN接続を作成します。|<p><br></p><p>Option A is incorrect because SSL endpoint cannot be used here as it is used for securely accessing the database.</p><p>Option B is incorrect because replicating via EC2 instances is very time consuming and very expensive cost-wise.</p><p>Option C is incorrect because Data Pipeline is for batch jobs and not suitable for this scenario.</p><p>Option D is CORRECT because it is feasible to setup the secure IPSec VPN connection between the on premise server and AWS VPC using the VPN/Gateways.</p><p>See the image below:</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/d_38_1.png" alt="" width="673" height="375" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p></p><div>For more information on VPN connections , please visit the below URL:<br></div><div><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank">http://docs.aws.amazon.com/<wbr>AmazonVPC/latest/UserGuide/<wbr>VPC_VPN.html</a></div><br><p></p>	RDS cannot replicate to an on-premise database server. Instead, first configure the RDS instance to replicate to an EC2 instance with core MySQL, and then configure replication over a secure VPN/VPG connection. <br>  RDSはオンプレミスデータベースサーバーに複製できません。代わりに、コアMySQLを使用してEC 2インスタンスにレプリケートするようにRDSインスタンスを構成し、セキュアなVPN / VPG接続を介してレプリケーションを構成します。	Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint.  <br>  毎晩MySQLデータをエクスポートし、HTTPSエンドポイントからS3を安全にダウンロードするデータパイプラインを作成します。	Configure the RDS instance as the master and enable replication over the open internet using a secure SSL endpoint to the on-premise server. <br>  RDSインスタンスをマスターとして構成し、セキュアSSLエンドポイントを使用してオンプレミスサーバーにインターネット経由でレプリケーションを有効にします。
Diag-39. <p>You are setting up a video streaming service with the main components of the set up being S3, CloudFront, and Transcoder. Your video content will be stored on AWS S3 and it should only be viewed by the subscribers who have paid for the service. Your first job is to upload 10 videos to S3 and make sure they are secure before you even begin to start thinking of streaming the videos. The 10 videos have just finished uploading to S3, so you now need to secure them with encryption at rest. Which of the following would be the best way to do this? Choose the correct answer from the options below:</p> |  <p>設定の主なコンポーネントをS3、CloudFront、およびTranscoderとするビデオストリーミングサービスを設定しています。動画コンテンツはAWS S3に保存され、サービスの費用を支払った加入者のみが視聴する必要があります。あなたの最初の仕事は、ビデオをストリーミングすることを考え始める前に、10個のビデオをS3にアップロードし、それらが安全であることを確認することです。 10個のビデオはS3へのアップロードが完了したので、残りの部分を暗号化して保護する必要があります。次のうちどれを実行するのが最適な方法ですか？下記のオプションから正解を選んでください：</p>	sa:	Use KMS to encrypt source data and decrypt resulting output. Also, use Origin Access Identity on your CloudFront distribution, so the content is only able to be served via CloudFront, not S3 URLs.  <br>  KMSを使用してソースデータを暗号化し、結果の出力を復号化します。 また、CloudFrontディストリビューションでOrigin Access Identityを使用すると、コンテンツはS3 URLではなくCloudFrontを介してのみ配信できます。|<p><br></p><p>There are two main considerations in this scenario: (1) the data in S3 should be encrypted "at rest", and (2) only the authenticated users should be able to stream the video.</p><p><br></p><p>Option A is incorrect because because AWS CloudHSM is used for generating the user's own encryption keys on the AWS Cloud. It does not encrypt any data on S3 at rest.</p><p>Option B is incorrect because, even though it encrypts the data at rest, storing the keys in the CloudFront for private access to the authenticated users is an invalid solution.</p><p>Option C is incorrect because there is no checkbox in AWS Console to apply the encryption on S3 data. You need to apply the appropriate policy to the bucket if you require server-side encryption for all objects in it.</p><p>Option D is CORRECT because, (a) it uses KMS for encrypting and decrypting the data, and (b) it ensures that only the authenticated users will have access to the videos by using the Origin Access Identity (OAI) on the CloudFront distribution and disabling the access via S3 URLs.</p><p><br></p><p>Below is a good link for how to use either SSE S3 or KMS encryption</p><p><a href="https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/#more-1038" target="_blank">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/#more-1038</a></p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a><br></p>	Encrypt your data using AES-256. After the object is encrypted, the encryption key you used needs to be stored on AWS CloudFront so that only authenticated users can stream the videos. <br>  オブジェクトを暗号化したら、使用した暗号化キーをAWS CloudFrontに保存して、認証されたユーザーだけがビデオをストリーミングできるようにする必要があります。	Set an API flag, or check a box in the AWS Management Console, to have data encrypted in Amazon S3. Create IAM Users to access the videos from S3. <br>  AWS管理コンソールでAPIフラグを設定するか、チェックボックスをオンにして、Amazon S3でデータを暗号化します。S3からのビデオにアクセスするためのIAMユーザーを作成します。	Use AWS CloudHSM appliance with both physical and logical tamper detection and response mechanisms that trigger zeroization of the appliance. <br>  アプライアンスのゼロ化をトリガーする物理的および論理的な改ざん検出および応答メカニズムの両方を備えたAWS CloudHSMアプライアンスを使用します。
Diag-40. <p>You currently have 9 EC2 instances running in a Placement Group. All these nine instances were initially launched at the same time and seemed to be performing as expected. You decide that you need to add two new instances to the group; however, when you attempt to do this you receive a 'capacity error.' Which of the following actions will most likely fix this problem? Choose the correct answer from the options below</p> |  <p>現在、9個のEC2インスタンスがプレースメントグループで実行されています。これらの9つのインスタンスはすべて、最初は同時に起動され、期待どおりに実行されていたようです。グループに2つの新しいインスタンスを追加する必要があると決めました。ただし、これを実行しようとすると、「容量エラー」が表示されます。この問題を解決する可能性が最も高いのは次のうちどれですか？下記のオプションから正解を選択してください</p>	sa:	Stop and restart the instances in the Placement Group and then try the launch again. <br>  プレースメントグループ内のインスタンスを停止してから再起動し、再度起動してください。|<p><br></p><p>Option A is incorrect because to benefit from the enhanced networking, all the instances should be in the same Placement Group. Launching the new ones in a new Placement Group will not work in this case.</p><p>Option B is CORRECT because the most likely reason for the "Capacity Error" is that the underlying hardware may not have the capacity to launch any additional instances on it. If the instances are stopped and restarted, AWS may move the instances to a hardware that has capacity for all the requested instances.</p><p>Option C is incorrect because there is no such limit on the number of instances in a Placement Group (however, you can not exceed your EC2 instance limit allocated to your account per region).</p><p>Option D is incorrect because the capacity error is not related to the instance size and just ensuring that the instances are of same size will not resolve the capacity error.</p><p><br></p><p><b>More information on Cluster Placement Group</b></p><p> If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has the capacity for all the requested instances.</p><p><img src="https://s3.amazonaws.com/awssap/d_40_1.png" alt="" width="743" height="488" role="presentation" class="img-responsive atto_image_button_text-bottom"><br><br>For more information on this, please refer to the below URL<br><br><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>	Make a new Placement Group and launch the new instances in the new group. Make sure the Placement Groups are in the same subnet. <br>  新しいプレースメントグループを作成し、新しいグループに新しいインスタンスを起動します。プレースメントグループが同じサブネットにあることを確認します。	Request a capacity increase from AWS as you are initially limited to 10 instances per Placement Group.  <br>  最初にプレースメントグループごとに10インスタンスに制限されているため、AWSから容量を増やすことをリクエストしてください。	Make sure all the instances are the same size and then try the launch again. <br>  すべてのインスタンスが同じサイズであることを確認してから、再度起動してください。
Diag-41. <p>A company has two batch processing applications that consume financial data about the day's stock transactions. Each transaction needs to be stored durably and guarantee that a record of each application is delivered so the audit and billing batch processing applications can process the data. However, the two applications run separately and several hours apart and need access to the same transaction information in a serial order. After reviewing the transaction information for the day, the information no longer needs to be stored. What is the best way to architect this application? Choose the correct answer from the options below</p> |  <p>会社には、1日の株式取引に関する財務データを消費する2つのバッチ処理アプリケーションがあります。各トランザクションは永続的に保管し、各アプリケーションのレコードが配信されるようにして、監査および請求処理アプリケーションがデータを処理できるようにする必要があります。ただし、2つのアプリケーションは別々に数時間離れて実行され、同じトランザクション情報に連続してアクセスする必要があります。その日の取引情報を見直した後、その情報はもはや格納する必要はない。このアプリケーションを設計する最良の方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Use Kinesis to store the transaction information. The billing application will consume data from the stream, the audit application can consume the same data several hours later. <br>  取引情報を保存するには、Kinesisを使用します。 課金アプリケーションはストリームからデータを消費し、監査アプリケーションは数時間後に同じデータを消費する可能性があります。|<p><br></p><p>The main architectural considerations in this scenario are: (1) each transaction needs to be stored durably (no loss of any transaction), (2)  they should be processed in serial order, (3) guaranteed delivery of each record to the audit and billing batch processing, and (4) the processing of the record by two application is done with a time gap of several hours.</p><p><span style="font-size: 1rem;">Based on the considerations above, it seems that we must use Amazon Kinesis Data Streams which enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</span></p><p><br></p><p>Option A is incorrect because (a) the onus of ensuring that each message is copied to the audit queue is on the application, and (b) this option is not fail-proof. i.e. If the application fails to copy the message between the queue, there is no logic to put the message back to the billing queue.</p><p>Option B is incorrect because, even though it uses SQS, there is an overhead of ensuring that the message is put back into the same queue. Also, there is a possibility of processing the same record (message) multiple time by the instances processing it (there is no way to know if the record has been already processed).</p><p>Option C is incorrect because it adds the overhead of delivery guarantee on the application itself. Also, the use of DynamoDB in this scenario is not a cost effective solution.</p><p>Option D is CORRECT because Amazon Kinesis is best suited for applocations which needs to process large real-time transactional records and have the ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.</p><p><br></p><p><b>More information on </b><b style="font-size: 1rem;"><b>when Amazon Kinesis Data Streams and Amazon SQS should be used:</b></b></p><div><div><p>AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:</p><ul><li>Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.</li><li>Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.</li><li>Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</li><li>Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.</li></ul><p>AWS recommends Amazon SQS for use cases with requirements that are similar to the following:</p><ul><li>Messaging semantics (such as message-level ack/fail) and visibility timeout. For example, you have a queue of work items and want to track the successful completion of each item independently. Amazon SQS tracks the ack/fail, so the application does not have to maintain a persistent checkpoint/cursor. Amazon SQS will delete acked messages and redeliver failed messages after a configured visibility timeout.</li><li>Individual message delay. For example, you have a job queue and need to schedule individual jobs with a delay. With Amazon SQS, you can configure individual messages to have a delay of up to 15 minutes.</li><li>Dynamically increasing concurrency/throughput at read time. For example, you have a work queue and want to add more readers until the backlog is cleared. With Amazon Kinesis Data Streams, you can scale up to a sufficient number of shards (note, however, that you'll need to provision enough shards ahead of time).</li><li>Leveraging Amazon SQS’s ability to scale transparently. For example, you buffer requests and the load changes as a result of occasional load spikes or the natural growth of your business. Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.<br></li></ul></div></div><div></div><span><a href="https://aws.amazon.com/kinesis/data-streams/faqs/" target="_blank" style="">https://aws.amazon.com/kinesis/data-streams/faqs/</a><br><br></span><p></p>	Use SQS for storing the transaction messages; when the billing batch process performs first and consumes the message, write the code in a way that does not remove the message after consumed, so it is available for the audit application several hours later. The audit application can consume the SQS message and remove it from the queue when completed. <br>  トランザクションメッセージを格納するにはSQSを使用します。 請求処理プロセスが最初に実行してメッセージを消費すると、消費後にメッセージを削除しない方法でコードを書き込むので、数時間後に監査アプリケーションで使用できます。 監査アプリケーションは、SQSメッセージを消費し、完了したらキューから削除することができます。	Store the transaction information in a DynamoDB table. The billing application can read the rows while the audit application will read the rows them remove the data.  <br>  トランザクション情報をDynamoDBテーブルに格納します。課金アプリケーションは行を読み取ることができますが、監査アプリケーションはデータを削除する行を読み取ります。	Use SQS for storing the transaction messages. When the billing batch process consumes each message, have the application create an identical message and place it in a different SQS for the audit application to use several hours later. <br>  トランザクションメッセージの格納にはSQSを使用します。 請求処理プロセスが各メッセージを消費するとき、アプリケーションに同じメッセージを作成させ、監査アプリケーションが数時間後に使用するために別のSQSに配置します。
Diag-42. <p>A company is running data application on-premise that requires large amounts of data to be transferred to a VPC containing EC2 instances in an AWS region. The company is concerned about the total overall transfer costs required for this application and is potentially not going deploy a hybrid environment for the customer-facing part of the application to run in a VPC. Given that the data transferred to AWS is new data every time, what suggestions could you make to the company to help reduce the overall cost of data transfer to AWS? Choose the correct answer from the options below</p> |  <p>企業は、AWS領域のEC2インスタンスを含むVPCに大量のデータを転送する必要があるデータアプリケーションを社内で実行しています。 同社は、このアプリケーションに必要な総転送コストを懸念しており、VPCで稼動するアプリケーションの顧客対応部分にハイブリッド環境を導入することはない可能性があります。 AWSに転送されるデータが毎回新しいデータであることを考えると、AWSへのデータ転送の総コストを削減するために、どのような提案を企業に提出できますか？ 下のオプションから正解を選んでください</p>	sa:	Suggest provisioning a Direct Connect connection between the on-premise data center and the AWS region. <br>  オンプレミスデータセンターとAWSリージョン間のダイレクトコネクト接続のプロビジョニングを提案する。|<p><br></p><p>In this question, the customer wants to transfer large amount of data to VPC from the on-premises data center. The main architectural considerations are (1) the cost should be low, and (2) the data being transferred is new data every time.</p><p><br></p><p>Option A is incorrect because, although setting up a VPN is a cost effective solution, it may not have sufficient bandwidth for the data being trasferred, especially since the data is new every time. Also,the data will be transferred over the internet. So, the new data adds to the unpredictability of the performance that the VPN connection would yield. So, the VPN connection may stay much longer than expected adding to the overall cost.</p><p>Option B is CORRECT because (a) since it is a dedicated connection from on-premises data center to AWS, it takes out the unpredictable nature of the internet out of the equation, and (2) due to the high bandwidth availability, Direct Connect would most probably transfer the large amount of data quickly compared to VPN connection. Hence, it may well save the cost for the customer.</p><p>Option C is incorrect because AWS Import/Export is not an ideal option since the data being transferred is new every time, since Import/Export is preferred mainly for first time data migration and using VPN/Direct Connect later on. </p><p>Option D is incorrect because it is the requirement that the data must be transferred to AWS, hence ruling out the option of leaving the data on-premise.</p><p><br></p><p>For more information on AWS direct connect, just browse to the below URL</p><p><a href="https://aws.amazon.com/directconnect/" target="_blank">https://aws.amazon.com/directconnect/</a></p><p><br></p><p><b>Note: </b>While I agree that the Direct Connect is costly, but compared to other options given, it is certainly the most viable. With the dedicated network connectivity and higher bandwidth, the data transfer would get done quicker compared to over the internet. With Direct Connect, the initial setup cost would be more. But in the long run, it would be the most suitable solution even with regards to keeping the cost low.</p>	Provision a VPN connection between the on-premise data center and the AWS region using the VPN section of a VPC. <br>  VPCのVPNセクションを使用して、オンプレミスデータセンターとAWSリージョン間のVPN接続をプロビジョニングします。	Suggest using AWS import/export to transfer the TBs of data while synchronizing the new data as it arrives. <br>  新しいデータが到着したときに同期しながら、データのTBを転送するには、AWSのインポート/エクスポートを使用することをお勧めします。	Suggest leaving the data required for the application on-premise and use a VPN to query the on-premise database data from EC2 when required.  <br>  オンプレミスでアプリケーションに必要なデータを残し、必要に応じてVPNを使用してEC 2からのオンプレミスデータベースデータを照会することを推奨します。
Diag-43. <p>An application has multiple components. The single application and all the components are hosted on a single EC2 instance (without an ELB) in a VPC. You have been told that this needs to be set up with two separate SSLs for each component. Which of the following would best achieve the setting up of the two separate SSLs while using still only using one EC2 instance? Choose the correct answer from the options below</p> |  <p>アプリケーションには複数のコンポーネントがあります。単一のアプリケーションとすべてのコンポーネントは、VPC内の単一のEC2インスタンス（ELBなし）でホストされます。これは、各コンポーネントに対して2つの別々のSSLを使用して設定する必要があると言われています。 EC2インスタンスを1つだけ使用しているときに、2つの別々のSSLの設定を最も効果的に実行できるのは次のうちどれですか？下記のオプションから正解を選択してください</p>	sa:	Create an EC2 instance which has multiple network interfaces with multiple elastic IP addresses. <br>  複数の弾性IPアドレスを持つ複数のネットワークインターフェイスを持つEC 2インスタンスを作成します。|<p><br></p><p>It can be useful to assign multiple IP addresses to an instance in your VPC to do the following:</p><p><br></p><p>(1) Host multiple websites on a single server by using multiple SSL certificates on a single server and associating each certificate with a specific IP address.</p><p><span style="font-size: 1rem;">(2) Operate network appliances, such as firewalls or load balancers, that have multiple IP addresses for each network interface.</span></p><p><span style="font-size: 1rem;">(3) Redirect internal traffic to a standby instance in case your instance fails, by reassigning the secondary IP address to the standby instance.</span></p><p><br></p><p>Option A is CORRECT because, as mentioned above, if you have multiple elastic network interfaces (ENIs) attached to the EC2 instance, each network IP can have a component running with a separate SSL certificate.</p><p>Option B is incorrect because having separate rules in security group as well as NACL does not mean that the instance supports multiple SSLs.</p><p>Option C is incorrect because an EC2 instance cannot have multiple subnets.</p><p>Option D is incorrect because NAT address is not related to supporting multiple SSLs.</p><p><br></p><p>For more information on Multiple IP Addresses, please refer to the link below:</p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html" target="_blank">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html</a></p>	Create an EC2 instance which has both an ACL and the security group attached to it and have separate rules for each IP address. <br>  ACLとそれに接続されたセキュリティグループの両方を持つEC 2インスタンスを作成し、各IPアドレスに対して個別のルールを設定します。	Create an EC2 instance which has multiple subnets attached to it and each will have a separate IP address.  <br>  複数のサブネットが接続されたEC 2インスタンスを作成し、それぞれに別々のIPアドレスを割り当てます。	Create an EC2 instance with a NAT address. <br>  NATアドレスを持つEC 2インスタンスを作成します。
Diag-44. <p>You are working as a consultant for a company designing a new hybrid architecture to manage part of their application infrastructure in the cloud and on-premise. As part of the infrastructure, they need to consistently transfer high amounts of data. They require a low latency and high consistency traffic to AWS. The company is looking to keep costs as low possible and is willing to accept slow traffic in the event of primary failure. Given these requirements how would you design a hybrid architecture? Choose the correct answer from the options below</p> |  <p>クラウドとオンプレミスのアプリケーションインフラストラクチャの一部を管理する新しいハイブリッドアーキテクチャを設計している企業のコンサルタントとして働いています。インフラストラクチャの一環として、大量のデータを一貫して転送する必要があります。 AWSへの低遅延で整合性の高いトラフィックが必要です。同社は、コストを可能な限り低く抑えようとしており、主な障害が発生した場合に低速トラフィックを受け入れる意思がある。これらの要件を前提に、ハイブリッドアーキテクチャをどのように設計しますか？下記のオプションから正解を選択してください</p>	sa:	Provision a Direct Connect connection to an AWS region using a Direct Connect partner. Provision a VPN connection as a backup in the event of Direct Connect connection failure.  <br>  直接接続パートナーを使用して、Provisiona Direct ConnectをAWSリージョンに接続します。ダイレクトコネクトの接続に失敗した場合のバックアップとしてVPN接続をプロビジョニングします。|<p><br></p><p>AWS Direct Connect makes it easy to establish a dedicated network connection from your on-premises data center to AWS. i.e. You can establish private connectivity between AWS and the on-premises data center, which helps to reduce the overall network cost, increase bandwidth throughput, and provide more consistent network experience than the internet based connection. </p><p>A VPN connection is a low-cost, an appliance based access to the AWS resources that is given to the on-premises resources via gateways over the internet. Compared to AWS Direct Connect, the VPN connection may experience slow connection speed and limited bandwidth due to unpredictability of the internet.</p><p><span style="font-size: 1rem;">Since cost is a factor for the backup and the company does not mind the reduced traffic, the backup option can a VPN connection instead of another direct connect solution.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>Option A is CORRECT because it sets up a Direct Connect as the primary connection which provides consistent bandwidth for transferring high amounts of data, and in case of failure, sets up a VPN which is a low-cost solution.</p><p>Option B is incorrect because VPN (although set up as dual) does not provide low latency connection as it still has network unpredictability, and consistency as the Direct Connect would do.</p><p>Option C is incorrect because there is no automatic failover or redundancy in Direct Connect.</p><p><span style=""></span></p><p>Option D is incorrect because setting up two Direct Connect connections would be costly.</p><p><br></p><p>For more information on AWS direct connect, just browse to the below URL</p><p><a href="https://aws.amazon.com/directconnect/" target="_blank">https://aws.amazon.com/directconnect/</a></p>	Create a dual VPN tunnel for private connectivity, which increases network consistency and reduces latency. The dual tunnel provides a backup VPN in the case of primary failover. <br>  プライベート接続用にデュアルVPNトンネルを作成することで、ネットワークの整合性が向上し、レイテンシが短縮されます。デュアルトンネルは、プライマリフェールオーバーの場合にバックアップVPNを提供します。	Provision a Direct Connect connection which has automatic failover and backup built into the service. <br>  自動フェイルオーバーとバックアップがサービスに組み込まれているダイレクトコネクト接続をプロビジョニングします。	Provision a Direct Connect connection to an AWS region using a Direct Connect provider. Provision a secondary Direct Connect connection as a failover. <br>  ダイレクトコネクトプロバイダを使用して、AWSリージョンへのダイレクトコネクト接続をプロビジョニングします。セカンダリダイレクトコネクト接続をフェイルオーバーとしてプロビジョニングします。
Diag-45. <p>You have a massive social networking application which is already deployed on N.Virginia region with around 100 EC2 instances, you want to deploy your application to multiple regions for better availability. You don’t want to handle multiple key pairs and want to reuse existing key pairs for N.Virginia region. How will you accomplish this?</p> |  <p>約100のEC2インスタンスを持つN.Virginia地域にすでに展開されている大規模なソーシャルネットワーキングアプリケーションを使用している場合は、アプリケーションを複数の地域に展開して可用性を高める必要があります。複数の鍵ペアを処理する必要はなく、N.Virginia領域の既存の鍵ペアを再利用する必要があります。これをどのように達成しますか？</p>	sa:	Using import key-pair feature using AWS web console <br>  AWS Webコンソールを使用したインポートキーペア機能の使用|<p><br></p><p>Key pairs across regions is not possible. In order to use key pairs across regions you need to import the key pairs in the respective regions.</p><p>You need to go the respective region and from the EC2 dashboard , click on Import Key pair and choose the relevant key pair.</p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/d_45_1.png" alt="" width="640" height="183" role="presentation" class="img-responsive atto_image_button_middle"><br></span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Option A is incorrect because key pair is region specific – not global.</span></p><p>Option B is incorrect because keys cannot be copied across different regions, they need to be imported.</p><p>Option C is CORRECT because import key pair functionality enables migrating an EC2 instance from one region to another and use the same PEM key. </p><p>Option D is incorrect because PEM keys cannot be copied to another region as part of the AMI.</p><p><br></p><p>For more information on bringing your own key pair, please refer to the below URL<br> <a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-feature-bring-your-own-keypair/" target="_blank">https://aws.amazon.com/blogs/aws/new-amazon-ec2-feature-bring-your-own-keypair/</a></p>	Use copy key command line API to transfer key to different regions <br>  コピーキーのコマンドラインAPIを使用して、異なる地域にキーを転送する	Key pair is not a region level concept, all the keys are available globally  <br>  キーペアはリージョンレベルの概念ではなく、すべてのキーがグローバルに使用可能です	Copy AMI of your EC2 machine between regions and start an instance from that AMI <br>  リージョン間でEC 2マシンのAMIをコピーし、そのAMIからインスタンスを開始する
Diag-46. <p>A third party auditor is being brought in to review security processes and configurations for all of a company's AWS accounts. Currently, the company does not use any on-premise identity provider. Instead, they rely on IAM accounts in each of their AWS accounts. The auditor needs read-only access to all AWS resources for each AWS account.Given the requirements, what is the best security method for architecting access for the security auditor? Choose the correct answer from the options below</p> |  <p>サードパーティの監査員が、企業のAWSアカウントのすべてのセキュリティプロセスと設定を確認するために持ち込まれています。現在、同社は社内のIDプロバイダを使用していません。代わりに、各AWSアカウントのIAMアカウントに依存しています。監査人は、各AWSアカウントのすべてのAWSリソースへの読み取り専用アクセスが必要です。要件を満たしていれば、セキュリティ監査員のアクセスを設計するための最良のセキュリティ方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Create an IAM role with read-only permissions to all AWS services in each AWS account. Create one auditor IAM account and add a permissions policy that allows the auditor to assume the ARN role for each AWS account that has an assigned role. <br>  各AWSアカウントでIAMロールを作成します。1つの監査人IAMアカウントを作成し、割り当てられた役割を持つ各AWSアカウントに対して監査人がARNロールを引き受けることを許可する許可ポリシーを追加します。|<p><br></p><p>Option A is incorrect because creating an IAM User for each AWS account is an overhead and less preferred way compared to creating IAM Role.</p><p>Option B is incorrect because the scenario says that the company does not have any on-premises identity provider.</p><p>Option C is CORRECT because it creates an IAM Role which has all the necessary permission policies attached to it which allows the auditor to assume the appropriate role while accessing the resources.</p><p>Option D is incorrect because using the IAM Role that has the required permissions is the preferred and more secure way of accessing the AWS resources than using the Amazon credentials. Also, this option does not use any Security Token Service that gives temporary credentials to login. Hence this is a less secure way of accessing the AWS resources.</p><p><br></p><p><span style="font-size: 1rem;">For more information on IAM roles please refer to the below URL</span></p><p><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p>	Configure an on-premise AD server and enable SAML and identify federation for single sign-on to each AWS account. <br>  オンプレミスADサーバーを設定し、SAMLを有効にし、フェデレーションを各AWSアカウントへのシングルサインオンとして識別します。	Create an IAM user for each AWS account with read-only permission policies for the auditor, and disable each account when the audit is complete.  <br>  監査担当者の読み取り専用権限ポリシーを使用して各AWSアカウントのIAMユーザーを作成し、監査が完了したら各アカウントを無効にします。	Create a custom identity broker application that allows the auditor to use existing Amazon credentials to log into the AWS environments. <br>  監査員が既存のAmazonの認証情報を使用してAWS環境にログインできるようにするカスタムIDブローカー・アプリケーションを作成します。
Diag-47. <p>An auditor needs access to logs that record all the API events on AWS. The auditor only needs read-only access to the log files and does not need access to each AWS account. The company has multiple AWS accounts, and the auditor needs access to all the logs for all the accounts. What is the best way to configure access for the auditor to view event logs from all accounts?  Choose the correct answer from the options below</p> |  <p>監査人はAWSのすべてのAPIイベントを記録するログにアクセスする必要があります。監査人はログファイルへの読み取り専用アクセスしか必要とせず、各AWSアカウントへのアクセスは必要ありません。会社には複数のAWSアカウントがあり、監査人はすべてのアカウントのすべてのログにアクセスする必要があります。すべてのアカウントのイベントログを表示するための監査人のアクセスを設定する最善の方法は何ですか？   以下のオプションから正解を選択します。</p>	sa:	Configure the CloudTrail service in each AWS account and have the logs delivered to a single AWS bucket named "seclog" in? a separate account created specifically for storing logs. Provide the auditor to access only "seclog" in that separate account. <br>各AWSアカウントでCloudTrailサービスを設定し、ログを "seclog"という名前の単一のAWSバケットに配信させますか？ ログを保存するために特別に作成された別のアカウント。 その別のアカウントの "seclog"にのみアクセスするように監査人に指示してください。 <br>  別のアカウントの "seclog"にのみアクセスするように監査人に指示します。|<p>You can have CloudTrail deliver log files from multiple AWS accounts into a single Amazon S3 bucket. For example, you have four AWS accounts with account IDs 111111111111, 222222222222, 333333333333, and 444444444444, and you want to configure CloudTrail to deliver log files from all four of these accounts to a bucket belonging to account 111111111111. To accomplish this, complete the following steps in order:<br></p><ol dir="auto"><li><p dir="auto">Turn on CloudTrail in the account where the destination bucket will belong (111111111111 in this example). Do not turn on CloudTrail in any other accounts yet.</p></li><li><p dir="auto">Update the bucket policy on your destination bucket to grant cross-account permissions to CloudTrail.</p></li><li><p dir="auto">Turn on CloudTrail in the other accounts you want (222222222222, 333333333333, and 444444444444 in this example). Configure CloudTrail in these accounts to use the same bucket belonging to the account that you specified in step 1 (111111111111 in this example).</p><p dir="auto"><br></p></li></ol><p>The AWS CloudTrail service provides with an option to deliver the log files for all the regions in a single S3 bucket. This feature is very useful when you need to access the logs related to all the resources in multiple regions for all the AWS accounts via single S3 bucket. Please see the images below:</p><p><img src="https://s3.amazonaws.com/awssap/d_47_1.png" alt="" width="684" height="170" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p><img src="https://s3.amazonaws.com/awssap/d_47_2.png" alt="" width="774" height="219" role="presentation" class="img-responsive atto_image_button_middle"><br></p><p>Option A is incorrect because delivering the logs in multiple buckets is an unnecessary overhead. Instead, you can have CloudTrail deliver the logs to a single S3 bucket.</p><p>Option B is incorrect because consolidated billing will not help the auditor to get the records of all the API events in AWS. </p><p>Option C is incorrect because there is no consolidated logging feature in AWS CloudTrail.</p><p>Option D is CORRECT because it it delivers the logs pertaining to different AWS accounts to a single S3 bucket in the primary account and grants the auditor the access to it.</p><p><br></p><p>More information on this topic regarding CloudTrail:</p><p>You can have CloudTrail deliver log files from multiple AWS accounts into a single Amazon S3 bucket. For example, if you have four AWS accounts with account IDs A, B, C, and D, and you can configure CloudTrail to deliver log files from all four of these accounts to a bucket belonging to account A. See the link below:<br></p><p><a href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html" target="_blank">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html</a><br></p><p><br></p>	Configure the CloudTrail service in the primary AWS account and configure consolidated billing for all the secondary accounts. Then grant the auditor access to the S3 bucket that receives the CloudTrail log files. <br>  プライマリAWSアカウントでCloudTrailサービスを設定し、すべてのセカンダリアカウントの統合請求を設定します。 次に、AuditorがCloudTrailログファイルを受け取るS3バケットにアクセスできるようにします。	Configure the CloudTrail service in each AWS account and enable consolidated logging inside of CloudTrail. <br>  各AWSアカウントでCloudTrailサービスを設定し、CloudTrail内で統合ログを有効にします。	Configure the CloudTrail service in each AWS account, and make the logs delivered to an AWS bucket on each account, while granting the auditor permissions to the bucket via roles in the secondary accounts and a single primary IAM account that can assume a read-only role in the secondary AWS accounts. <br>  各AWSアカウントでCloudTrailサービスを設定し、各アカウントのAWSバケットにログを配信し、セカンダリアカウントのロールと読み取り専用の役割を担うことができる1つのプライマリIAMアカウントを介してバケットに監査者権限を付与する 
Diag-48. <p>An employee keeps terminating EC2 instances on the production environment. You've determined the best way to ensure this doesn't happen to add an extra layer of defense against terminating the instances. What is the best method to ensure that the employee does not terminate the production instances?</p> <p>Choose the 2 correct answers from the options below</p> |  <p>従業員は本番環境でEC2インスタンスを終了し続けます。インスタンスの終了に対して特別な防御層を追加することがないようにするための最良の方法を決定しました。従業員が本番インスタンスを終了しないようにする最良の方法は何ですか？</p> <p>以下のオプションから2つの正解を選択します。</p>	ma:	o:Tag the instance with a production-identifying tag and add resource-level permissions to the employee user with an explicit deny on the terminate API call to instances with the production tag.  <br>  本番タグを使用してインスタンスにタグ付けし、本番タグ付きインスタンスへのAPI呼び出しの終了を明示的に拒否して、従業員ユーザーにリソースレベルの権限を追加します。|<p><br></p><p>To stop the users from manipulating any AWS resources, you can either create the applicable (allow/deny) resource level permissions and apply them to those users, or create an individual or group policy which explicitly denies the action on that resource and apply it to the individual user or the group.</p><p><br></p><p>Option A is CORRECT because it (a) identifies the instances with proper tag, and (b) creates a resource level permission and explicitly denies the user the terminate option.</p><p>Option B is CORRECT because it (a) identifies the instances with proper tag, and (b) creates a policy with explicit deny of terminating the instances and applies that policy to the group which contains the employees (who are not supposed to have the access to terminate the instances).</p><p>Option C and D are incorrect because MFA is an additional layer of security given to the users for logging into AWS and accessing the resources. However, either enabling or disabling MFA cannot prevent the users from performing resource level actions.</p><p><br></p><p><b>More information on Tags</b></p><p>Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type — you can quickly identify a specific resource based on the tags you've assigned to it. Each tag consists of a key and an optional value, both of which you define.</p><p>For more information on tagging AWS resources please refer to the below URL</p><p></p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a><br><p></p>	o:Tag the instance with a production-identifying tag and modify the employees group to allow only start, stop, and reboot API calls and not the terminate instance call. <br>  インスタンスを実動識別タグでタグ付けして、インスタンス・コールの終了ではなく、開始、停止、および再起動のAPI呼び出しのみを許可するようにemployeesグループを変更します。	x:Modify the IAM policy on the user to require MFA before deleting EC2 instances and disable MFA access to the employee <br>  EC 2インスタンスを削除して従業員へのMFAアクセスを無効にする前に、MFAを要求するようにIAMポリシーを変更する	x:Modify the IAM policy on the user to require MFA before deleting EC2 instances  <br>  EC 2インスタンスを削除する前にMFAを要求するようにユーザーのIAMポリシーを変更する
Diag-49. <p>A company is managing a customer's application which currently includes a three-tier application configuration. The first tier manages the web instances and is configured in a public subnet. The second layer is the application layer. As part of the application code, the application instances upload large amounts of data to Amazon S3. Currently, the private subnets that the application instances are running on have a route to a single NAT t2.micro NAT instance. The application, during peak loads, becomes slow and customer uploads from the application to S3 are not completing and taking a long time. Which steps might you take to solve the issue using the most cost-efficient method? Choose the correct answer from the options below</p> |  <p>企業は現在、3層アプリケーション構成を含む顧客のアプリケーションを管理しています。第1層はWebインスタンスを管理し、パブリックサブネットで構成されます。 2番目の層はアプリケーション層です。アプリケーションコードの一部として、アプリケーションインスタンスは大量のデータをAmazon S3にアップロードします。現在、アプリケーションインスタンスが実行しているプラ​​イベートサブネットには、単一のNAT t2.micro NATインスタンスへのルートがあります。負荷がピーク時にアプリケーションが遅くなり、アプリケーションからS3への顧客のアップロードが完了せず、時間がかかります。最も費用対効果の高い方法を使用して問題を解決するにはどの手順を取ることができますか？下記のオプションから正解を選択してください</p>	sa:	Create a VPC S3 endpoint  <br>  VPC S3エンドポイントを作成する|<p><br></p><p>In this scenario, the NAT instance is the bottleneck, which during the peak loads, becomes slow. The possible solutions are either scale up or scale out the NAT instance, or setup S3 as the endpoint of the VPC, so that the VPC can privately and securely connect to the S3 buckets. See the images below for setting up the S3 as VPC Endpoint:</p><p><img src="https://s3.amazonaws.com/awssap/d_49_1.png" alt="" width="998" height="506" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/d_49_2.png" alt="" width="968" height="825" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>Option A is incorrect because even though placing NAT instance in auto scale would handle the increase in load, addition of the NAT instances would not be as cost-efficient as creating an S3 endpoint.</p><p>Option B is CORRECT because with S3 Endpoint, the VPC can privately and securely connect to the S3 buckets. No additional infrastructure provisioning such as NAT or Gateway is needed, hence saving the cost.</p><p>Option C is incorrect because increasing in NAT instance size would add to the cost.</p><p>Option D is incorrect because provisioning additional NAT instances would add to the cost.</p><p><br></p><p>For more information on VPC endpoints please refer to the below URL:</p><p><a href="https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/" target="_blank">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p><p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html" target="_blank">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html</a></p>	Configure Auto Scaling for the NAT instance in order to handle increase in load <br>  負荷の増加に対処するためにNATインスタンスの自動スケーリングを設定する	Increase the NAT instance size; network throughput increases with an increase in instance size <br>  NATインスタンスのサイズを増やします。インスタンスサイズの増加に伴うネットワークスループットの増加	Launch an additional NAT instance in another subnet and replace one of the routes in a subnet to the new instance <br>  別のサブネットで追加のNATインスタンスを起動し、サブネット内のいずれかのルートを新しいインスタンスに置き換えます
Diag-50. <p>A company has many employees who need to run internal applications that access the company's AWS resources. These employees already have user credentials in the company's current identity authentication system, which does not support SAML 2.0. The company does not want to create a separate IAM user for each company employee. <span style="font-size: 1rem;">How should the SSO setup be designed?</span></p> <p>Choose the 2 correct answers from the options below</p> |  <p>会社には、会社のAWSリソースにアクセスする内部アプリケーションを実行する必要がある多くの従業員がいます。これらの従業員は、SAML 2.0をサポートしていない現在の身元認証システムにすでにユーザー資格情報を持っています。 <span style = "font-size：1rem;"> SSOの設定はどのように設計するのですか？</ span> </p> <p>下のオプションから2つの正解を選択してください</p>	ma:	x:Create an IAM user to share based off of employee roles in the company. <br>  会社の従業員の役割に基づいて共有するIAMユーザーを作成します。|<p><br></p><p>Option A is incorrect because creating IAM users is not a best practice and not recommended.</p><p>Option B is CORRECT because, (a) it creates custom identity broker application for authenticating the users using their existing credentials, (b) it gets temporary access credentials using STS, and (3) it uses federated access for accessing the AWS resources.</p><p>Option C is CORRECT because (a) it creates custom identity broker application for authenticating the users using their existing credentials, and (b) it uses AssumeRole API for accessing the resources using temporary role.</p><p>Option D is incorrect because AssumeRoleWithSAML works only with SAML complaint identity providers and the given scenario does not support SAML 2.0.</p><p><br></p><p><b>More information on AssumeRole and GetFederatedToken:</b></p><p>Assume Role - Returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) that you can use to access AWS resources that you might not normally have access to. Typically, you use AssumeRole for cross-account access or federation.</p><p>For more information , please visit the below URL</p><p><a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html" target="_blank">http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p><p>GetFederationToken - Returns a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token) for a federated user. A typical use is in a proxy application that gets temporary security credentials on behalf of distributed applications inside a corporate network</p><p>For more information , please visit the below URL:</p><p><a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_GetFederationToken.html" target="_blank">http://docs.aws.amazon.com/STS/latest/APIReference/API_GetFederationToken.html</a></p><p><br></p>	o:Create a custom identity broker application which authenticates the employees using the existing system, uses the GetFederationToken API call and passes a permission policy to gain temporary access credentials from STS.  <br>  システムを使用して従業員を認証し、GetFederationToken API呼び出しを使用し、アクセス許可ポリシーを渡してSTSから一時アクセス資格情報を取得するカスタムIDブローカー・アプリケーションを作成します。	o:Create a custom identity broker application which authenticates employees using the existing system and uses the AssumeRole API call to gain temporary, role-based access to AWS. <br>  AssumeRole APIコールを使用して従業員を認証し、AWSへのロールベースのアクセスを一時的に取得するカスタムIDブローカーアプリケーションを作成します。	x:Configure an AD server which synchronizes from the company's current Identity Provide and configures SAML-based single sign-on which will then use the AssumeRoleWithSAML API calls to generate credentials for the employees.  <br>  会社の現在のIDと同期するADサーバーを構成するSAMLベースのシングルサインオンを提供して構成し、AssumeRoleWithSAML APIコールを使用して従業員の資格情報を生成します。
Diag-51. <p>You have created a mobile application that serves data stored in an Amazon DynamoDB table. Your primary concern is scalability of the application and being able to handle millions of visitors and data requests. As part of your application, the customer needs access to the data located in the DynamoDB table. Given the application requirements, what would be the best method to design the application? Choose the correct answer from the options below</p> |  <p> Amazon DynamoDBテーブルに格納されたデータを扱うモバイルアプリケーションを作成しました。主な関心事は、アプリケーションのスケーラビリティと、何百万というビジターとデータ要求を処理できることです。アプリケーションの一部として、お客様はDynamoDBテーブルにあるデータにアクセスする必要があります。アプリケーション要件を考えると、アプリケーションを設計する最良の方法は何でしょうか？下記のオプションから正解を選択してください</p>	sa:	Let the users sign in to the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the JavaScript interface in an S3 bucket.  <br>  ユーザーがAmazon、Google、Facebookなどの第三者IDプロバイダを使用してアプリにログインできるようにします。 AssumeRoleWithWebIdentity API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。 JavaScriptでアプリケーションを記述し、S3バケットにJavaScriptインターフェイスをホストします。|<p><br></p><p>The AssumeRolewithWebIdentity returns a set of temporary security credentials for users who have been authenticated in a mobile or web application with a web identity provider, such as Amazon Cognito, Login with Amazon, Facebook, Google, or any OpenID Connect-compatible identity provider.</p><p>Out of option C and D, Option C is invalid because S3 is used to host static websites and not server side language websites.</p><p><br></p><p><img src="https://s3.amazonaws.com/awssap/d_51_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>For more information on AssumeRolewithWebIdentity, please visit the below URL</p><p></p><a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html</a><br><p></p>	Let the users sign into the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWith API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in JavaScript and host the JavaScript interface in an S3 bucket. <br>  Amazon、Google、Facebookなどの第三者のアイデンティティプロバイダを使用して、ユーザーがアプリケーションにログインできるようにします。 AssumeRoleWith API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。 JavaScriptでアプリケーションを記述し、S3バケットにJavaScriptインターフェイスをホストします。	Let the users sign into the app using a third party identity provider such as Amazon, Google, or Facebook. Use the AssumeRoleWithWebIdentity API call to assume the role containing the proper permissions to communicate with the DynamoDB table. Write the application in a server-side language using the AWS SDK and host the application in an S3 bucket for scalability. <br>  Amazon、Google、Facebookなどの第三者のアイデンティティプロバイダを使用して、ユーザーがアプリケーションにログインできるようにします。 AssumeRoleWithWebIdentity API呼び出しを使用して、DynamoDBテーブルと通信するための適切な権限を含むロールを想定します。 AWS SDKを使用してアプリケーションをサーバー側言語で記述し、S3バケットにアプリケーションを配置してスケーラビリティを実現します。	Configure an on-premise AD server utilizing SAML 2.0 to manage the application users inside the on-premise AD server and write code that authenticates against the LD serves. Grant a role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table. <br>  SAML 2.0を利用するオンプレミスADサーバーを構成して、オンプレミスADサーバー内のアプリケーションユーザーを管理し、LDサービスに対して認証するコードを作成します。 エンドユーザーがDynamoDBテーブル内の必要なデータにアクセスできるように、STSトークンに割り当てられたロールを付与します。
Diag-52. <p>Which of the following is NOT a way to minimize the attack surface area as a DDOS minimization strategy in AWS? Choose the correct answer from the options below</p> |  <p> AWSのDDoS最小化戦略として攻撃領域を最小限に抑える方法として、次のうちどれが適切でないですか？下記のオプションから正解を選択してください</p>	sa:	Configure services such as Elastic Load Balancing and Auto Scaling to automatically scale.  <br>  Elastic Load BalancingやAuto Scalingなどのサービスを自動的に拡大/縮小するように設定します。|<p>Some important consideration when architecting on AWS is to limit the opportunities that an attacker may have to target your application. For example, if you do not expect an end user to directly interact with certain resources you will want to make sure that those resources are not accessible from the Internet. Similarly, if you do not expect end-users or external applications to communicate with your application on certain ports or protocols, you will want to make sure that traffic is not accepted. This concept is known as attack surface reduction.</p><p><span style="font-size: 1rem;">Option A is CORRECT because it is used for mitigating the DDoS attack where the system scales to absorb the application layer traffic in order to keep it responsive. </span></p><p><span style="font-size: 1rem;">Option B, C and D are incorrect as they all are used for reducing the DDoS attack surface.</span></p><p>For more information on DDoS attacks in AWS, please visit the below URL</p><p><a href="https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p>	Reduce the number of necessary Internet entry points. <br>  必要なインターネットエントリポイントの数を減らします。	Separate end user traffic from management traffic. <br>  エンドユーザーのトラフィックを管理トラフィックから分離する。	Eliminate non-critical Internet entry points. <br>  重要ではないインターネットエントリポイントを排除します。
Diag-53. <p><span id="docs-internal-guid-846a0cce-307f-93c8-4766-acc6f0a11f45"></span></p><p dir="ltr" style="">A customer has established an AWS Direct Connect connection to AWS. The link is up and routes are being advertised from the customer’s end, however the customer is unable to connect from EC2 instances inside its VPC to servers residing in its data center.</p><p dir="ltr" style="">Which of the following options provide a viable solution to remedy this situation? (Choose 2 answers).</p><br><p></p> |  <p> <span id = "docs-internal-guid-846a0cce-307f-93c8-4766-acc6f0a11f45"> </ span> <p dir = "ltr" style = ""> AWS Direct接続をAWSに接続します。リンクはアップしており、ルートは顧客側からアドバタイズされていますが、顧客はVPC内のEC2インスタンスからデータセンターにあるサーバーに接続できません。</p> <p dir = "ltr" style = "" >このような状況を改善するために実行可能な解決策を提供する次のオプションはどれですか？ （2つの回答を選択してください）。</p> <br> <p> </p>	ma:	x:Add a route to the route table with an IPsec VPN connection as the target. <br>  IPsec VPN接続をターゲットとしてルートテーブルにルートを追加します。|<p dir="ltr" style=""><br></p><p dir="ltr" style="">Option A is incorrect because adding an option of VPN is unnecessary. </p><p dir="ltr" style="">Option B is CORRECT because VGW is the other side of the connection (on the AWS side) and the route propagation needs to be enabled for the Direct Connect to work.</p><p dir="ltr" style="">Option C is incorrect because the question mentions that the routes are being advertised from the customer’s end. So no changes are needed at the customer side.</p><p dir="ltr" style="">Option D is incorrect because there is no “route” command available on the instances in the VPC.</p><p dir="ltr" style="">Option E is CORRECT because the route table needs to be updated so that the EC2 instances can communicate with the on-premise environment. </p></span><br><p><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/troubleshooting-response-errors.html" target="_blank"></a></p>	o:Enable route propagation to the Virtual Private Gateway (VGW). <br>  仮想プライベートゲートウェイ（VGW）へのルート伝播を有効にします。	x:Enable route propagation to the customer gateway (CGW).  <br>  カスタマーゲートウェイ（CGW）へのルート伝播を有効にします。	x:Modify the route table of all Instances using the ‘route’ command. <br>   'route'コマンドを使用して、すべてのインスタンスのルートテーブルを変更します。	o:Modify the Instances VPC subnet route table by adding a route back to the customer’s on-premises environment.  <br>  顧客のオンプレミス環境にルートを追加して、インスタンスVPCサブネットルートテーブルを変更します。
Diag-54. <p>You are setting up a website for a small company. This website serves up images and is very resource intensive. You have decided to serve up the images using Cloudfront. There is a requirement though, that the content should be served up using a custom domain and should work with https. <br></p><p>What can you do to ensure this requirement is fulfilled?</p> |  <p>あなたは小さな会社のウェブサイトを設定しています。このウェブサイトはイメージを提供し、非常にリソース集約的です。 Cloudfrontを使用して画像を提供することに決めました。ただし、カスタムドメインを使用してコンテンツを配信する必要があり、httpsで動作する必要があります。 <br> </p> <p>この要件が満たされていることを確認するにはどうすればよいですか？</p>	sa:	You must provision Server Name Indication (SNI) Custom SSL for your CloudFront Distribution. <br>  CloudFront Distributionのサーバー名表示（SNI）カスタムSSLをプロビジョニングする必要があります。|<p>Custom SSL certificate support lets you deliver content over HTTPS using your own domain name and your own SSL certificate. This gives visitors to your website the security benefits of CloudFront over an SSL connection that uses your own domain name in addition to lower latency and higher reliability.<br>Note: Please note that some older browsers do not support SNI and will not be able to establish a connection with CloudFront to load the HTTPS version of your content. <br><br></p><ul><li><a href="https://aws.amazon.com/cloudfront/custom-ssl-domains/" rel="noreferrer" style="font-size: 1rem;">https://aws.amazon.com/cloudfront/custom-ssl-domains/</a></li></ul><p></p><p>Option A is incorrect because custom SSL certificate or third-party certificate can not be configured in Route53.</p><p><span style="font-size: 1rem;">Option C is incorrect because ALIAS is just to configure Route 53 and does not handle the custom domain and custom SSL certificate aspect.</span></p><p>Option D is incorrect because Origin Access identity(OAI) does not deal with custom SSL, it is only used to ensure that the origin is accessible with CloudFront distribution only.</p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;"><b>More information on Custom SSL Domains:</b></span></p><br><p><span style="font-size: 1rem;">AWS Cloudfront can use IAM certificates.</span></p><div>Reference Link: </div><div><a href="https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-custom-certificate/" target="_blank">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-custom-certificate/</a><br></div><div><br>Also there is a discussion forum on the same topic ""ssl certificate IAM" in the Amazon CloudFront Discussion Forum". </div><div>It is helpful in understanding about this topic further.<br></div><br><span style="font-size: 1rem;">For more information on CloudFront custom SSL domains, please visit the below URL</span><p></p><p></p><a href="https://aws.amazon.com/cloudfront/custom-ssl-domains/" target="_blank" style="font-size: 1rem;">https://aws.amazon.com/cloudfront/custom-ssl-domains/</a><br><br><p></p>	You must provision and configure your own SSL certificate in Route 53 and associate it to your CloudFront distribution. <br>  独自のSSL証明書をRoute 53にプロビジョニングして設定し、それをCloudFrontディストリビューションに関連付ける必要があります。	You must provision and configure an ALIAS in Route 53 and associate it to your CloudFront distribution <br>  Route 53にALIASをプロビジョニングして設定し、CloudFrontディストリビューションに関連付ける必要があります	You must create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket where the images are stored.  <br>  CloudFrontのOrigin Access Identity（OAI）を作成し、イメージが格納されているS3バケット内のオブジェクトへのアクセスを許可する必要があります。
Diag-55. <p>You have recently migrated an application from a customer's on-premise data center to the AWS cloud. Currently, you're using the ELB to serve traffic to the legacy application. The ELB is also using HTTP port 80 as the health check ping port. The application is currently responding by returning a website to port 80 when you test the IP address directly. However, the instance is not registering as healthy even though the appropriate amount of time has passed for the health check to register as healthy. How might the issue be resolved? Choose the correct answer from the options below</p> |  <p>お客様のオンプレミスデータセンターからAWSクラウドにアプリケーションを移行したことがあります。現在、ELBを使用してレガシーアプリケーションへのトラフィックを提供しています。 ELBは、ヘルスチェックのpingポートとしてHTTPポート80も使用しています。アプリケーションは現在、IPアドレスを直接テストするときにWebサイトをポート80に戻すことで応答しています。ただし、健全性チェックが正常に登録されるまでに適切な時間が経過しても、インスタンスは正常に登録されていません。問題はどのように解決されますか？下記のオプションから正解を選択してください</p>	sa:	Change the ELB listener port from HTTP port 80 to TCP port 80 for the instance to register as healthy  <br>  インスタンスが正常に登録されるように、ELBリスナーポートをHTTPポート80からTCPポート80に変更します。|<p><br></p><p>Since the application is a custom application and not a standard HTTP application, hence you need to have the TCP ports open.</p><p>Before you start using Elastic Load Balancing, you must configure one or more <em>listeners</em> for your Classic Load Balancer. A listener is a process that checks for connection requests. It is configured with a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instance) connections.</p><p>Elastic Load Balancing supports the following protocols:</p> <ul> <li>HTTP</li> <li>HTTPS (secure HTTP)</li> <li>TCP</li> <li>SSL (secure TCP)</li> </ul> <p> </p><p>For more information on listener configuration for ELB, please see the below link:</p><p></p><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html</a><br><br><p></p>	Change the ELB listener port from ping port 80 to HTTPS port 80 for the instance to register as healthy <br>  ELBリスナーポートをpingポート80からHTTPSポート80に変更して、インスタンスを正常として登録します。	Change the ELB listener port from HTTP port 80 to HTTPS port 80 for the instance to register as healthy <br>  正常に登録するために、ELBリスナーポートをHTTPポート80からHTTPSポート80に変更します。	Change the ELB listener port from HTTP port 80 to TCP port 443 for the instance to register as healthy <br>  HTTPポート80からTCPポート443にELBリスナーポートを変更して、インスタンスを正常として登録する
Diag-56. <p>Suppose you are hosting a website in an S3 bucket. Your users load the website endpoint https://website.s3-website-us-east-1.amazonaws.com. Now you want to use CSS on the web pages that are stored in a different bucket which is also public. But layout on the client browser is not loading properly. What might have gone wrong? Choose the correct option from given below</p> |  <p> S3バケットにウェブサイトをホストしているとします。ユーザーがWebサイトのエンドポイントhttps://website.s3-website-us-east-1.amazonaws.comを読み込みます。今では、公開されている別のバケットに格納されているWebページでCSSを使用したいと考えています。しかし、クライアントブラウザのレイアウトが正しく読み込まれていません。何がうまくいかなかったのでしょうか？下記の正しいオプションを選択してください。</p>	sa:	You can configure your bucket to explicitly enable cross-origin requests from website.s3-website-us-east-1.amazonaws.com. <br>  バケットを設定して、website.s3-website-us-east-1.amazonaws.comからのクロスオリジン要求を明示的に有効にすることができます。|<p>Option A is CORRECT because Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support in Amazon S3, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.</p><p>Option B and C are incorrect because updating bucket policy on just one bucket does not give the CORS access to other buckets.</p><p>Option D is incorrect because this can be achieved using CORS configuration.</p><p><br></p><p>For more information on Cross-origin resource sharing, please refer to the below URL<br> <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html" target="_blank">http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html</a></p>	Modify bucket policy on css bucket to able to access website bucket <br>  cssバケットのバケットポリシーをウェブサイトのバケットにアクセスできるように変更する	Modify bucket policy on website bucket to able to access css bucket  <br>  CSSバケットにアクセスできるようにウェブサイトバケット上のバケットポリシーを変更する	This is not possible <br>  これは不可能です
Diag-57. <p>Your supervisor gave you brief of a client who needs a web application set up on AWS. The most important requirement is that MySQL must be used as the database, and this database must not be hosted in the public cloud, rather at the client's data center due to security risks. Which of the following solutions would be the best to assure that the client’s requirements are met? Choose the correct answer from the options below</p> |  <p>あなたの管理者は、AWSで設定されたウェブアプリケーションを必要とするクライアントの簡単な  最も重要な要件は、MySQLをデータベースとして使用する必要があり、セキュリティリスクのためにこのデータベースをパブリッククラウドではなく、クライアントのデータセンターでホストしないことです。クライアントの要件が満たされていることを保証するには、次の解決策のどれがベストでしょうか？下記のオプションから正解を選択してください</p>	sa:	Build the application server on a public subnet and the database at the client’s data center. Connect them with a VPN connection which uses IPsec. <br> パブリックサブネット上にアプリケーションサーバーを構築し、クライアントのデータセンターにデータベースを構築します。 IPsecを使用するVPN接続で接続します。|<p><br></p><p>The main architectural consideration in this scenario is that the database should remain on the client's data center.  S<span style="font-size: 1rem;">ince the database should not be hosted on the cloud, you cannot put the database in any subnet in AWS.</span></p><p><br></p><p>Option A is CORRECT because it puts the application servers in public subnet and keeps the database server at the client's data center.</p><p>Option B is incorrect because MySQL must be used as the database. Also, configuring the storage gateway is an unnecessary overhead.</p><p>Option C is incorrect because the requirement is to keep the database server at the client's data center. So you cannot put it in any AWS VPC subnet.</p><p>Option D is incorrect because building the database in private subnet and accessing the database server at client's data center via ssh is totally unnecessary and non-feasible.</p><p><br></p><p>The best option is to create a VPN connection for securing traffic as shown below</p><p> <img src="https://s3.amazonaws.com/awssap/d_57_1.png" alt="" width="830" height="423" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on VPN connections, please visit the below URL</p><p></p><a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a><br><br><p></p>	Use the public subnet for the application server and use RDS with a storage gateway to access and synchronize the data securely from the local data center. <br>  アプリケーションサーバーにはパブリックサブネットを使用し、ストレージゲートウェイではRDSを使用して、ローカルデータセンターから安全にデータにアクセスし、同期させます。	Build the application server on a public subnet and the database on a private subnet with a NAT instance between them. <br>  パブリックサブネット上のアプリケーションサーバーと、プライベートサブネット上のデータベースをNATインスタンスとともにビルドします。	Build the application server on a public subnet and build the database in a private subnet with a secure ssh connection to the private subnet from the client's data center. <br>  パブリックサブネット上にアプリケーションサーバーを構築し、クライアントのデータセンターからのプライベートサブネットへの安全なssh接続を使用してプライベートサブネットにデータベースを構築します。
Diag-58. <p>Regarding encryption on data stored on your databases, namely Amazon RDS, which of the following statements is the true? Choose the correct answer from the options below</p> |  <p>あなたのデータベース、つまりAmazon RDSに格納されているデータの暗号化に関しては、次の文のどちらが本当ですか？下記のオプションから正解を選択してください</p>	sa:	Encryption can be enabled on RDS instances to encrypt the underlying storage, and this will by default also encrypt snapshots as they are created. No additional configuration needs to be made on the client side for this to work. <br>  RDSインスタンスで暗号化を有効にして、基本ストレージを暗号化することができます。これにより、デフォルトでスナップショットが作成されたときに暗号化されます。 これが機能するには、クライアント側で追加の構成を行う必要はありません。|<p><br></p><p>Tip for the exam: You can encrypt your Amazon RDS instances and snapshots at rest by enabling the encryption option for your Amazon RDS DB instance (only certain EC2 instance types support encryption, more information is given below). Data that is encrypted at rest includes the underlying storage for a DB instance, its automated backups, Read Replicas, and snapshots.</p><p><br></p><p>Option A is incorrect because the RDS instance encryption supports only those Master Keys that are created and managed by KMS.</p><p>Option B is CORRECT because once the encryption is enabled, its automated backups, read replicas, and snapshots are automatically encrypted without need of any addition settings.</p><p><span style="font-size: 1rem;">Option C is incorrect because no additional configurations need to be made from client side, once the encryption is enabled.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because, as mentioned above, the snapshots get automatically encrypted once the encryption is turned-on on the RDS instance.</span></p><p><br></p><p>For more information on RDS encryption, please visit the below url</p><p><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html" target="_blank">http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a></p><p>See the list of instance types that support the encryption:</p><p><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Availability" target="_blank">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html#Overview.Encryption.Availability</a></p>	Encryption cannot be enabled on RDS instances unless the keys are not managed by KMS. <br>  キーがKMSによって管理されていない限り、RDSインスタンスで暗号化を有効にすることはできません。	Encryption can be enabled on RDS instances to encrypt the underlying storage, and this will by default also encrypt snapshots as they are created. However, some additional configuration needs to be made on the client side for this to work. <br>  RDSインスタンスで暗号化を有効にして、基本ストレージを暗号化することができます。これにより、デフォルトでスナップショットが作成されたときに暗号化されます。 ただし、これを機能させるには、クライアント側で追加の設定を行う必要があります。	Encryption can be enabled on RDS instances to encrypt the underlying storage, but you cannot encrypt snapshots as they are created. <br>  RDSインスタンスで暗号化を有効にして、基になるストレージを暗号化できますが、スナップショットが作成されると暗号化することはできません。
Diag-59. <p>You are setting up a VPN for a customer to connect his remote network to his Amazon VPC environment. There are many ways to accomplish this. Also, you have given a list of the things that the customer has specified that the network needs to be able to do. They are as follows:</p> <p> </p> <p>- Predictable network performance</p> <p>- Support for BGP peering and routing policies</p> <p>- A secure IPsec VPN connection but not over the Internet</p> <p> </p> <p>Which of the following VPN options would best satisfy the customer's requirements? Choose the correct answer from the options below</p> |  <p>お客様がリモートネットワークをAmazon VPC環境に接続するためのVPNを設定しています。これを達成する方法はたくさんあります。また、ネットワークが必要とするように顧客が指定したもののリストを指定しました。</p> <p> - 予測可能なネットワークパフォーマンス</p> <p>  -  BGPピアリングとルーティングポリシーのサポート</p> <p>  - 安全なIPsec VPN接続ではなく、インターネット経由ではありません。</p><p>次のVPNオプションはどれですか？下記のオプションから正解を選択してください</p>	sa:	AWS Direct Connect and IPsec Hardware VPN connection over private lines <br>  AWSダイレクトコネクトとIPsecハードウェアVPN接続（専用回線経由）|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">Since one of the requirements does not use the internet, Option A and D is not advisable since they would traverse over the internet, and the internet connectivity always has unpredictability as one of the factors.</span></p><p>Since there is a need for predictable network performance, AWS Direct connect becomes the best option. Along with a Hardware VPN connection, it can create the secure VPN connection that is demanded. See the image below:</p><p> <img src="https://s3.amazonaws.com/awssap/d_59_1.png" alt="" width="627" height="422" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p><br></p><p>Option A and D are incorrect because both approaches uses internet connectivity - which is not what the scenario wants to use.</p><p>Option B is CORRECT because (a) with AWS Direct Connect, you would always get the predictable network performance without using the internet, and (b) it uses Hardware VPN Connection which is a secure way of logging into the AWS platform.</p><p>Option C is incorrect because CloudHub is used when your remote sites want to communicate with each other, and not just with the AWS VPC. AWS Direct Connect with Hardware VPN is the best architectural solution here.</p><p><br></p><p>There is a good read on different connection options for AWS. Please visit the below URL on the same.</p><p><a href="https://d0.awsstatic.com/whitepapers/aws-amazon-vpc-connectivity-options.pdf" target="_blank">https://d0.awsstatic.com/whitepapers/aws-amazon-vpc-connectivity-options.pdf</a></p><p>More information on AWS CloudHub:</p><p><a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPN_CloudHub.html" target="_blank">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPN_CloudHub.html</a><br></p>	Software appliance-based VPN connection with IPsec <br>  IPsecによるソフトウェアアプライアンスベースのVPN接続	AWS Direct Connect with AWS VPN CloudHub <br>  AWS VPN CloudHubとの直接接続	AWS VPN CloudHub <br>  AWS VPN CloudHub
Diag-60. <p>Your security officer has told you that you need to tighten up the logging of all events that occur on your AWS account. He wants to be able to access all events that occur on the account across all regions quickly and in the simplest possible manner. He also wants to make sure he is the only person that has access to these events in the most secure way possible. Which of the following would be the best solution to assure his requirements are met? Choose the correct answer from the options below</p> |  <p>セキュリティ担当者から、AWSアカウントで発生するすべてのイベントのログを強化する必要があることが示されています。彼は、すべての地域のアカウントで発生したすべてのイベントに、簡単かつ迅速にアクセスできるようにしたいと考えています。彼はまた、可能な限り安全な方法でこれらのイベントにアクセスできる唯一の人物であることを確認したいと考えています。次のうち、要件が満たされていることを確認する最適なソリューションはどれですか？下記のオプションから正解を選択してください</p>	sa:	Use CloudTrail to log all events to one S3 bucket. Make this S3 bucket only accessible by your security officer with a bucket policy that restricts access to his user only and also add MFA to the policy for a further level of security. <br>  CloudTrailを使用して、すべてのイベントを1つのS3バケットに記録します。 このS3バケットは、セキュリティ管理者がアクセスできるのはバケットポリシーだけで、彼のユーザへのアクセスを制限し、セキュリティレベルをさらに高めるためにポリシーに追加します。|<p><br></p><p>The main points to consider in this scenario is: (1)  the security officer needs to access all events that occur on the account <b>across all the regions</b>, and (2) only that security officer should have the access.</p><p><br></p><p></p><p>Option A is CORRECT because it configures only one S3 bucket for all the CloudTrail log events on the account across all the regions. It also restricts the access to the security officer only via the bucket policy. See the images below:</p><p><img src="https://s3.amazonaws.com/awssap/d_60_1.png" alt="" width="857" height="214" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><img src="https://s3.amazonaws.com/awssap/d_60_2.png" alt="" width="969" height="276" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Option B is incorrect because it uses Amazon Glacier vaults which is an archival solution and not used for storing the CloudTrail logs .</p><p>Option C is incorrect because sending the API calls to CloudWatch is unnecessary. Also notifying the security officer via email is not a good nor a secure architecture.</p><p>Option D is incorrect because CloudTrail provides with an option where are all the logs get delivered to a single S3 bucket. Putting all the logs in separate buckets is an overhead .</p><p><br></p><p><b>More information on AWS CloudTrail</b></p><p></p><p>AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting.</p><p>You can design CloudTrail to send all logs to a central S3 bucket.</p><p>For more information on CloudTrail, please visit the below URL</p><p><a href="https://aws.amazon.com/cloudtrail/" target="_blank">https://aws.amazon.com/cloudtrail/</a></p>	Use CloudTrail to log all events to an Amazon Glacier Vault. Make sure the vault access policy only grants access to the security officer's IP address. <br>  CloudTrailを使用して、すべてのイベントをAmazon Glacier Vaultに記録します。ボールトアクセスポリシーでは、セキュリティ担当者のIPアドレスへのアクセスのみが許可されていることを確認してください。	Use CloudTrail to send all API calls to CloudWatch and send an email to the security officer every time an API call is made. Make sure the emails are encrypted. <br>  CloudTrailを使用してCloudWatchにすべてのAPI呼び出しを送信し、API呼び出しが行われるたびにセキュリティ担当者に電子メールを送信します。電子メールが暗号化されていることを確認してください。	Use CloudTrail to log all events to a separate S3 bucket in each region as CloudTrail cannot write to a bucket in a different region. Use MFA and bucket policies on all the different buckets. <br>  CloudTrailを使用すると、CloudTrailが別の地域のバケットに書き込むことができないため、すべてのイベントを各地域の別々のS3バケットに記録できます。すべての異なるバケットでMFAとバケットポリシーを使用します。
Diag-61. <p>You have created a temporary application that accepts image uploads, stores them in S3, and records information about the image in RDS. After building this architecture and accepting images for the duration required, it's time to delete the CloudFormation template. However, your manager has informed you that for archival reasons the RDS data needs to be stored and the S3 bucket with the images needs to remain. Your manager has also instructed you to ensure that the application can be restored by a CloudFormation template and run next year during the same period. Knowing that when a CloudFormation template is deleted, it will remove the resources it created, what is the best method to achieve the desired goals? Choose the correct answer from the options below</p> |  <p>イメージのアップロードを受け付け、S3に保存し、RDSのイメージに関する情報を記録する一時的なアプリケーションを作成しました。 このアーキテクチャを構築し、必要な期間画像を受け入れたら、CloudFormationテンプレートを削除します。 しかし、あなたのマネージャーは、アーカイブの理由から、RDSデータを保存する必要があり、イメージ付きのS3バケットを残す必要があることを通知しました。 あなたのマネージャーは、CloudFormationテンプレートでアプリケーションを復元し、同じ期間に翌年に実行できるようにするよう指示しました。 CloudFormationテンプレートが削除されたときに作成されたリソースが削除されることがわかっているので、目的の目標を達成するための最良の方法は何ですか？ 下のオプションから正解を選んでください</p>	sa:	Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to retain, set the RDS resource declaration DeletionPolicy to snapshot. <br>  CloudFormationテンプレートのS3リソース宣言のDeletionPolicyをRetainに設定し、RDSリソース宣言のDeletionPolicyをSnapshotに設定します。|<p><br></p><p>The main points in this scenario are: even if the CloudFormation stack is deleted,  (1) the RDS data needs to be stored, and (2) the S3 bucket with the images should remain (not be deleted).</p><p><br></p><p>Option A is incorrect because even if the images are backed up to another bucket, the original bucket would be deleted if the CloudFormation stack is deleted. One of the requirements is to retain the S3 bucket.</p><p>Option B is incorrect because DeletionPolicy attribute for RDS should be <i>snapshot</i>, not <i>retain </i>because with <i>snapshot </i>option, the backup of the RDS instance would be stored in the form of snapshots (which is the requirement). With <i>retain </i>option, CF will keep the RDS instance alive which is unwanted.</p><p>Option C is incorrect because the DeletionPolicy of the S3 bucket should be <i>retain</i>, not <i>snapshot</i>.</p><p>Option D is CORRECT because it correctly sets the DeletionPolicy of <i>retain </i>on S3 bucket and <i>snapshot </i>on RDS instance.</p><p><br></p><p><b>More information on DeletionPolicy on CloudFormation</b></p><p><span></span></p><p style="">DeletionPolicy options include:</p><ul style=""><li><span><b>Retain</b>:</span> You retain the resource in the event of a stack deletion.</li><li><span><b>Snapshot</b>:</span> You get a snapshot of the resource before it’s deleted. This option is available only for resources that support snapshots.</li><li><span><b>Delete</b>:</span> You delete the resource along with the stack. This is the default outcome if you don’t set a DeletionPolicy.</li></ul><p style="">To keep or copy resources when you delete a stack, you can specify either the Retain or Snapshot policy options.</p><p></p><p>With the DeletionPolicy attribute, you can preserve or (in some cases) backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.</p><p> <img src="https://s3.amazonaws.com/awssap/d_61_1.png" alt="" width="738" height="243" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on Cloudformation deletion policy, please visit the below URL</p><p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html" target="_blank">http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p>	For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to Retain <br>  CloudFormationテンプレートのRDSリソースタイプとS3リソースタイプの両方で、DeletionPolicyをRetainに設定します	Set the DeletionPolicy on the S3 resource to snapshot and the DeletionPolicy on the RDS resource to snapshot. <br>  S3リソースのDeletionPolicyをスナップショットに設定し、RDSリソースのDeletionPolicyをスナップショットに設定します。	Enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects, set the deletion policy for the RDS instance to snapshot. <br>  ソースバケット上のS3バケットレプリケーションを宛先バケットに有効にして、すべてのS3オブジェクトのコピーを維持し、RDSインスタンスの削除ポリシーをスナップショットに設定します。
Diag-62. <p>An auditor has been advised to go through the VPC artifacts in your AWS account. Which of the below options should be carried out so that the auditor can carry out the audit? Choose the correct answer from the options below.</p> |  <p>監査人は、AWSアカウントのVPC成果物を通過するように指示されています。 監査人が監査を実行できるように、以下のオプションのうちどれを実行する必要がありますか？ 下のオプションから正解を選んでください。</p>	sa:	Create an IAM Role with the read only permissions to access the AWS VPC infrastructure and assign that role to the auditor. <br>  AWS VPCインフラストラクチャにアクセスし、そのロールを監査人に割り当てるための読み取り専用権限を持つIAMロールを作成します。|<p><br></p><p>Generally, you should refrain from giving high-level permissions and give only the required permissions. In this case, option C fits well by just providing the relevant access which is required.</p><p><br></p><p>Option A is incorrect because you should create an IAM Role with the needed permissions.  </p><p>Option B is incorrect because you should not give the root access as it will give the user full access to all AWS resources.</p><p>Option C is CORRECT because IAM Role gives just the minimum required permissions (read-only) to audit the VPC infrastructure to the auditor.</p><p>Option D is incorrect because you should not give the auditor full access to the VPC.</p><p><br></p><p>For more information on Identity and Access Management, please visit the below URL</p><p><a href="https://aws.amazon.com/iam/" target="_blank">https://aws.amazon.com/iam/</a></p>	Give the auditor root access to your AWS Infrastructure, because an auditor will always need access to every service. <br>  監査人は常にすべてのサービスにアクセスする必要があるため、監査人にAWSインフラストラクチャへのアクセス権を与えます。	Create an IAM user tied to an administrator role. Also provide an additional level of security with MFA. <br>  管理者ロールに関連付けられたIAMユーザーを作成します。また、MFAのセキュリティをさらに強化します。	Create an IAM user with full VPC access but set a condition that will not allow the auditor to modify anything if the request is from any IP other than their own. <br>  完全なVPCアクセスを持つIAMユーザーを作成しますが、要求が自分以外のIPからのものであれば、監査人が何も変更できないような条件を設定します。
Diag-63. <p>You are launching your first ElastiCache cache cluster, and start using Memcached. Which of the following is NOT a key features of Memcache. Choose the correct answer from the options below</p> | <p>最初のElastiCacheキャッシュクラスタを起動し、Memcachedの使用を開始します。Memcacheの主な機能ではないものは次のどれですか。下記のオプションから正解を選択してください</p>	sa:	You use more advanced data types, such as lists, hashes, and sets. <br>  リスト、ハッシュ、セットなど、より高度なデータ型を使用します。|<p>Option B is CORRECT because it is Redis, not Memcached,which supports advanced/complexdata types such as strings, hashes, lists, sets, sorted sets, and bitmaps.</p><p>Option A, C and D are all incorrect because these are the main features of Memcached.</p><p><br></p><p>For the exam, it is very important to remember the differences between Memcached and Redis. Both are excellent solutions, but used for different scenarios. Please see the notes given below by the AWS documentation: </p><p> </p><p><b>Choose Memcached if the following apply to your situation:</b></p><ul type="disc"><li><p><u>You need the simplest model possible.</u></p></li><li><p>You need to run large nodes with multiple cores or threads.</p></li><li><p><u>You need the ability to scale out/in</u>, adding and removing nodes as demand on your system increases and decreases.</p></li><li><p><u>You need to cache objects, such as a database.</u></p></li></ul><p><b>Choose Redis 2.8.x or Redis 3.2.4 (non-clustered mode) if the following apply to your situation:</b></p><ul type="disc"><li><p><u>You need complex data types, such as strings, hashes, lists, sets, sorted sets, and bitmaps.</u></p></li><li><p>You need to sort or rank in-memory data-sets.</p></li><li><p>You need persistence of your key store.</p></li><li><p>You need to replicate your data from the primary to one or more read replicas for read intensive applications.</p></li><li><p>You need automatic failover if your primary node fails.</p></li><li><p>You need publish and subscribe (pub/sub) capabilities—to inform clients about events on the server.</p></li><li><p>You need backup and restore capabilities.</p></li><li><p>You need to support multiple databases.</p></li></ul><br><p></p><p>For more information on the various caching engines, please visit the below url</p><p><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/SelectEngine.Uses.html" target="_blank">http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/SelectEngine.Uses.html</a></p>	You need the ability to scale your cache horizontally as you grow. <br>  あなたが成長するにつれてキャッシュを水平に拡大する能力が必要です。	You need as simple a caching model as possible. <br>  できるだけ単純なキャッシュモデルが必要です。	Object caching is your primary goal to offload your database. <br>  オブジェクトキャッシュは、データベースの負荷を軽減することを第一の目標としています。
Diag-64. <p>A new client may use your company to move all their existing Data Center applications and infrastructure to AWS. This is going to be a huge contract for your company, and you have been handed the entire contract and need to provide an initial scope to this possible new client. One of the things you notice concerning the existing infrastructure is that it has few legacy applications that you are almost certain will not work on AWS. Which of the following would be the best strategy to employ regarding the migration of these legacy applications? Choose the correct answer from the options below</p> | <p>新しいクライアントがあなたの会社を使用して、既存のすべてのデータセンターアプリケーションとインフラストラクチャをAWSに移行できます。これはあなたの会社のための巨大な契約になるでしょう、あなたは契約全体を手渡されており、この可能な新しいクライアントに最初の範囲を提供する必要があります。既存のインフラストラクチャに関して気づいていることの1つは、AWSで動作しないことがほぼ確実なレガシーアプリケーションはほとんどないということです（ゼロではない）。これらのレガシーアプリケーションの移行に関して、以下のうちどれを採用するのが最適な戦略ですか？下記のオプションから正解を選択してください</p>	sa:	Create a hybrid cloud by configuring a VPN tunnel to the on-premises location of the Data Center. <br>  オンプレミスのデータセンターの場所にVPNトンネルを構成することで、ハイブリッドクラウドを作成します。|<p><br></p><p>Option A is incorrect because, there are some legacy application that will not work on AWS platform. So creating VPC for such applications will not be possible.</p><p>Option B is incorrect because the scenario explicitly mentions that there are some components of the application (legacy part) that will not work with AWS. So, it is highly presumptuous that the legacy application can be run by an AWS Machine Image (legacy application may consist of more than just AMIs).</p><p>Option C is CORRECT because it uses<span style="font-size: 1rem;"> hybrid approach - where the legacy application stays on-premises. It should definitely work as the remaining infrastructure would be on AWS. The communication between the two infrastructures would be taken care by establishing the VPN connection. This is certainly the most viable, time and cost saving solution among the given options.</span></p><p>Option D is incorrect because it is the least feasible solution. First of all, de-comissioning the legacy application may not be possible for the client; especially when the scenario says that the legacy application is almost surely not going to work on AWS. Still, even if they agree, it would be a big impact on the client in terms of time, cost and efforts to re-architect the solution to replace the legacy application.<br><br></p><p><b>More information on the hybrid setup:</b></p><p>The best option is to have a dual mode wherein you have the legacy apps running on-premise and start migrating the apps which have compatibility in the cloud. Have a VPN connection from the on-premise to the cloud for ensuring communication can happen from each environment to the other.</p><p> <img src="https://s3.amazonaws.com/awssap/d_64_1.png" alt="" width="784" height="630" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For the full fundamentals on AWS networking options, please visit the URL:</p><p><a href="https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/" target="_blank">https://aws.amazon.com/blogs/apn/amazon-vpc-for-on-premises-network-engineers-part-one/</a></p>	Move the legacy applications onto AWS first, before you build any infrastructure. There is sure to be an AWS Machine Image that can run this legacy application. <br>  インフラストラクチャを構築する前に、レガシーアプリケーションをAWSにまず移動してください。 このレガシーアプリケーションを実行できるAWSマシンイメージがあることは間違いありません。	Create two VPCs. One containing all the legacy applications and the other containing all the other applications. Make a connection between them with VPC peering. <br>  2つのVPCを作成します。1つのエントリーはすべてのレガシーアプリケーションと他のエントリーです。	Convince the client to look for another solution by de-commissioning these applications and seeking out new ones that will run on AWS. <br>  これらのアプリケーションをデコミッショニングし、AWS上で実行される新しいアプリケーションを探すことによって、クライアントに別のソリューションを探すよう説得します。
Diag-65. <p>You're building a mobile application game. The application needs permission for each user to communicate and store data in DynamoDB tables. What is the best method for granting each mobile device (that installs your application) to access DynamoDB tables for storage when required? Choose the correct answer from the options below</p> | <p>あなたはモバイルアプリケーションゲームを構築しています。 このアプリケーションでは、各ユーザーが通信してDynamoDBテーブルにデータを格納する権限が必要です。 必要に応じてストレージ用のDynamoDBテーブルにアクセスするための各モバイルデバイス（アプリケーションをインストールする）を認可するための最良の方法は何ですか？ 下のオプションから正解を選んでください</p>	sa:	Create an IAM role with the proper permission policy to communicate with the DynamoDB table. Use web identity federation, which assumes the IAM role using AssumeRoleWithWebIdentity, when the user signs in, granting temporary security credentials using STS. <br>  DynamoDBテーブルを使用するには、適切なアクセス許可ポリシーを使用してIAMロールを作成します。ユーザーがサインインしたときにAssumeRoleWithWebIdentityを使用してIAMロールを想定し、STSを使用して一時的なセキュリティ資格情報を付与するWeb IDフェデレーションを使用します。|<p><span style="font-size: 1rem;"><br></span></p><p>Option A is incorrect because IAM Roles are preferred over IAM Users, because IAM Users have to access the AWS resources using access and secret keys, which is a security concern.</p><p>Option B is this is not a feasible configuration.</p><p>Option C is CORRECT because it (a) creates an IAM Role with the needed permissions to connect to DynamoDB, (b) it authenticates the users with Web Identity Federation, and (c) the application accesses the DynamoDB with temporary credentials that are given by STS.</p><p>Option D is incorrect because the step to create the Active Directory (AD) server and using AD for authenticating is unnecessary and costly.</p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">See the image below for more information on AssumeRoleWithWebIdentity API</span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/d_65_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_middle"><br></span></p><p><br></p><p><span style="font-size: 1rem;">For more information on AssumeRolewithWebIdentity, please visit the below URL</span></p><p><a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html" target="_blank">http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html</a></p>	Create an IAM group that gives access to your application and to the DynamoDB tables. Then, when writing to DynamoDB, simply include the unique device ID to associate the data with that specific user. <br>  アプリケーションとDynamo DBテーブルにアクセスできるIAMグループを作成します。次に、DynamoDBに書き込むときは、固有のデバイスIDを指定して、その特定のユーザーとデータを関連付けるだけです。	During the install and game configuration process, have each user create an IAM credential and assign the IAM user to a group with proper permissions to communicate with DynamoDB. <br>  インストールとゲームの設定プロセス中に、各ユーザーにIAM資格を作成させ、IAMユーザーをDynamo DBと通信するための適切な権限を持つグループに割り当てます。	Create an Active Directory server and an AD user for each mobile application user. When the user signs in to the AD sign-on, allow the AD server to federate using SAML 2.0 to IAM and assign a role to the AD user which is the assumed with AssumeRoleWithSAML. <br>  ユーザーがADサインオンにサインオンすると、ADサーバーはSAML 2.0を使用してIAMにフェデレートし、AssumeRoleWithSAMLで想定されるADユーザーに役割を割り当てます。
Diag-66. <p>You have been given the task of designing a backup strategy for your organization's AWS resources with the only caveat being that you must use the AWS Storage Gateway. Which of the following is the correct/appropriate statement surrounding the backup strategy on the AWS Storage Gateway? Choose the correct answer from the options below</p> | <p> AWSストレージゲートウェイを使用する必要があるという唯一の注意点がありますが、組織のAWSリソースのバックアップ戦略を設計する作業があります。AWS Storage Gatewayのバックアップ戦略に関する正しい/適切な声明はどれですか？下記のオプションから正解を選択してください</p>	sa:	You should use Gateway-Stored Volumes as it is preferable to Gateway-Cached Volumes as a backup storage medium. <br>  ゲートウェイストレージボリュームは、バックアップストレージメディアとしてゲートウェイキャッシュボリュームよりも望ましいので、ゲートウェイストレージボリュームを使用する必要があります。|<p><br></p><p>Option A is incorrect because Gateway-Stored Volumes are used for backing up the data from on-premises to the Amazon S3.</p><p>Option B is incorrect because it keeps only the frequently accessed data (not the entire data) on the on-premises server to which you get the quick access.</p><p>Option C is incorrect because both Gateway-Cached Volume as well as Gateway-Stored Volume can be independently deployed as storage/backup options and need not necessarily be combined with VTL.</p><p>Option D is CORRECT because (a) the scenario in the question is asking you to design a backup (not a storage) strategy, (b)  Gateway-Stored Volume <i>backs up</i> the data on Amazon S3 while keeping the data on the on-premises server, and (c) Gateway-Cached Volume only keeps the frequently accessed data on the on-premises server and <i>stores </i>the data on Amazon S3.</p><p><br></p><p><b>More information on AWS Storage Gateway</b></p><p><span></span></p><p style="">Volume Gateway – A volume gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers. The gateway supports the following volume configurations:</p><div style=""><ul type="disc"><li><p>Cached volumes – You store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally. Cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data.</p></li><li><p>Stored volumes – If you need low-latency access to your entire dataset, first configure your on-premises gateway to store all your data locally. Then asynchronously back up point-in-time snapshots of this data to Amazon S3. This configuration provides durable and inexpensive offsite backups that you can recover to your local data center or Amazon EC2. For example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2.</p></li></ul></div><p style="">Tape Gateway – With a tape gateway, you can cost-effectively and durably archive backup data in Amazon Glacier. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p><br><p></p><p>For more information on Storage gateways, please visit the below URL:</p><p><a href="https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html" target="_blank">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><a href="https://aws.amazon.com/storagegateway/faqs/" target="_blank">https://aws.amazon.com/storagegateway/faqs/</a></p>	You should use Gateway-Cached Volumes. You will have quicker access to the data, and it is a more preferred backup solution than Gateway-Stored Volumes. <br>  ゲートウェイキャッシュボリュームを使用する必要があります。 データにすばやくアクセスでき、ゲートウェイ格納ボリュームよりも好ましいバックアップソリューションです。	It doesn't matter whether you use Gateway-Cached Volumes or Gateway-Stored Volumes as long as you also combine either of these solutions with the Gateway-Virtual Tape Library (VTL). <br>  これらのソリューションをGateway-VTL（Virtual Tape Library）と組み合わせて使用​​している場合は、Gateway-Cached VolumesまたはGateway-Stored Volumesを使用しても問題はありません。	You should use the Gateway-Virtual Tape Library (VTL) since the Gateway-Cached Volumes and Gateway-Stored Volumes cannot be used for backups. <br>  Gateway-Cached VolumesとGateway-Stored Volumesはバックアップに使用できないため、ゲートウェイ仮想テープライブラリ（VTL）を使用する必要があります。
Diag-67. <p>Your company has an e-commerce platform which is expanding all over the globe, you have EC2 instances deployed in multiple regions you want to monitor the performance of all of these EC2 instances. How will you setup CloudWatch to monitor EC2 instances in multiple regions?</p> | <p>貴社には世界中に広がっているeコマースプラットフォームがあり、これらのEC2インスタンスのすべてのパフォーマンスを監視したい複数の地域にEC2インスタンスを導入しています。複数の地域のEC2インスタンスを監視するためにCloudWatchを設定する方法は？</p>	sa:	Have one single dashboard that reports the metrics from CloudWatch pertaining to different regions <br>  異なる地域に関連するCloudWatchの指標を報告する単一のダッシュボードを用意する|<p>You can monitor AWS resources in multiple regions using a single CloudWatch dashboard. For example, you can create a dashboard that shows CPU utilization for an EC2 instance located in the us-west-2 region with your billing metrics, which are located in the us-east-1 region.</p><p>Please see the following snapshot which shows how a global CloudWatch Dashboard looks:</p><p><img src="https://s3.amazonaws.com/awssap/d_67_1.png" alt="" width="1820" height="523" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>For more information on Cloudwatch dashboard, please refer to the below URL<br> <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cross_region_dashboard.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cross_region_dashboard.html</a></p>	Register instances running on different regions to CloudWatch <br>  異なる地域で実行されているインスタンスをCloudWatchに登録する	Create separate dashboards in every region <br>  各地域に別々のダッシュボードを作成する	This is not possible <br>  これは不可能です
Diag-68. <p>What does the below custom IAM Policy achieve?</p> <p>{</p><p>    "Version": "2012-10-17",</p><p>    "Statement": [</p><p>        {</p><p>            "Sid": "VisualEditor0",</p><p>            "Effect": "Allow",</p><p>            "Action": [</p><p>                "ec2:TerminateInstances",</p><p>                "ec2:StartInstances",</p><p>                "ec2:RunInstances",</p><p>                "ec2:StopInstances"</p><p>            ],</p><p>            "Resource": "arn:aws:ec2:*:*:instance/*"</p><p>        },</p><p>        {</p><p>            "Sid": "VisualEditor1",</p><p>            "Effect": "Allow",</p><p>            "Action": "ec2:DescribeInstances",</p><p>            "Resource": "*"</p><p>        }</p><p>    ]</p><p>}</p> | <p>以下のカスタムIAMポリシーはどのように達成されますか？</p> <p> {</p> <p>    "バージョン"： "2012-10-17"、</p> <p>      "声明"：[</p> <p>            {</p> <p>                  "Sid"： "VisualEditor0"、</p> <p>                  "効果"： "許可"、</p> <p>                  "アクション"：[</p> <p>                        "ec2：TerminateInstances"、</p> <p>         そして、nbsp;             "ec2：StartInstances"、</p> <p>                        "ec2：RunInstances"、</p> <p>                        "ec2：StopInstances" </p> <p>                  ]、</p> <p>                  "リソース"： "arn：aws：ec2：*：*：インスタンス/ *" </p> <p>            }、</p> <p>            {</p> <p>                  "Sid"： "VisualEditor1"、</p> < p>                  "効果"： "許可"、</p> <p>                  "Action"： "ec2：DescribeInstances"、</p> <p>                  "リソース"： "*" </p> <p>            } </p> <p>      ] </p> <p>} </p> "*" </p> <p>            } </p> <p>      ] </p> <p>} </p> "*" </p> <p>            } </p> <p>      ] </p> <p>} </p>	sa:	Permits the user start, stop, and terminate the existing instances. <br>  既存のインスタンスの開始、停止、終了を許可します。|<p><br></p><p>Option A is CORRECT because, although the policy given in the question allows the access to launch the EC2 instance by including "ec2:RunInstances" in the Actions, it will not allow the user to launch the EC2 instances.<span> (<span style="">Try creating the same policy, attach it to a new user. </span><span style="">You can login using that user credentials and see if you can launch any EC2 instance. You will not be able to do so. You will get the error shown below.). </span></span>In order to allow users to launch an instance, the policy needs to be updated to grant the user more privileges: access to launch using an EC2 key pair, a security group, an Elastic Block Store (EBS) volume, and an Amazon Machine Image (AMI). To do this, you will have to create a separate statement for the RunInstances action.</p><p>Option B is incorrect because, as mentioned above, the user will not be able to launch any EC2 instance and will get an error (shown below) about not having the permission to do so.</p><p><img src="https://s3.amazonaws.com/awssap/d_68_1.png" alt="" width="624" height="186" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>Option C is incorrect because the user can start, stop, and terminate existing instances with this policy<span style="font-size: 1rem;">.</span></p><p><span style="font-size: 1rem;">Option D is incorrect because the user will be able to start, stop and terminate existing EC2 instances.</span></p><p><span style="font-size: 1rem;"><br></span></p><p>For more information on EC2 resource-level permissions please visit the below URL and for further explanation as to why only the TerminateInstances, StopInstances, and StartInstances actions are allowed please visit the below URL</p><p><a href="https://aws.amazon.com/blogs/security/demystifying-ec2-resource-level-permissions/" target="_blank">https://aws.amazon.com/blogs/security/demystifying-ec2-resource-level-permissions/</a></p>	Permits the user to launch a new instance as well as start, stop, and terminate the existing instances. <br>  ユーザーが新しいインスタンスを起動し、既存のインスタンスを開始、停止、終了することを許可します。	Permits the user to only describe the instances (read only), and will not be able to start, stop, or terminate instances, since it overrides the allowed actions of TerminateInstances, RunInstances, StartInstances, and StopInstances in the policy.? <br>  これは、ポリシー内のTerminateInstances、RunInstances、StartInstances、およびStopInstancesの許可されたアクションをオーバーライドします。	None of the above. <br>  上記のどれでもない。
Diag-69. <p>A company is running a production load Redshift cluster for a client. The client has an RTO objective of one hour and an RPO of one day. While configuring the initial cluster what configuration would best meet the recovery needs of the client for this specific Redshift cluster configuration? Choose the correct answer from the options below</p> | <p>ある会社がクライアント用の実動負荷Redshiftクラスタを実行しています。クライアントのRTO目標は1時間、RPOは1日です。この特定のRedshiftクラスタ構成では、初期クラスタの設定中にクライアントのリカバリニーズに最も適した構成は何ですか。下記のオプションから正解を選択してください</p>	sa:	Enable automatic snapshots and configure automatic snapshot copy from the current production cluster to the disaster recovery region. <br>  自動スナップショットを有効にし、現在の本番クラスタから障害回復領域への自動スナップショットコピーを構成します。|<p><br></p><p>Option A is incorrect because it copies the snapshot from the destination region (disaster recovery region).</p><p>Option B is CORRECT because it copies the snapshot from source region (production) to the destination region (disaster recovery region).</p><p>Option C is incorrect because you do not need to copy the manual snapshots (as it is an overhead), Redshift copies the snapshot from source to destination region.</p><p>Option D is incorrect because Redshift replicates the data to another region using snapshots. Once the snapshot is copied from the source to destination region, you need to restore cluster from the snapshot and use its DSN.</p><p><br></p><p><b>More information on Amazon Redshift Snapshots</b></p><p>Snapshots are point-in-time backups of a cluster. There are two types of snapshots: <em>automated</em> and <em>manual</em>. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection. If you need to restore from a snapshot, Amazon Redshift creates a new cluster and imports data from the snapshot that you specify.</p><p>When you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately. The cluster streams data on demand from the snapshot in response to the active queries then loads the remaining data in the background.</p><p>Amazon Redshift periodically takes snapshots and tracks incremental changes to the cluster since the last snapshot. Amazon Redshift retains all of the data required to restore a cluster from a snapshot.</p><p>For more information on RedShift snapshots, please visit the below URL</p><p><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p>	Enable automatic snapshots on the cluster in the production region FROM the disaster recovery region so snapshots are available in the disaster recovery region and can be launched in the event of a disaster. <br>  ディザスタリカバリ領域のプロダクション領域のクラスタで自動スナップショットを有効にすることで、ディザスタリカバリ領域でスナップショットを利用できるようになり、災害時に起動することができます。	Enable automatic snapshots on a Redshift cluster. In the event of a disaster, a failover to the backup region is needed. Manually copy the snapshot from the primary region to the secondary region. <br>  Redshiftクラスタで自動スナップショットを有効にする。 災害が発生した場合、バックアップ領域へのフェイルオーバーが必要です。 スナップショットをプライマリ領域からセカンダリ領域に手動でコピーします。	Create the cluster configuration and enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. In the event of a disaster, change the DNS endpoint to the secondary cluster's leader node. <br>  クラスタ構成を作成し、プライマリリージョンで実行されているクラスタからセカンダリリージョンで動作しているクラスタにRedshiftレプリケーションを有効にします。 災害が発生した場合は、DNSエンドポイントをセカンダリクラスタのリーダーノードに変更します。
Diag-70. <p>A company runs their current application entirely on-premise. However, they are expecting a big boost in traffic and need to figure out a way to decrease the load to handle the scale. Unfortunately, they cannot migrate their application to AWS in the period required. What could they do with their current on-premise application to help offload some of the traffic and scale to meet the demand expected in 24 hours in a cost-effective way? Choose the correct answer from the options below.</p> | <p>企業は、現在のアプリケーションを完全に社内で実行します。しかし、彼らはトラフィックの大幅な増加を期待しており、規模を処理するために負荷を減らす方法を見つけ出す必要があります。残念ながら、必要な期間、AWSにアプリケーションを移行することはできません。現在のオンプレミスアプリケーションを使用して、費用対効果の高い方法で24時間以内に予想される需要に対応するために、トラフィックの一部と規模を削減することができますか？下記のオプションから正解を選択してください。</p>	sa:	Create a CloudFront CDN, enable query string forwarding and TTL of zero on the origin. Offload the DNS to AWS to handle CloudFront CDN traffic but use on-premise load balancers as the origin. <br>  CloudFront CDNを作成し、クエリ文字列の転送と原点のゼロのTTLを有効にします。CloudFront CDNトラフィックを処理するためにDNSをAWSにオフロードしますが、オンプレミスのロードバランサを起点として使用します。|<p><br></p><p>The main point to consider is that the application should entirely stay on the on-premises server but still leverage AWS offerings  for handling the peak traffic and scale on demand. CloudFront is best suited for such situation because it can use the on-premises server as the custom origin.</p><p><br></p><p>Option A is incorrect because even though OpsWork can work with on-premises servers, setting up the EC2 instances with Auto Scaling would be a costly solution.</p><p>Option B is incorrect because moving to static files to S3 is not sufficient to improve the scalability to handle the peak load.</p><p>Option C is incorrect because the requirement explicitly mentions that the application cannot be migrated to AWS.</p><p>Option D is CORRECT because CloudFront - which is an AWS managed - is a highly available, scalable service that can use the on-premises server as the origin. By setting the TTL to 0, the content will be delivered from the origin as soon as it gets changed.  See the image below:</p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;"><img src="https://s3.amazonaws.com/awssap/d_70_1.png" alt="" width="1582" height="768" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></span></p><p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">For more information on CloudFront, please visit the below URL</span></p><p><a href="https://aws.amazon.com/cloudfront/" target="_blank">https://aws.amazon.com/cloudfront/</a></p><p><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/" target="_blank">https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/</a></p>	Upload all static files to Amazon S3 and create a CloudFront distribution serving those static files. <br>  すべての静的ファイルをAmazon S3にアップロードし、静的ファイルを提供するCloudFrontディストリビューションを作成します。	Duplicate half your web infrastructure on AWS, offload the DNS to Route 53 and configure weighted based DNS routing to send half the traffic to AWS . <br>  AWS上のWebインフラストラクチャの半分を複製し、DNSをRoute 53にオフロードし、加重ベースのDNSルーティングを構成してトラフィックの半分をAWSに送信します。	Deploy OpsWorks on-premise to manage the instance in order to configure on-premise auto scaling to meet the demand. <br>  需要を満たすためにオンプレミスの自動スケーリングを設定するために、OpsWorksをオンプレミスで導入してインスタンスを管理します。
Diag-71. <span style="">If one needs to establish a low latency dedicated connection to an S3 public endpoint over the Direct Connect dedicated low latency connection, what steps need to be taken to accomplish configuring a direct connection to a public S3 endpoint? Choose the correct answer from the options below</span><a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank"><br></a> | <span style = ""> Direct Connect専用の低レイテンシ接続を介してS3公開エンドポイントへのレイテンシの低い専用接続を確立する必要がある場合、パブリックS3エンドポイントへの直接接続を設定するためにはどのような手順を実行する必要がありますか？下記のオプションから正しい答えを選択してください。</ span> <a href="https://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank"> <br> </ span> a>	sa:	Configure a public virtual interface to connect to a public S3 endpoint resource. <br>  パブリックS3エンドポイントリソースに接続するようにパブリック仮想インターフェイスを構成します。|<p><span style="font-size: 1rem;">You can create a public virtual interface to connect to public resources or a private virtual interface to connect to your VPC. You can configure multiple virtual interfaces on a single AWS Direct Connect connection, and you'll need one private virtual interface for each VPC to connect to. Each virtual interface needs a VLAN ID, interface IP address, ASN, and BGP key. See the image below:</span></p><p><img src="https://s3.amazonaws.com/awssap/d_71_1.jpg" alt="" width="638" height="359" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p>Option A is CORRECT because, as mentioned above, it creates a public virtual interface to connect to S3 endpoint.</p><p>Option B is incorrect because to connect to S3 endpoint, a public virtual interface needs to be created, not VPN.</p><p>Option C is incorrect because to connect to S3 endpoint, a <b>public </b>virtual interface needs to be created, <b>not</b> <b>private</b>.</p><p>Option D is incorrect because this setup will not help connecting to the S3 endpoint.</p><p><br></p><p>For more information on virtual interfaces, please visit the below URL</p><p></p><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank" style="font-size: 1rem;">http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html</a><br><p></p>	Establish a VPN connection from the VPC to the public S3 endpoint. <br>  VPCからパブリックS3エンドポイントへのVPN接続を確立します。	Configure a private virtual interface to connect to the public S3 endpoint via the Direct Connect connection. <br>  直接接続接続を介してパブリックS3エンドポイントに接続するようにプライベート仮想インターフェイスを設定します。	Add a BGP route as part of the on-premise router; this will route S3 related traffic to the public S3 endpoint to dedicated AWS region. <br>  BGPルートをオンプレミスルータの一部として追加します。S3関連のトラフィックをパブリックS3エンドポイントに専用のAWS領域にルーティングします。
Diag-72. <p>A company has a Redshift cluster for petabyte-scale data warehousing. The data within the cluster is easily reproducible from additional data stored on Amazon S3. The company wants to reduce the overall total cost of running this Redshift cluster. Which scenario would best meet the needs of the running cluster, while still reducing total overall ownership cost of the cluster? Choose the correct answer from the options below</p> | <p>ある企業には、ペタバイト規模のデータウェアハウジング用のRedshiftクラスタがあります。クラスタ内のデータは、Amazon S3に保存されている追加データから簡単に再現できます。同社は、このRedshiftクラスタを実行するための全体的な総コストを削減したいと考えています。どのシナリオが実行中のクラスタのニーズを最も満たしながらも、クラスタ全体の全体的な所有コストを引き下げますか？下記のオプションから正解を選択してください</p>	sa:	Disable automated and manual snapshots on the cluster <br>  クラスタ上の自動スナップショットと手動スナップショットを無効にする|<p>Snapshots are point-in-time backups of a cluster. There are two types of snapshots: <em>automated</em> and <em>manual</em>. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection. If you need to restore from a snapshot, Amazon Redshift creates a new cluster and imports data from the snapshot that you specify.</p><p>Now since the question already mentions that the cluster is easily reproducible from additional data stored on Amazon S3 then you don’t need to maintain any sort of snapshots.</p><p><br></p><p>Option A is incorrect because (a) manual snapshots are going to be costly, ans (b) since the cluster can be reproducible from the data stored in S3, the step copying to another region is not needed.</p><p>Option B is incorrect because taking automated snapshots is an expensive solution here.</p><p>Option C is incorrect because implementing daily backup is going to be expensive as well.</p><p>Option D is CORRECT because taking any of automated and manual snapshots is unnecessary as the cluster can easily be restored via the data stored in S3. Hence, once the snapshot taking is disabled, the cost would be lowered.</p><p><br></p><p>For more information on Redshift snapshots, please visit the below URL</p><p><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html" target="_blank">http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p>	Enable automated snapshots but set the retention period to a lower number to reduce storage costs <br>  自動化されたスナップショットを有効にしますが、保存期間を短縮してストレージコストを削減します	Implement daily backups, but do not enable multi-region copy to save data transfer costs. <br>  毎日のバックアップを実装しますが、マルチリージョンコピーを有効にしてデータ転送コストを節約しないでください。	Instead of implementing automatic daily backups, write a CLI script that creates manual snapshots every few days. Copy the manual snapshot to a secondary AWS region for disaster recovery situations. <br>  ディザスタリカバリのために、手動スナップショットをセカンダリAWSリージョンにコピーします。自動日次バックアップを実装する代わりに、数日ごとに手動スナップショットを作成するCLIスクリプトを作成します。
Diag-73. <p>A company is running a web application that has a high amount of dynamic content. The company is looking to reduce load time by implementing a caching solution that will help reduce load times for clients requesting the application. What is the best possible solution and why? Choose the correct answer from the options below</p> | <p>会社は動的コンテンツの量が多いWebアプリケーションを実行しています。アプリケーションを要求するクライアントの負荷時間を短縮するキャッシングソリューションを実装することで、ロード時間を短縮することを検討しています。何が最良の解決策であり、なぜですか？下記のオプションから正解を選択してください</p>	sa:	Create a CloudFront distribution, enable query string forwarding, set the TTL to 0: This will keep TCP connections open from CloudFront to origin, reducing the time it takes for TCP handshake to occur. <br>  CloudFrontディストリビューションを作成し、クエリ文字列の転送を有効にし、TTLを0に設定します。これにより、TCP接続がCloudFrontから開かれたままになり、TCPハンドシェイクが発生するのにかかる時間が短縮されます。|<p><br></p><p>The scenario requires a caching solution which should help reducing the load time of the dynamic content. Although ElastiCache is a good solution for caching, it is most suited for caching the static content so that the read intensive load is reduced on the database instance. CloudFront is a good solution to improve the performance of a distributed application that has a high amount of dynamic content.</p><p><br></p><p>Option A is incorrect because Route 53 helps improving the resolving of the DNS queries for the multi-region application. It does not help caching of the dynamic content.</p><p>Option B is incorrect because ElastiCache is most suited for caching the static content so that the read intensive load is reduced on the database instance.</p><p>Option C is CORRECT because (a) it uses CloudFront distribution which is AWS managed highly available and scalable service, (b) it sets the TTL to 0, so that whenever the content changes at the origin, the updated content immediately gets cached at all the edge locations, giving users the latest content, and (c) it uses query string forwarding to get the custom or dynamic content generated at the origin server using the query string parameters. See the image below for the CloudFront settings.</p><p><img src="https://s3.amazonaws.com/awssap/d_73_1.png" alt="" width="1035" height="783" role="presentation" class="img-responsive atto_image_button_text-bottom"><img src="https://s3.amazonaws.com/awssap/d_73_2.png"><br></p><p>Option D is incorrect because the query string forwarding should be enabled in order to get the custom or dynamic content generated at the origin server using the query string parameters.</p><p><br></p><p>NOTE:</p><p></p><p><i>TTL is not set to 0 to just "fetch for content every time". It is set to 0 so that the users always get the "most up-to-date" dynamic content whenever they fetch it. Setting TTL to 0 does not mean that the content does not get cached. It is still cached by CloudFront. It just makes a GET request with <b>If-Modified-Since</b> header to the origin. The origin then decides whether the CloudFront can continue to use the cached content (if it is not changed). A GET request doesn't necessarily mean that the object itself is retrieved from the origin, rather the origin should return the <a href="http://en.wikipedia.org/wiki/List_of_HTTP_status_codes#3xx_Redirection" rel="noreferrer" target="_blank" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=http://en.wikipedia.org/wiki/List_of_HTTP_status_codes%233xx_Redirection&amp;source=gmail&amp;ust=1520885551539000&amp;usg=AFQjCNFweq7kcV68iPv44wI-MhYlj6TsUQ">HTTP status code 304 - Not Modified</a> where applicable. Refer to <a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/" target="_blank" data-saferedirecturl="https://www.google.com/url?hl=en&amp;q=https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-dynamic-content/&amp;source=gmail&amp;ust=1520885551539000&amp;usg=AFQjCNEnQnHXcay23PxLQYzaKoLfkPU5jQ">https://aws.amazon.com/<wbr>blogs/aws/amazon-cloudfront-<wbr>support-for-dynamic-content/</a></i></p><p><i style="font-size: 1rem;">Just because the CloudFront queries the origin does not make the application slow, and even if it does minutely, it is necessary to give the user the most up-to-date dynamic data.</i></p><p></p><p><br></p><p><b>More information on CloudFront, TTL, and Query String Forwarding</b></p><p>Amazon CloudFront works seamlessly with dynamic web applications running in Amazon EC2 or your origin running outside of AWS without any custom coding or proprietary configurations, making the service simple to deploy and manage. </p><p>For more information on CloudFront dynamic content, please visit the below URL</p><p><a href="https://aws.amazon.com/cloudfront/dynamic-content/" target="_blank">https://aws.amazon.com/cloudfront/dynamic-content/</a></p>	Create an ElastiCache cluster, write code that caches the correct dynamic content and places it in front of the RDS dynamic content. This will reduce the amount of time it takes to request the dynamic content since it is cached. <br>  ElastiCacheクラスタを作成し、正しい動的コンテンツをキャッシュしてRDSの動的コンテンツの前に配置するコードを記述します。 これにより、キャッシュされている動的コンテンツを要求するのに要する時間が短縮されます。	Offload the DNS to Route 53; Route 53 has DNS servers all around the world and routes the request to the closest region which reduces DNS latency. <br>  DNSをRoute 53にオフロードします。 Route 53には世界中のDNSサーバがあり、DNSレイテンシを短縮する最も近い領域に要求をルーティングします。	Create a CloudFront distribution; disable query string forwarding, set the TTL to 0. This will keep TCP connections open from CloudFront to origin, reducing the time it takes for TCP handshake to occur <br>  CloudFrontディストリビューションを作成します。 クエリ文字列の転送を無効にし、TTLを0に設定します。これにより、TCP接続がCloudFrontから始点に開かれ、TCPハンドシェイクが発生するのにかかる時間が短縮されます
Diag-74. <p>You are running an online gaming server, with one of its requirements being a need for 100,000 IOPS of write performance on its EBS volumes. Given the fact that EBS volumes can only provision a maximum of up to 20,000 IOPS which of the following would be a reasonable solution if instance bandwidth is not an issue? Choose the correct answer from the options below</p> | <p> EBSボリュームで100,000 IOPSの書き込みパフォーマンスが必要な要件を満たすオンラインゲームサーバーを実行しています。EBSボリュームで最大20,000 IOPSまでしかプロビジョニングできないことを考慮すると、インスタンス帯域幅が問題にならない場合は、次のうちどれが合理的な解決策になりますか？下記のオプションから正解を選択してください</p>	sa:	Create a RAID 0 configuration for five 20,000 IOPS EBS volumes. <br>  5つの20,000 IOPS EBSボリュームに対してRAID 0構成を作成します。|<p><br></p><p>Option A is CORRECT because creating a RAID 0 array allows you to achieve a higher level of performance for a file system than you can provision on a single Amazon EBS volume and the resulting size of a RAID 0 array is the sum of the sizes of the volumes within it, and the bandwidth is the sum of the available bandwidth of the volumes within it. </p><p>Option B is incorrect because ephemeral storage may not always have consistent and reliable I/O performance as given by PIOPS EBS Volumes.</p><p>Option C is incorrect because (a) instance bandwidth is not an issue, and (b) auto-scaling with spot instances will not increase the IOPS of the EBS volumes.</p><p>Option D is incorrect because launching the instances in a placement group does not increase the IOPS of the EBS volumes, it only increases the overall network performance.</p><p><br></p><p><b>More information on EBS with RAID Configuration</b></p><p>With Amazon EBS you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level. For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together.</p><p>An example of better throughout with RAID 0 configuration is also given in the AWS documentation</p><p> <img src="https://s3.amazonaws.com/awssap/d_74_1.png" alt="" width="749" height="134" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p>For more information on RAID configuration, please visit the below URL:</p><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>	Use ephemeral storage which gives a much larger IOPS write performance. <br>  より大きなIOPS書き込みパフォーマンスを提供する一時記憶域を使用します。	Use Auto Scaling to use spot instances when required to increase the IOPS write performance when required. <br>  必要に応じて自動スケーリングを使用して、必要に応じてIOPSの書き込みパフォーマンスを向上させる必要がある場合に、スポットインスタンスを使用します。	Create a Placement Group with five 20,000 IOPS EBS volumes. <br>  20,000 IOPSのEBSボリュームを5つ含むプレースメントグループを作成します。
Diag-75. <p>While implementation of cost-cutting measurements in your organization, you have been told that you need to migrate some of your existing resources to another region. The first task you have been given is to copy all of your Amazon Machine Images from Asia Pacific (Sydney) to US West (Oregon). One of the things that you are unsure of is how the PEM keys on your Amazon Machine Images need to be migrated. Which of the following best describes how your PEM keys are affected when AMIs are migrated between regions? Choose the correct answer from the options below</p> | <p>組織でコスト削減測定を実施している間に、既存のリソースの一部を別の地域に移行する必要があるという声がありました。あなたが与えられた最初の仕事は、アジア太平洋（シドニー）から米国西部（オレゴン州）にあなたのすべてのAmazon Machine Imagesをコピーすることです。あなたが確信していることの1つは、Amazon Machine Image上のPEMキーをどのように移行する必要があるかです。リージョン間でAMIを移行する際に、PEMキーがどのように影響を受けるかを次のどれかに当てはめますか？下記のオプションから正解を選択してください</p>	sa:	The PEM keys will not be copied to the new region but the authorization keys will still be in the operating system of the AMI. You need to ensure when the new AMI is launched that it is launched with the same PEM key. <br>  PEMキーは新しい領域にコピーされませんが、AMIのオペレーティングシステムには依然として認可キーが存在します。 新しいAMIがいつ起動され、同じPEMキーで起動されるかを確認する必要があります。|<p><br></p><p>Option A is incorrect because, as mentioned above, the PEM Keys are private keys and are never copied across the regions.</p><p>Option B is incorrect because the PEM keys are not user-specific.</p><p>Option C is incorrect because the authorization keys are copied across the region.</p><p>Option D is CORRECT because the authorization key is included in the AMI, hence copied across the region; however, the PEM keys are not copied; hence, need to be imported explicitly. See the AWS Console option for importing the PEM key.</p><p><img src="https://s3.amazonaws.com/awssap/d_75_1.png" alt="" width="929" height="657" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><br></p><p><span style="font-size: 1rem;">For more information on EC2 key pairs, please visit the below URL</span></p><p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html</a></p><p>For more information on this subject, please visit the below forums on AWS</p><p><a href="https://forums.aws.amazon.com/thread.jspa?threadID=52654" target="_blank">https://forums.aws.amazon.com/thread.jspa?threadID=52654</a></p>	The PEM keys will also be copied across; however, they will only work for users who have already accessed them in the old region. If you need new users to access the instances then new keys will need to be generated. <br>  PEMキーもコピーされます。 ただし、古い地域で既にアクセスしたユーザーにのみ機能します。 新しいユーザーがインスタンスにアクセスする必要がある場合は、新しいキーを生成する必要があります。	Neither the PEM key nor the authorized key is copied and consequently you need to create new keys when you launch the new instance. <br>  PEMキーも認可されたキーもコピーされないため、新しいインスタンスを起動するときに新しいキーを作成する必要があります。	The PEM keys will also be copied across so you don't need to do anything except launch the new instance. <br>  PEMキーもコピーされるので、新しいインスタンスを起動する以外は何もする必要はありません。
Diag-76. <p>You're working as a consultant for a company that has a three-tier application. The application layer of this architecture sends over 20Gbps of data per seconds during peak hours to and from Amazon S3. Currently, you're running two NAT gateways in two subnets to transfer the data from your private application layer to Amazon S3. You will also need to ensure that the instances receive software patches from a third party repository. What architecture changes should be made, if any? Choose the correct answer from the options below.</p> | <p>あなたは3層アプリケーションを持つ企業のコンサルタントとして働いています。このアーキテクチャのアプリケーション層は、ピーク時にAmazon S3との間で秒当たり20Gbps以上のデータを送信します。現在、プライベートアプリケーション層からAmazon S3にデータを転送するために、2つのサブネットに2つのNATゲートウェイを実行しています。また、インスタンスがサードパーティのリポジトリからソフトウェアパッチを受け取るようにする必要があります。もしあれば、どのようなアーキテクチャ変更を行うべきですか？下記のオプションから正解を選択してください。</p>	sa:	Keep the NAT gateways and create a VPC S3 endpoint which allows for higher bandwidth throughput as well as tighter security <br>  NATゲートウェイを維持し、高い帯域幅スループットとより厳しいセキュリティを実現するVPC S3エンドポイントを作成する|<p><br></p><p></p><p>VPC Endpoints for Amazon S3 are easy to configure, highly reliable, and provide a secure connection to S3 that does not require a gateway or NAT instances. The <span style="font-size: 1rem;">EC2 instances running in a private subnets of a VPC can now have controlled access to S3 buckets, objects, and API functions that are in the same region as the VPC. You can use an S3 bucket policy to indicate which VPCs and which VPC Endpoints have access to your S3 buckets.</span></p><br><p></p><p>Option A is incorrect because adding a third NAT Gateway for communicating with S3 bucket is a costly solution compared to creating an S3 endpoint.</p><p>Option B is CORRECT because (a) you can securely connect with S3 via the S3 endpoint, and (b) even though you can connect to S3 endpoint without requiring a NAT gateway, you still need to keep it because the instances in the VPC needs to receive the software patches from the third party repository. See the image in the <i>More information on VPC Endpoint for S3</i> section.</p><p>Option C is incorrect because you need to connect to the Amazon S3 via VPC endpoint as the current NAT gateways may not be sufficient to handle the peak load.</p><p>Option D is incorrect because if you remove the NAT Gateway, the instances in the VPC will not be able to receive the software patches from the third party repository.</p><p><br></p><p><b>More information on VPC Endpoint for S3</b></p><p>VPC endpoints alleviate the need for everything to go through the  NAT instance</p><p> <img src="https://s3.amazonaws.com/awssap/d_76_1.png" alt="" width="923" height="165" role="presentation" class="img-responsive atto_image_button_text-bottom"></p><p><img src="https://s3.amazonaws.com/awssap/d_76_2.png" alt="" width="550" height="366" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p>For more information on VPC endpoints please refer to the below URL:</p><p><a href="https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/" target="_blank">https://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a></p><p><br></p>	NAT gateways support network performance of 10 Gbps and two of them are running: Add a third NAT Gateway to a separate subnet to allow for any increase in demand. <br>  NATゲートウェイは10 Gbpsのネットワークパフォーマンスをサポートし、そのうち2つが実行されています。第3のNATゲートウェイを別のサブネットに追加して、需要の増加を可能にします。	NAT gateways support 10Gbps and two are running: No changes are required to improve this architecture. <br>  NATゲートウェイは10 Gbpsをサポートし、2つは動作しています。このアーキテクチャを改善するために変更は必要ありません。	Remove the NAT gateways and create a VPC S3 endpoint which allows for higher bandwidth throughput as well as tighter security. <br>  NATゲートウェイを削除し、VPC S3エンドポイントを作成します。これにより、より高い帯域幅のスループットとより厳しいセキュリティが可能になります。
Diag-77. <p>BCJC is running Oracle DB workloads on AWS. Currently, they are running the Oracle RAC configuration on the AWS public cloud. You've been tasked with configuring backups on the RAC cluster to enable durability. What is the best method for configuring backups? Choose the correct answer from the options below</p> | <p> BCJCはAWS上でOracle DBワークロードを実行しています。現在、AWSパブリッククラウド上でOracle RAC設定を実行しています。耐久性を有効にするために、RACクラスタでバックアップを構成する必要があります。バックアップを設定する最も良い方法は何ですか？下記のオプションから正解を選択してください</p>	sa:	Create a script that runs snapshots against the EBS volumes to create backups and durability. <br>  EBSボリュームに対してスナップショットを実行するスクリプトを作成し、バックアップと耐久性を作成します。|<p>Currently, Oracle Real Application Cluster (RAC) is not supported as per the AWS documentation. However, you can deploy scalable RAC on Amazon EC2 using the recently-published <a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/" target="_blank">tutorial </a>and Amazon Machine Images (AMI). So, in order to take the backups, you need to take the backup in the form of EBS volume snapshots of the EC2 that is deployed for RAC.</p><p><br></p><p>Option A, B, and D are all incorrect because RDS does not support Oracle RAC.</p><p>Option C is CORRECT because Oracle RAC is supported via the deployment using Amazon EC2. Hence, for the data backup, you can create a script that takes the snapshots of the EBS volumes.</p><p><br></p><p>For more information on Oracle RAC on AWS, please visit the below URL:</p><p><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/</a></span></p><p><a href="https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/" target="_blank">https://aws.amazon.com/articles/oracle-rac-on-amazon-ec2/</a></p><p><a href="https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/" target="_blank">https://aws.amazon.com/blogs/database/amazon-aurora-as-an-alternative-to-oracle-rac/</a><br></p>	Enable Multi-AZ failover on the RDS RAC cluster to reduce the RPO and RTO in the event of disaster or failure. <br>  災害や障害が発生した場合にRPOとRTOを減らすために、RDSクラスタで複数AZのフェールオーバーを有効にします。	Create manual snapshots of the RDS backup and write a script that runs the manual snapshot. <br>  RDSバックアップの手動スナップショットを作成し、手動スナップショットを実行するスクリプトを作成します。	Enable automated backups on the RDS RAC cluster; enable auto snapshot copy to a backup region to reduce RPO and RTO. <br>  RDS RACクラスタで自動バックアップを有効にする。RPOとRTOを減らすために、バックアップ領域への自動スナップショットコピーを有効にします。
Diag-78. <p>You have multiple EC2 instances in three availability zones (AZs), with a load balancer configured for your application. You observe that only one of those AZs is receiving all the traffic. How can you ensure that all the AZs receive balanced traffic? Choose two correct answers from the options below:</p> | <p> 3つのアベイラビリティゾーン（AZ）に複数のEC2インスタンスがあり、アプリケーション用にロードバランサが構成されています。これらのAZのうちの1つだけがすべてのトラフィックを受信して​​いることがわかります。どのようにすべてのAZがバランスの取れたトラフィックを受け取れるようにすることができますか？下のオプションから2つの正解を選んでください：</p>	ma:	o:Disable sticky sessions <br>  固定セッションを無効にする|<p><br></p><p>Since the traffic is routed to only one availability zone (AZ) and none of the other AZs are receiving any, the ELB must have only one AZ registered in it. First, you have to make sure that the ELB is configured to support multiple AZs via Cross-Zone load balancing. Even after enabling the cross zone load balancing, if the traffic is routed to particular EC2 instances in an AZ, the users' sessions must have tied to those EC2 instances. This symptoms seem to be related to the sticky sessions (session affinity). So, second thing you must ensure that the sticky sessions need to be either disabled or configured to be expiring after specific period.</p><p><br></p><p>Option A is CORRECT because, as mentioned above, sticky sessions could be a reason for traffic being routed to specific EC2 instances in a specific AZ.</p><p>Option B is incorrect because reducing the health check frequency will not help in balancing the traffic between different AZs.</p><p>Option C is CORRECT because cross zone load balancing needs to be enabled on the ELB and the other AZs must be registered under this ELB.</p><p>Option D is incorrect because there is no such recommendation from Amazon about ELB.</p><p><br></p><p><b>More information on ELB, Sticky Sessions, and Cross Zone Load Balancing:</b></p><p><br></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html" target="_blank">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-crosszone-lb.html</a></p><p><img src="https://s3.amazonaws.com/awssap/d_78_1.png" alt="" width="1190" height="587" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html" target="_blank">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html</a></p><p><img src="https://s3.amazonaws.com/awssap/d_78_2.png" alt="" width="1860" height="805" role="presentation" class="img-responsive atto_image_button_text-bottom"><br></p>	x:Reduce the frequency of the health checks <br>  ヘルスチェックの頻度を減らす	o:Enable cross zone load balancer <br>  クロスゾーンロードバランサを有効にする	x:Amazon recommends to use two availability zone behind ELB <br>  AmazonはELBの後ろに2つの可用性ゾーンを使用することを推奨しています
Diag-79. <p>You are excited that your company has just purchased a Direct Connect link from AWS as everything you now do on AWS should be much faster and more reliable. Your company is based in Sydney, Australia so obviously, the Direct Connect Link to AWS will go into the Asia Pacific (Sydney) region. Your first job after the new link purchase is to create a multi-region design across the Asia Pacific(Sydney) region and the US West (N. California) region. You soon discover that all the infrastructure you deploy in the Asia Pacific(Sydney) region is extremely fast and reliable, however, the infrastructure you deploy in the US West(N. California) region is much slower and unreliable. Which of the following would be the best option to make the US West(N. California) region a more reliable connection? Choose the correct answer from the options below</p> | <p> あなたの会社はAWSから直接接続するリンクを購入したばかりで、AWSで今できることはずっと高速で信頼性が高いはずです。 あなたの会社はオーストラリアのシドニーに本拠を置くため、AWSへのダイレクトコネクトリンクはアジア太平洋（シドニー）地域に進出します。 新しいリンクを購入した後の最初の仕事は、アジア太平洋（シドニー）地域と米国西部（米国カリフォルニア州）地域にわたるマルチリージョンデザインを作成することです。 アジア太平洋（シドニー）地域に展開するすべてのインフラストラクチャは非常に高速で信頼性が高いとすぐにわかりますが、米国西部（カリフォルニア）地域に展開するインフラストラクチャははるかに低速で信頼性がありません。 米国西部（カリフォルニア州）地域をより信頼できる接続にするための最良の選択肢はどれですか？ 下のオプションから正解を選んでください</p>	sa:	Create a public virtual interface to the US West region's public end points and use VPN over the public virtual interface to protect the data. <br>  米国西部地域の公開エンドポイントへのパブリック仮想インターフェイスを作成し、パブリック仮想インターフェイス上でVPNを使用してデータを保護します。|<p><span style="font-size: 1rem;"><br></span></p><p><span style="font-size: 1rem;">AWS Direct Connect provides two types of virtual interfaces: public and private. To connect to AWS public endpoints, such as an Amazon Elastic Compute Cloud (Amazon EC2) or Amazon Simple Storage Service (Amazon S3), with dedicated network performance, use a public virtual interface. To connect to private services, such as an Amazon Virtual Private Cloud (Amazon VPC), with dedicated network performance, use a private virtual interface.</span></p><p><br></p><p>Since the scenario does not mention VPC only, you need to create a public virtual interface - which allows you to connect to all AWS public IP spaces globally.</p><p><br></p><p>Option A and B are incorrect because you need to create a public virtual interface to the US West region.</p><p>Option C is incorrect because you need to create a public virtual interface to the US West region - not Asia Pacific Region.</p><p>Option D is CORRECT because it creates a public virtual interface to the US West region which allows you to connect to the Asia Pacific region. Also, it uses secure VPN connection over the public virtual interface for the data protection.</p><p><br></p><p>For more information on virtual interfaces, please visit the below URLs:</p><p></p><ul><li><span style="font-size: 1rem; background-color: rgb(255, 255, 255);"><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html" target="_blank">http://docs.aws.amazon.com/directconnect/latest/UserGuide/WorkingWithVirtualInterfaces.html</a></span></li><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/" target="_blank">https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/</a></span></li><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/" target="_blank">https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/</a></span></li></ul><p></p>	Create a private virtual interface to the US West region's public end points and use VPN over the public virtual interface to protect the data <br>  米国西部地域の公開エンドポイントへのプライベート仮想インターフェイスを作成し、パブリック仮想インターフェイス経由でVPNを使用してデータを保護する	Create a public virtual interface to the Asia Pacific region's public end points and use VPN over the public virtual interface to protect the data. <br>  アジア太平洋地域の公開エンドポイントへのパブリック仮想インターフェイスを作成し、パブリック仮想インターフェイス上でVPNを使用してデータを保護します。	Create a private virtual interface to the Asia Pacific region's public end points and use VPN over the public virtual interface to protect the data. <br>  アジア太平洋地域の公開エンドポイントへのプライベート仮想インターフェイスを作成し、パブリック仮想インターフェイス上でVPNを使用してデータを保護します。
Diag-80. <p>In Cloudfront, what is the Origin Protocol policy that must be chosen to ensure that the communication with the origin is done either via HTTP or HTTPS? Choose an answer from the options below</p> | <p> Cloudfrontでは、発信元との通信がHTTPまたはHTTPSのどちらでも行われるようにするために選択する必要があるOrigin Protocolポリシーは何ですか？下記のオプションから回答を選択してください。</p>	sa:	Match Viewer <br>  マッチビューア|<p>Option A, B, and D are all incorrect because the answer is Match Viewer.</p><p>Option C is CORRECT because if the Origin Protocol Policy is set to Match Viewer, the CloudFront communicates with the origin using HTTP or HTTPS depending upon the protocol of the viewer request.</p><p><br></p><p></p><pre> <img src="https://s3.amazonaws.com/awssap/d_80_1.png" alt="" width="747" height="445" role="presentation" class="img-responsive atto_image_button_text-bottom" style="font-size: 1rem; font-family: &quot;Open Sans&quot;, sans-serif;"></pre><p></p><p>For more information on Cloudfront CDN please see the below link</p><p></p><ul><li><span style="background-color: rgb(255, 255, 255); font-size: 1rem;"><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html" target="_blank">http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html</a></span></li></ul><p></p>	HTTPS <br>  HTTPS	HTTP <br>  HTTP	None of the above <br>  上記のどれでもない
